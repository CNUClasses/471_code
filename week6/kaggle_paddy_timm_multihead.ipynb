{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d6533b",
   "metadata": {},
   "source": [
    "\n",
    "# Paddy Disease Classification — **PyTorch + timm** and Multi‑Head model\n",
    "\n",
    "This notebook demonstrates a **multi‑task** setup using **PyTorch** and and **[timm](https://github.com/huggingface/pytorch-image-models)** (Torch Image Models) for the Kaggle **Paddy** dataset.\n",
    "\n",
    "1) Set up the environment and choose a timm backbone (`convnext_tiny`).  \n",
    "2) Build a **custom Dataset** reading `train.csv` (`image_id`, `label`, `variety`, `age`).  \n",
    "   - Images are stored in subfolders named by **label** (e.g., `train/<label>/<image_id>`).  \n",
    "   - Each sample returns a tuple: **`(image_tensor, variety_idx, age_float, label_idx)`** as requested.  \n",
    "3) Create DataLoaders with timm‑compatible transforms.  \n",
    "4) Define a **multi‑head model**:  \n",
    "   - Head A → **disease label** classification  \n",
    "   - Head B → **variety** classification  \n",
    "   - Head R → **age** regression  \n",
    "5) Train and evaluate with a minimal, well‑commented loop.\n",
    "\n",
    "> **Note:** Point `DATA_DIR` to your local Kaggle Paddy dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e712e2b6",
   "metadata": {},
   "source": [
    "## 1) Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de007fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If timm isn't installed, uncomment:\n",
    "# !pip install timm --quiet\n",
    "\n",
    "import os, random, math, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import timm\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "DATA_DIR = Path(\"./data/\")  \n",
    "TRAIN_CSV = DATA_DIR / 'train.csv'\n",
    "TRAIN_IMG_ROOT = DATA_DIR   / 'train_images'\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "#this makes sure that colab instances running this notebook can get to included utils_kp.py\n",
    "try:\n",
    "    import utils_kp as ut\n",
    "except ModuleNotFoundError:\n",
    "    !wget https://raw.githubusercontent.com/CNUClasses/471_code/master/week6/utils_kp.py\n",
    "import utils_kp as ut\n",
    " \n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# lr=0.01 #start with this one (its too high see lr finder below)\n",
    "lr=2e-3 #lr finder approved this one\n",
    "NUM_EPOCHS=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9cb6c1",
   "metadata": {},
   "source": [
    "## 2) Custom Dataset (returns `(image, variety, age, label)`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02edd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# see utils_kp.py for PaddyDataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6049e",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Create datasets with timm transforms \n",
    "\n",
    "Not going to use ImageFolder dataset.  Instead use PaddyMultitaskDataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a14265f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'crop_mode': 'center',\n",
      "    'crop_pct': 0.875,\n",
      "    'input_size': (3, 224, 224),\n",
      "    'interpolation': 'bicubic',\n",
      "    'mean': (0.485, 0.456, 0.406),\n",
      "    'std': (0.229, 0.224, 0.225)}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# MODEL_NAME = \"resnet18\"  # try: 'efficientnet_b0', 'convnext_tiny', 'mobilenetv3_large_100', ...\n",
    "MODEL_NAME = 'convnext_tiny'\n",
    "config = timm.data.resolve_data_config({}, model=MODEL_NAME)\n",
    "train_tfms = timm.data.create_transform(**config, is_training=True, hflip=0.5, auto_augment=None)\n",
    "valid_tfms = timm.data.create_transform(**config, is_training=False)\n",
    "\n",
    "pp.pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0600bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label classes: ['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro']\n",
      "Variety classes: ['ADT45', 'AndraPonni', 'AtchayaPonni', 'IR20', 'KarnatakaPonni', 'Onthanel', 'Ponni', 'RR', 'Surya', 'Zonal']\n",
      "Train/Valid/test sizes: 8327 1040 1040\n"
     ]
    }
   ],
   "source": [
    "#get the split datasets\n",
    "train_df, valid_df, test_df=ut.getdataframes(DATA_DIR, TRAIN_CSV, TRAIN_IMG_ROOT, valid_pct=0.1, test_pct=0.1,verbose=True )\n",
    "\n",
    "train_ds = ut.PaddyMultitaskDataset(train_df, TRAIN_IMG_ROOT, transform=train_tfms)\n",
    "valid_ds = ut.PaddyMultitaskDataset(valid_df,   TRAIN_IMG_ROOT, transform=valid_tfms)\n",
    "test_ds  = ut.PaddyMultitaskDataset(test_df,  TRAIN_IMG_ROOT, transform=valid_tfms)\n",
    "\n",
    "print('Label classes:', train_ds.labels)\n",
    "print('Variety classes:', train_ds.varieties)\n",
    "print('Train/Valid/test sizes:', len(train_ds), len(valid_ds), len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e93f40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bacterial_leaf_blight',\n",
       " 'bacterial_leaf_streak',\n",
       " 'bacterial_panicle_blight',\n",
       " 'blast',\n",
       " 'brown_spot',\n",
       " 'dead_heart',\n",
       " 'downy_mildew',\n",
       " 'hispa',\n",
       " 'normal',\n",
       " 'tungro']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names=train_ds.labels\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f6870",
   "metadata": {},
   "source": [
    "## 3) Create Dataloaders\n",
    "\n",
    "- Shuffle the training loader; keep validation loader deterministic.  \n",
    "- Adjust `BATCH_SIZE` to fit your GPU/CPU memory.\n",
    "\n",
    "This is a datascience competition:<br>\n",
    "the train_images folder contains images with class membership info (in the train.csv file).<br>\n",
    "the test_images folder contains images that your model infers membership on.  These inferences are bundled into a file (see sample_submission.csv) which is submitted for ranking  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0afefd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes: torch.Size([256, 3, 224, 224]) torch.Size([256]) torch.Size([256]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "xb, y_var, y_age, y_lbl = next(iter(train_loader))\n",
    "print('Batch shapes:', xb.shape, y_var.shape, y_age.shape, y_lbl.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567303c6",
   "metadata": {},
   "source": [
    "## 4) Load Multi‑head timm Model (Transfer Learning with **timm**)\n",
    "\n",
    "- Create a **pretrained** model with 3 heads, one to predict label, one to predict variety, and one to predict age\n",
    "- **Warm-up:** freeze backbone; train the classifier head first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "816197a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params (heads only): 16149\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MultiHeadNet(nn.Module):\n",
    "    \"\"\"\n",
    "    MultiHeadNet: a simple multi-task head on top of a timm backbone.\n",
    "\n",
    "    Architecture:\n",
    "     - backbone (timm model, num_classes=0, global_pool='avg') produces pooled feature vector of shape (B, feat_dim)\n",
    "     - head_label   : Linear(feat_dim -> num_label_classes)    -> classification logits for disease label\n",
    "     - head_variety : Linear(feat_dim -> num_variety_classes)  -> classification logits for variety\n",
    "     - head_age     : Linear(feat_dim -> 1)                    -> scalar age prediction (regression)\n",
    "\n",
    "    Forward input:\n",
    "     - x : image tensor of shape (B, 3, H, W)\n",
    "\n",
    "    Forward output:\n",
    "     - dict with keys 'label', 'variety', 'age'\n",
    "        - 'label'   : Tensor (B, num_label_classes)  (use CrossEntropyLoss)\n",
    "        - 'variety' : Tensor (B, num_variety_classes) (use CrossEntropyLoss)\n",
    "        - 'age'     : Tensor (B,)                     (use MSELoss or L1)\n",
    "\n",
    "    Notes:\n",
    "     - The backbone is created in __init__ below; you can freeze it after instantiation:\n",
    "         for p in model.backbone.parameters(): p.requires_grad = False\n",
    "     - This class is intended for transfer learning: warm up heads first, then optionally fine-tune backbone.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, num_label_classes: int, num_variety_classes: int, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n",
    "        feat_dim = self.backbone.num_features\n",
    "        self.head_label   = nn.Linear(feat_dim, num_label_classes)\n",
    "        self.head_variety = nn.Linear(feat_dim, num_variety_classes)\n",
    "        self.head_age     = nn.Linear(feat_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        logits_label   = self.head_label(feats)\n",
    "        logits_variety = self.head_variety(feats)\n",
    "        age_pred       = self.head_age(feats).squeeze(1)\n",
    "        return {'label': logits_label, 'variety': logits_variety, 'age': age_pred}  #returns a dictionary of outputs\n",
    "\n",
    "model = MultiHeadNet(MODEL_NAME, train_ds.num_label_classes, train_ds.num_variety_classes, pretrained=True).to(device)\n",
    "\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "print('Trainable params (heads only):', sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a411ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see the model structure\n",
    "# print(model.backbone)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee8d2803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure just training the last layer\n",
    "# for name, p in model.named_parameters():\n",
    "#     print (f'Name={name},p.shape={p.shape}, p.requires_grad = {p.requires_grad}')\n",
    "\n",
    "#stopped here 10/1/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cdb817",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Optimizer & loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36a239fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = {\n",
    "    'label':   nn.CrossEntropyLoss(),\n",
    "    'variety': nn.CrossEntropyLoss(),\n",
    "    'age':     nn.MSELoss()\n",
    "}\n",
    "loss_weights = {'label': 1.0, 'variety': 0.7, 'age': 0.5}\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c23d6",
   "metadata": {},
   "source": [
    "# 6) Training & validation loop\n",
    "\n",
    "- Log train/valid **loss** and **accuracy** per epoch.  \n",
    "- warm up heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "087df5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss=2.8847 tr_acc_label=0.443 tr_acc_variety=0.680 | valid loss=2.0369 va_acc_label=0.622 va_acc_variety=0.714 va_mae_age=0.696\n",
      "Epoch 02 | train loss=2.0492 tr_acc_label=0.596 tr_acc_variety=0.752 | valid loss=1.7100 va_acc_label=0.643 va_acc_variety=0.751 va_mae_age=0.585\n",
      "Epoch 03 | train loss=1.8736 tr_acc_label=0.628 tr_acc_variety=0.779 | valid loss=1.5222 va_acc_label=0.714 va_acc_variety=0.802 va_mae_age=0.566\n",
      "Epoch 04 | train loss=1.7485 tr_acc_label=0.654 tr_acc_variety=0.797 | valid loss=1.4017 va_acc_label=0.736 va_acc_variety=0.809 va_mae_age=0.571\n",
      "Epoch 05 | train loss=1.6760 tr_acc_label=0.670 tr_acc_variety=0.808 | valid loss=1.3235 va_acc_label=0.761 va_acc_variety=0.826 va_mae_age=0.548\n",
      "CPU times: user 39.8 s, sys: 8.51 s, total: 48.3 s\n",
      "Wall time: 3min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "def compute_multitask_loss(outputs, targets, criteria, weights=None):\n",
    "    total = 0.0\n",
    "    for k in ['label', 'variety', 'age']:\n",
    "        w = 1.0 if (weights is None) else weights[k]\n",
    "        # print(f'for key={k}, loss={criteria[k](outputs[k], targets[k])}')\n",
    "        total = total + w * criteria[k](outputs[k], targets[k])\n",
    "\n",
    "    #maybe return the average?\n",
    "    return total\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criteria, device, weights=None):\n",
    "    model.train()\n",
    "    run_loss= 0.0\n",
    "    correct_label, correct_variety, total = 0, 0, 0\n",
    "    for images, y_var, y_age, y_lbl in loader:\n",
    "        images = images.to(device); y_lbl = y_lbl.to(device); y_var = y_var.to(device); y_age = y_age.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = model(images)  #only feeding in images\n",
    "        loss = compute_multitask_loss(outputs, {'label': y_lbl, 'variety': y_var, 'age': y_age}, criteria, weights)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        bs = images.size(0)\n",
    "        run_loss += loss.item() * bs;  \n",
    "        total += bs\n",
    "        correct_label   += (outputs['label'].argmax(1) == y_lbl).sum().item()\n",
    "        correct_variety += (outputs['variety'].argmax(1) == y_var).sum().item()\n",
    "    return run_loss / total, correct_label / total, correct_variety / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criteria, device, weights=None):\n",
    "    model.eval()\n",
    "    run_loss= 0.0\n",
    "    correct_label, correct_variety, total, mae_age = 0, 0, 0, 0.0\n",
    "    for images, y_var, y_age, y_lbl in loader:\n",
    "        images = images.to(device); y_lbl = y_lbl.to(device); y_var = y_var.to(device); y_age = y_age.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = compute_multitask_loss(outputs, {'label': y_lbl, 'variety': y_var, 'age': y_age}, criteria, weights)\n",
    "        bs = images.size(0)\n",
    "        run_loss += loss.item() * bs; total += bs\n",
    "        correct_label   += (outputs['label'].argmax(1) == y_lbl).sum().item()\n",
    "        correct_variety += (outputs['variety'].argmax(1) == y_var).sum().item()\n",
    "        mae_age         += torch.abs(outputs['age'] - y_age).sum().item()\n",
    "    return run_loss / total, correct_label / total, correct_variety / total, mae_age / total\n",
    "\n",
    "def train_and_evaluate(model, train_loader, valid_loader, optimizer, criterion, loss_weights=None, num_epochs=10):\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Train for one epoch\n",
    "        tr_loss, tr_acc_lbl, tr_acc_var = train_one_epoch(model, train_loader, optimizer, criterion, device, weights=loss_weights)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        va_loss, va_acc_lbl, va_acc_var, va_mae_age = evaluate(model, valid_loader, criterion, device, weights=loss_weights)\n",
    "        print(f\"Epoch {epoch:02d} | train loss={tr_loss:.4f} tr_acc_label={tr_acc_lbl:.3f} tr_acc_variety={tr_acc_var:.3f} | valid loss={va_loss:.4f} va_acc_label={va_acc_lbl:.3f} va_acc_variety={va_acc_var:.3f} va_mae_age={va_mae_age:.3f}\")\n",
    "\n",
    "train_and_evaluate(model, train_loader, valid_loader, optimizer, criterion, loss_weights=loss_weights, num_epochs=NUM_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72f395f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=1.3664 acc_label=0.744 acc_variety=0.828\n"
     ]
    }
   ],
   "source": [
    "def eval(loader=test_loader):\n",
    "    run_loss, correct_label, correct_variety, mae_age = evaluate(model, loader, criterion, device, weights=loss_weights)\n",
    "    print(f\"loss={run_loss:.4f} acc_label={correct_label:.3f} acc_variety={correct_variety:.3f}\")\n",
    "\n",
    "eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b06063",
   "metadata": {},
   "source": [
    "### (Optional) Fine-tune the whole network\n",
    "\n",
    "After warming up the head, unfreeze the backbone and fine-tune at a **smaller LR**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b3ec264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss=1.6699 tr_acc_label=0.680 tr_acc_variety=0.807 | valid loss=0.9981 va_acc_label=0.830 va_acc_variety=0.854 va_mae_age=0.508\n",
      "Epoch 02 | train loss=1.1864 tr_acc_label=0.774 tr_acc_variety=0.859 | valid loss=0.7778 va_acc_label=0.861 va_acc_variety=0.912 va_mae_age=0.468\n",
      "Epoch 03 | train loss=0.9459 tr_acc_label=0.817 tr_acc_variety=0.893 | valid loss=0.6062 va_acc_label=0.895 va_acc_variety=0.927 va_mae_age=0.409\n",
      "Epoch 04 | train loss=0.8361 tr_acc_label=0.840 tr_acc_variety=0.914 | valid loss=0.5067 va_acc_label=0.905 va_acc_variety=0.953 va_mae_age=0.382\n",
      "Epoch 05 | train loss=0.6903 tr_acc_label=0.870 tr_acc_variety=0.932 | valid loss=0.4626 va_acc_label=0.913 va_acc_variety=0.952 va_mae_age=0.362\n",
      "CPU times: user 2min 8s, sys: 7.63 s, total: 2min 16s\n",
      "Wall time: 3min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# train the whole thing (unfreeze all layers)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "#make lr smaller for fine-tuning\n",
    "lr1=lr/100\n",
    "\n",
    "#change learning rate for fine-tuning\n",
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = lr1\n",
    "    \n",
    "train_and_evaluate(model, train_loader, valid_loader, optimizer, criterion, loss_weights=loss_weights, num_epochs=NUM_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a433be9f",
   "metadata": {},
   "source": [
    "## 7) Save / Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "829b6ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to kaggle_paddy_timm_multihead.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1223465/3246647191.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('kaggle_paddy_timm_multihead.pth', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.save(model.state_dict(), 'kaggle_paddy_timm_multihead.pth')\n",
    "print('Saved to kaggle_paddy_timm_multihead.pth')\n",
    "model.load_state_dict(torch.load('kaggle_paddy_timm_multihead.pth', map_location=device))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b291e5f",
   "metadata": {},
   "source": [
    "## 8) Evaluation & Confusion Matrix\n",
    "\n",
    "Evaluate the model on the test_loader.  This is the **only** time the model sees this loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef2af28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (6x6 for PaddyDoctor):\n",
      "[[ 45   1   0   2   3   0   1   2   0   3]\n",
      " [  0  42   0   0   1   0   0   0   0   1]\n",
      " [  0   0  36   0   0   0   0   0   3   0]\n",
      " [  1   2   0 145   2   0   0   3   0  11]\n",
      " [  2   2   0   2  91   0   0   0   2   0]\n",
      " [  0   0   1   0   0 142   0   0   0   0]\n",
      " [  2   0   0   3   2   1  51   0   2   6]\n",
      " [  1   1   0   3   0   0   0 129   8   3]\n",
      " [  0   0   3   0   0   0   0   3 168   0]\n",
      " [  0   0   0   0   1   0   2   3   1 101]]\n",
      "\n",
      "Per-class (precision, recall, f1):\n",
      "bacterial_leaf_blight: P=0.882 R=0.789 F1=0.833\n",
      "bacterial_leaf_streak: P=0.875 R=0.955 F1=0.913\n",
      "bacterial_panicle_blight: P=0.900 R=0.923 F1=0.911\n",
      "     blast: P=0.935 R=0.884 F1=0.909\n",
      "brown_spot: P=0.910 R=0.919 F1=0.915\n",
      "dead_heart: P=0.993 R=0.993 F1=0.993\n",
      "downy_mildew: P=0.944 R=0.761 F1=0.843\n",
      "     hispa: P=0.921 R=0.890 F1=0.905\n",
      "    normal: P=0.913 R=0.966 F1=0.939\n",
      "    tungro: P=0.808 R=0.935 F1=0.867\n",
      "\n",
      "Macro avg: P=0.908 R=0.901 F1=0.903\n",
      "Overall Accuracy: 0.913\n"
     ]
    }
   ],
   "source": [
    "LOADER=test_loader\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_all_preds_targets(model, loader):\n",
    "    model.eval()\n",
    "    preds, targs = [], []\n",
    "    for xb,_,_,yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "\n",
    "        #just want the label preds\n",
    "        preds.append(logits['label'].argmax(1).cpu().numpy())\n",
    "        targs.append(yb.numpy())\n",
    "    return np.concatenate(preds), np.concatenate(targs)\n",
    "\n",
    "preds, targs = get_all_preds_targets(model, LOADER)\n",
    "\n",
    "# Confusion matrix\n",
    "num_classes = len(class_names)\n",
    "cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "for t, p in zip(targs, preds):\n",
    "    cm[t, p] += 1\n",
    "\n",
    "print('Confusion Matrix (6x6 for PaddyDoctor):')\n",
    "print(cm)\n",
    "\n",
    "# Per-class metrics\n",
    "per_class = []\n",
    "for k in range(num_classes):\n",
    "    TP = cm[k,k]\n",
    "    FP = cm[:,k].sum() - TP\n",
    "    FN = cm[k,:].sum() - TP\n",
    "    TN = cm.sum() - TP - FP - FN\n",
    "    prec = TP/(TP+FP) if (TP+FP)>0 else 0.0\n",
    "    rec  = TP/(TP+FN) if (TP+FN)>0 else 0.0\n",
    "    f1   = (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "    per_class.append((prec, rec, f1))\n",
    "\n",
    "macro_p = float(np.mean([p for p,_,_ in per_class]))\n",
    "macro_r = float(np.mean([r for _,r,_ in per_class]))\n",
    "macro_f = float(np.mean([f for _,_,f in per_class]))\n",
    "overall_acc = float((preds == targs).mean())\n",
    "\n",
    "print('\\nPer-class (precision, recall, f1):')\n",
    "for name,(p,r,f) in zip(class_names, per_class):\n",
    "    print(f'{name:>10s}: P={p:.3f} R={r:.3f} F1={f:.3f}')\n",
    "print(f\"\\nMacro avg: P={macro_p:.3f} R={macro_r:.3f} F1={macro_f:.3f}\")\n",
    "print(f'Overall Accuracy: {overall_acc:.3f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d6533b",
   "metadata": {},
   "source": [
    "\n",
    "# Paddy Disease Classification — **PyTorch + timm** and Multi‑Head model\n",
    "\n",
    "This notebook demonstrates a **multi‑task** setup using **PyTorch** and and **[timm](https://github.com/huggingface/pytorch-image-models)** (Torch Image Models) for the Kaggle **Paddy** dataset.\n",
    "\n",
    "1) Set up the environment and choose a timm backbone (`convnext_tiny`).  \n",
    "2) Build a **custom Dataset** reading `train.csv` (`image_id`, `label`, `variety`, `age`).  \n",
    "   - Images are stored in subfolders named by **label** (e.g., `train/<label>/<image_id>`).  \n",
    "   - Each sample returns a tuple: **`(image_tensor, variety_idx, age_float, label_idx)`** as requested.  \n",
    "3) Create DataLoaders with timm‑compatible transforms.  \n",
    "4) Define a **multi‑head model**:  \n",
    "   - Head A → **disease label** classification  \n",
    "   - Head B → **variety** classification  \n",
    "   - Head R → **age** regression  \n",
    "5) Train and evaluate with a minimal, well‑commented loop.\n",
    "\n",
    "> **Note:** Point `DATA_DIR` to your local Kaggle Paddy dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e712e2b6",
   "metadata": {},
   "source": [
    "## 1) Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de007fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If timm isn't installed, uncomment:\n",
    "# !pip install timm --quiet\n",
    "\n",
    "import os, random, math, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import timm\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "DATA_DIR = Path(\"./data/\")  \n",
    "TRAIN_CSV = DATA_DIR / 'train.csv'\n",
    "TRAIN_IMG_ROOT = DATA_DIR   / 'train_images'\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "#this makes sure that colab instances running this notebook can get to included utils_kp.py\n",
    "try:\n",
    "    import utils_kp as ut\n",
    "except ModuleNotFoundError:\n",
    "    !wget https://raw.githubusercontent.com/CNUClasses/471_code/master/week6/utils_kp.py\n",
    "import utils_kp as ut\n",
    " \n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# lr=0.01 #start with this one (its too high see lr finder below)\n",
    "lr=2e-3 #lr finder approved this one\n",
    "NUM_EPOCHS=25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9cb6c1",
   "metadata": {},
   "source": [
    "## 2) Custom Dataset (returns `(image, variety, age, label)`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02edd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# see utils_kp.py for PaddyDataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6049e",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Create datasets with timm transforms \n",
    "\n",
    "Not going to use ImageFolder dataset.  Instead use PaddyMultitaskDataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a14265f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'crop_mode': 'center',\n",
      "    'crop_pct': 0.875,\n",
      "    'input_size': (3, 224, 224),\n",
      "    'interpolation': 'bicubic',\n",
      "    'mean': (0.485, 0.456, 0.406),\n",
      "    'std': (0.229, 0.224, 0.225)}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# MODEL_NAME = \"resnet18\"  # try: 'efficientnet_b0', 'convnext_tiny', 'mobilenetv3_large_100', ...\n",
    "MODEL_NAME = 'convnext_tiny'\n",
    "config = timm.data.resolve_data_config({}, model=MODEL_NAME)\n",
    "train_tfms = timm.data.create_transform(**config, is_training=True, hflip=0.5, auto_augment=None)\n",
    "valid_tfms = timm.data.create_transform(**config, is_training=False)\n",
    "\n",
    "pp.pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0600bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label classes: ['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro']\n",
      "Variety classes: ['ADT45', 'AndraPonni', 'AtchayaPonni', 'IR20', 'KarnatakaPonni', 'Onthanel', 'Ponni', 'RR', 'Surya', 'Zonal']\n",
      "Train/Valid/test sizes: 8327 1040 1040\n"
     ]
    }
   ],
   "source": [
    "#get the split datasets\n",
    "train_df, valid_df, test_df=ut.getdataframes(DATA_DIR, TRAIN_CSV, TRAIN_IMG_ROOT, valid_pct=0.1, test_pct=0.1,verbose=True )\n",
    "\n",
    "train_ds = ut.PaddyMultitaskDataset(train_df, TRAIN_IMG_ROOT, transform=train_tfms)\n",
    "valid_ds = ut.PaddyMultitaskDataset(valid_df,   TRAIN_IMG_ROOT, transform=valid_tfms)\n",
    "test_ds  = ut.PaddyMultitaskDataset(test_df,  TRAIN_IMG_ROOT, transform=valid_tfms)\n",
    "\n",
    "print('Label classes:', train_ds.labels)\n",
    "print('Variety classes:', train_ds.varieties)\n",
    "print('Train/Valid/test sizes:', len(train_ds), len(valid_ds), len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e93f40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bacterial_leaf_blight',\n",
       " 'bacterial_leaf_streak',\n",
       " 'bacterial_panicle_blight',\n",
       " 'blast',\n",
       " 'brown_spot',\n",
       " 'dead_heart',\n",
       " 'downy_mildew',\n",
       " 'hispa',\n",
       " 'normal',\n",
       " 'tungro']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names=train_ds.labels\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f6870",
   "metadata": {},
   "source": [
    "## 3) Create Dataloaders\n",
    "\n",
    "- Shuffle the training loader; keep validation loader deterministic.  \n",
    "- Adjust `BATCH_SIZE` to fit your GPU/CPU memory.\n",
    "\n",
    "This is a datascience competition:<br>\n",
    "the train_images folder contains images with class membership info (in the train.csv file).<br>\n",
    "the test_images folder contains images that your model infers membership on.  These inferences are bundled into a file (see sample_submission.csv) which is submitted for ranking  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0afefd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes: torch.Size([256, 3, 224, 224]) torch.Size([256]) torch.Size([256]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "xb, y_var, y_age, y_lbl = next(iter(train_loader))\n",
    "print('Batch shapes:', xb.shape, y_var.shape, y_age.shape, y_lbl.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567303c6",
   "metadata": {},
   "source": [
    "## 4) Load Multi‑head timm Model (Transfer Learning with **timm**)\n",
    "\n",
    "- Create a **pretrained** model with 3 heads, one to predict label, one to predict variety, and one to predict age\n",
    "- **Warm-up:** freeze backbone; train the classifier head first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "816197a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params (heads only): 16149\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MultiHeadNet(nn.Module):\n",
    "    \"\"\"\n",
    "    MultiHeadNet: a simple multi-task head on top of a timm backbone.\n",
    "\n",
    "    Architecture:\n",
    "     - backbone (timm model, num_classes=0, global_pool='avg') produces pooled feature vector of shape (B, feat_dim)\n",
    "     - head_label   : Linear(feat_dim -> num_label_classes)    -> classification logits for disease label\n",
    "     - head_variety : Linear(feat_dim -> num_variety_classes)  -> classification logits for variety\n",
    "     - head_age     : Linear(feat_dim -> 1)                    -> scalar age prediction (regression)\n",
    "\n",
    "    Forward input:\n",
    "     - x : image tensor of shape (B, 3, H, W)\n",
    "\n",
    "    Forward output:\n",
    "     - dict with keys 'label', 'variety', 'age'\n",
    "        - 'label'   : Tensor (B, num_label_classes)  (use CrossEntropyLoss)\n",
    "        - 'variety' : Tensor (B, num_variety_classes) (use CrossEntropyLoss)\n",
    "        - 'age'     : Tensor (B,)                     (use MSELoss or L1)\n",
    "\n",
    "    Notes:\n",
    "     - The backbone is created in __init__ below; you can freeze it after instantiation:\n",
    "         for p in model.backbone.parameters(): p.requires_grad = False\n",
    "     - This class is intended for transfer learning: warm up heads first, then optionally fine-tune backbone.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, num_label_classes: int, num_variety_classes: int, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n",
    "        feat_dim = self.backbone.num_features\n",
    "        self.head_label   = nn.Linear(feat_dim, num_label_classes)\n",
    "        self.head_variety = nn.Linear(feat_dim, num_variety_classes)\n",
    "        self.head_age     = nn.Linear(feat_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        logits_label   = self.head_label(feats)\n",
    "        logits_variety = self.head_variety(feats)\n",
    "        age_pred       = self.head_age(feats).squeeze(1)\n",
    "        return {'label': logits_label, 'variety': logits_variety, 'age': age_pred}  #returns a dictionary of outputs\n",
    "\n",
    "model = MultiHeadNet(MODEL_NAME, train_ds.num_label_classes, train_ds.num_variety_classes, pretrained=True).to(device)\n",
    "\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "print('Trainable params (heads only):', sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a411ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see the model structure\n",
    "# print(model.backbone)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee8d2803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure just training the last layer\n",
    "# for name, p in model.named_parameters():\n",
    "#     print (f'Name={name},p.shape={p.shape}, p.requires_grad = {p.requires_grad}')\n",
    "\n",
    "#stopped here 10/1/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cdb817",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Optimizer & loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36a239fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = {\n",
    "    'label':   nn.CrossEntropyLoss(),\n",
    "    'variety': nn.CrossEntropyLoss(),\n",
    "    'age':     nn.MSELoss()\n",
    "}\n",
    "loss_weights = {'label': 1.0, 'variety': 0.7, 'age': 0.5}\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c23d6",
   "metadata": {},
   "source": [
    "# 6) Training & validation loop\n",
    "\n",
    "- Log train/valid **loss** and **accuracy** per epoch.  \n",
    "- warm up heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "087df5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss=2.8847 tr_acc_label=0.443 tr_acc_variety=0.680 | valid loss=2.0369 va_acc_label=0.622 va_acc_variety=0.714 va_mae_age=0.696\n",
      "Epoch 02 | train loss=2.0492 tr_acc_label=0.596 tr_acc_variety=0.752 | valid loss=1.7100 va_acc_label=0.643 va_acc_variety=0.751 va_mae_age=0.585\n",
      "Epoch 03 | train loss=1.8736 tr_acc_label=0.628 tr_acc_variety=0.779 | valid loss=1.5222 va_acc_label=0.714 va_acc_variety=0.802 va_mae_age=0.566\n",
      "Epoch 04 | train loss=1.7485 tr_acc_label=0.654 tr_acc_variety=0.797 | valid loss=1.4017 va_acc_label=0.736 va_acc_variety=0.809 va_mae_age=0.571\n",
      "Epoch 05 | train loss=1.6760 tr_acc_label=0.670 tr_acc_variety=0.808 | valid loss=1.3235 va_acc_label=0.761 va_acc_variety=0.826 va_mae_age=0.548\n",
      "Epoch 06 | train loss=1.6003 tr_acc_label=0.685 tr_acc_variety=0.819 | valid loss=1.2762 va_acc_label=0.775 va_acc_variety=0.840 va_mae_age=0.560\n",
      "Epoch 07 | train loss=1.5610 tr_acc_label=0.692 tr_acc_variety=0.823 | valid loss=1.2466 va_acc_label=0.774 va_acc_variety=0.852 va_mae_age=0.542\n",
      "Epoch 08 | train loss=1.5327 tr_acc_label=0.704 tr_acc_variety=0.827 | valid loss=1.1919 va_acc_label=0.793 va_acc_variety=0.855 va_mae_age=0.579\n",
      "Epoch 09 | train loss=1.4826 tr_acc_label=0.712 tr_acc_variety=0.837 | valid loss=1.1263 va_acc_label=0.792 va_acc_variety=0.856 va_mae_age=0.529\n",
      "Epoch 10 | train loss=1.4618 tr_acc_label=0.713 tr_acc_variety=0.841 | valid loss=1.1204 va_acc_label=0.804 va_acc_variety=0.869 va_mae_age=0.546\n",
      "Epoch 11 | train loss=1.4394 tr_acc_label=0.714 tr_acc_variety=0.844 | valid loss=1.0947 va_acc_label=0.810 va_acc_variety=0.875 va_mae_age=0.529\n",
      "Epoch 12 | train loss=1.4045 tr_acc_label=0.719 tr_acc_variety=0.850 | valid loss=1.0648 va_acc_label=0.812 va_acc_variety=0.876 va_mae_age=0.525\n",
      "Epoch 13 | train loss=1.4010 tr_acc_label=0.724 tr_acc_variety=0.848 | valid loss=1.0401 va_acc_label=0.821 va_acc_variety=0.879 va_mae_age=0.517\n",
      "Epoch 14 | train loss=1.3849 tr_acc_label=0.733 tr_acc_variety=0.850 | valid loss=1.0649 va_acc_label=0.807 va_acc_variety=0.870 va_mae_age=0.519\n",
      "Epoch 15 | train loss=1.3590 tr_acc_label=0.731 tr_acc_variety=0.855 | valid loss=1.0257 va_acc_label=0.809 va_acc_variety=0.876 va_mae_age=0.524\n",
      "Epoch 16 | train loss=1.3431 tr_acc_label=0.733 tr_acc_variety=0.862 | valid loss=1.0037 va_acc_label=0.824 va_acc_variety=0.876 va_mae_age=0.512\n",
      "Epoch 17 | train loss=1.3286 tr_acc_label=0.737 tr_acc_variety=0.861 | valid loss=1.0656 va_acc_label=0.812 va_acc_variety=0.891 va_mae_age=0.580\n",
      "Epoch 18 | train loss=1.3011 tr_acc_label=0.742 tr_acc_variety=0.869 | valid loss=0.9844 va_acc_label=0.818 va_acc_variety=0.890 va_mae_age=0.510\n",
      "Epoch 19 | train loss=1.3267 tr_acc_label=0.739 tr_acc_variety=0.860 | valid loss=0.9771 va_acc_label=0.812 va_acc_variety=0.895 va_mae_age=0.524\n",
      "Epoch 20 | train loss=1.3276 tr_acc_label=0.741 tr_acc_variety=0.857 | valid loss=1.0357 va_acc_label=0.786 va_acc_variety=0.870 va_mae_age=0.512\n",
      "Epoch 21 | train loss=1.3131 tr_acc_label=0.736 tr_acc_variety=0.864 | valid loss=0.9404 va_acc_label=0.834 va_acc_variety=0.902 va_mae_age=0.516\n",
      "Epoch 22 | train loss=1.2948 tr_acc_label=0.745 tr_acc_variety=0.868 | valid loss=0.9375 va_acc_label=0.838 va_acc_variety=0.887 va_mae_age=0.507\n",
      "Epoch 23 | train loss=1.2741 tr_acc_label=0.752 tr_acc_variety=0.869 | valid loss=0.9520 va_acc_label=0.829 va_acc_variety=0.898 va_mae_age=0.522\n",
      "Epoch 24 | train loss=1.2611 tr_acc_label=0.753 tr_acc_variety=0.869 | valid loss=0.9455 va_acc_label=0.829 va_acc_variety=0.900 va_mae_age=0.505\n",
      "Epoch 25 | train loss=1.2674 tr_acc_label=0.749 tr_acc_variety=0.864 | valid loss=0.9210 va_acc_label=0.829 va_acc_variety=0.895 va_mae_age=0.507\n",
      "CPU times: user 3min 20s, sys: 40.2 s, total: 4min 1s\n",
      "Wall time: 17min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "def compute_multitask_loss(outputs, targets, criteria, weights=None):\n",
    "    total = 0.0\n",
    "    for k in ['label', 'variety', 'age']:\n",
    "        w = 1.0 if (weights is None) else weights[k]\n",
    "        # print(f'for key={k}, loss={criteria[k](outputs[k], targets[k])}')\n",
    "        total = total + w * criteria[k](outputs[k], targets[k])\n",
    "\n",
    "    #maybe return the average?\n",
    "    return total\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criteria, device, weights=None):\n",
    "    model.train()\n",
    "    run_loss= 0.0\n",
    "    correct_label, correct_variety, total = 0, 0, 0\n",
    "    for images, y_var, y_age, y_lbl in loader:\n",
    "        images = images.to(device); y_lbl = y_lbl.to(device); y_var = y_var.to(device); y_age = y_age.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = model(images)  #only feeding in images\n",
    "        loss = compute_multitask_loss(outputs, {'label': y_lbl, 'variety': y_var, 'age': y_age}, criteria, weights)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        bs = images.size(0)\n",
    "        run_loss += loss.item() * bs;  \n",
    "        total += bs\n",
    "        correct_label   += (outputs['label'].argmax(1) == y_lbl).sum().item()\n",
    "        correct_variety += (outputs['variety'].argmax(1) == y_var).sum().item()\n",
    "    return run_loss / total, correct_label / total, correct_variety / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criteria, device, weights=None):\n",
    "    model.eval()\n",
    "    run_loss= 0.0\n",
    "    correct_label, correct_variety, total, mae_age = 0, 0, 0, 0.0\n",
    "    for images, y_var, y_age, y_lbl in loader:\n",
    "        images = images.to(device); y_lbl = y_lbl.to(device); y_var = y_var.to(device); y_age = y_age.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = compute_multitask_loss(outputs, {'label': y_lbl, 'variety': y_var, 'age': y_age}, criteria, weights)\n",
    "        bs = images.size(0)\n",
    "        run_loss += loss.item() * bs; total += bs\n",
    "        correct_label   += (outputs['label'].argmax(1) == y_lbl).sum().item()\n",
    "        correct_variety += (outputs['variety'].argmax(1) == y_var).sum().item()\n",
    "        mae_age         += torch.abs(outputs['age'] - y_age).sum().item()\n",
    "    return run_loss / total, correct_label / total, correct_variety / total, mae_age / total\n",
    "\n",
    "def train_and_evaluate(model, train_loader, valid_loader, optimizer, criterion, loss_weights=None, num_epochs=10):\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Train for one epoch\n",
    "        tr_loss, tr_acc_lbl, tr_acc_var = train_one_epoch(model, train_loader, optimizer, criterion, device, weights=loss_weights)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        va_loss, va_acc_lbl, va_acc_var, va_mae_age = evaluate(model, valid_loader, criterion, device, weights=loss_weights)\n",
    "        print(f\"Epoch {epoch:02d} | train loss={tr_loss:.4f} tr_acc_label={tr_acc_lbl:.3f} tr_acc_variety={tr_acc_var:.3f} | valid loss={va_loss:.4f} va_acc_label={va_acc_lbl:.3f} va_acc_variety={va_acc_var:.3f} va_mae_age={va_mae_age:.3f}\")\n",
    "\n",
    "train_and_evaluate(model, train_loader, valid_loader, optimizer, criterion, loss_weights=loss_weights, num_epochs=NUM_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72f395f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=0.9259 acc_label=0.838 acc_variety=0.904\n"
     ]
    }
   ],
   "source": [
    "def eval(loader=test_loader):\n",
    "    run_loss, correct_label, correct_variety, mae_age = evaluate(model, loader, criterion, device, weights=loss_weights)\n",
    "    print(f\"loss={run_loss:.4f} acc_label={correct_label:.3f} acc_variety={correct_variety:.3f}\")\n",
    "\n",
    "eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b06063",
   "metadata": {},
   "source": [
    "### (Optional) Fine-tune the whole network\n",
    "\n",
    "After warming up the head, unfreeze the backbone and fine-tune at a **smaller LR**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b3ec264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss=5.6709 tr_acc_label=0.190 tr_acc_variety=0.641 | valid loss=3.5787 va_acc_label=0.163 va_acc_variety=0.653 va_mae_age=0.846\n",
      "Epoch 02 | train loss=3.5022 tr_acc_label=0.201 tr_acc_variety=0.673 | valid loss=3.4605 va_acc_label=0.156 va_acc_variety=0.653 va_mae_age=0.808\n",
      "Epoch 03 | train loss=3.2795 tr_acc_label=0.259 tr_acc_variety=0.681 | valid loss=3.0166 va_acc_label=0.316 va_acc_variety=0.684 va_mae_age=0.782\n",
      "Epoch 04 | train loss=2.6709 tr_acc_label=0.440 tr_acc_variety=0.722 | valid loss=2.0289 va_acc_label=0.639 va_acc_variety=0.721 va_mae_age=0.691\n",
      "Epoch 05 | train loss=1.9371 tr_acc_label=0.633 tr_acc_variety=0.768 | valid loss=1.3277 va_acc_label=0.771 va_acc_variety=0.804 va_mae_age=0.575\n",
      "Epoch 06 | train loss=1.3817 tr_acc_label=0.738 tr_acc_variety=0.828 | valid loss=0.8417 va_acc_label=0.843 va_acc_variety=0.901 va_mae_age=0.475\n",
      "Epoch 07 | train loss=1.0308 tr_acc_label=0.802 tr_acc_variety=0.889 | valid loss=0.6673 va_acc_label=0.876 va_acc_variety=0.925 va_mae_age=0.383\n",
      "Epoch 08 | train loss=0.8131 tr_acc_label=0.841 tr_acc_variety=0.919 | valid loss=0.5300 va_acc_label=0.893 va_acc_variety=0.955 va_mae_age=0.363\n",
      "Epoch 09 | train loss=0.6566 tr_acc_label=0.870 tr_acc_variety=0.934 | valid loss=0.3973 va_acc_label=0.926 va_acc_variety=0.968 va_mae_age=0.303\n",
      "Epoch 10 | train loss=0.5749 tr_acc_label=0.888 tr_acc_variety=0.945 | valid loss=0.3672 va_acc_label=0.938 va_acc_variety=0.965 va_mae_age=0.364\n",
      "Epoch 11 | train loss=0.4518 tr_acc_label=0.914 tr_acc_variety=0.965 | valid loss=0.2718 va_acc_label=0.948 va_acc_variety=0.980 va_mae_age=0.273\n",
      "Epoch 12 | train loss=0.4258 tr_acc_label=0.919 tr_acc_variety=0.963 | valid loss=0.2860 va_acc_label=0.960 va_acc_variety=0.981 va_mae_age=0.384\n",
      "Epoch 13 | train loss=0.3855 tr_acc_label=0.929 tr_acc_variety=0.968 | valid loss=0.2637 va_acc_label=0.950 va_acc_variety=0.982 va_mae_age=0.283\n",
      "Epoch 14 | train loss=0.3655 tr_acc_label=0.926 tr_acc_variety=0.970 | valid loss=0.2234 va_acc_label=0.958 va_acc_variety=0.987 va_mae_age=0.252\n",
      "Epoch 15 | train loss=0.3064 tr_acc_label=0.940 tr_acc_variety=0.976 | valid loss=0.2474 va_acc_label=0.956 va_acc_variety=0.986 va_mae_age=0.232\n",
      "Epoch 16 | train loss=0.2997 tr_acc_label=0.945 tr_acc_variety=0.975 | valid loss=0.2098 va_acc_label=0.961 va_acc_variety=0.985 va_mae_age=0.219\n",
      "Epoch 17 | train loss=0.2726 tr_acc_label=0.947 tr_acc_variety=0.976 | valid loss=0.1817 va_acc_label=0.964 va_acc_variety=0.986 va_mae_age=0.194\n",
      "Epoch 18 | train loss=0.2460 tr_acc_label=0.949 tr_acc_variety=0.983 | valid loss=0.1664 va_acc_label=0.969 va_acc_variety=0.986 va_mae_age=0.202\n",
      "Epoch 19 | train loss=0.2485 tr_acc_label=0.951 tr_acc_variety=0.982 | valid loss=0.1884 va_acc_label=0.969 va_acc_variety=0.983 va_mae_age=0.213\n",
      "Epoch 20 | train loss=0.2314 tr_acc_label=0.955 tr_acc_variety=0.982 | valid loss=0.1996 va_acc_label=0.959 va_acc_variety=0.985 va_mae_age=0.201\n",
      "Epoch 21 | train loss=0.2261 tr_acc_label=0.955 tr_acc_variety=0.983 | valid loss=0.1626 va_acc_label=0.966 va_acc_variety=0.991 va_mae_age=0.203\n",
      "Epoch 22 | train loss=0.2135 tr_acc_label=0.958 tr_acc_variety=0.985 | valid loss=0.1904 va_acc_label=0.963 va_acc_variety=0.989 va_mae_age=0.210\n",
      "Epoch 23 | train loss=0.2228 tr_acc_label=0.957 tr_acc_variety=0.982 | valid loss=0.1798 va_acc_label=0.968 va_acc_variety=0.988 va_mae_age=0.207\n",
      "Epoch 24 | train loss=0.2238 tr_acc_label=0.955 tr_acc_variety=0.982 | valid loss=0.1923 va_acc_label=0.965 va_acc_variety=0.985 va_mae_age=0.197\n",
      "Epoch 25 | train loss=0.2105 tr_acc_label=0.957 tr_acc_variety=0.984 | valid loss=0.1553 va_acc_label=0.970 va_acc_variety=0.987 va_mae_age=0.184\n",
      "CPU times: user 10min 45s, sys: 39 s, total: 11min 24s\n",
      "Wall time: 17min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# train the whole thing (unfreeze all layers)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "#make lr smaller for fine-tuning\n",
    "lr1=lr/10\n",
    "\n",
    "#reinitialize optimizer so we can train all parameters\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr1)\n",
    "\n",
    "train_and_evaluate(model, train_loader, valid_loader, optimizer, criterion, loss_weights=loss_weights, num_epochs=NUM_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a433be9f",
   "metadata": {},
   "source": [
    "## 7) Save / Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "829b6ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to kaggle_paddy_timm_multihead.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_977419/3246647191.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('kaggle_paddy_timm_multihead.pth', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.save(model.state_dict(), 'kaggle_paddy_timm_multihead.pth')\n",
    "print('Saved to kaggle_paddy_timm_multihead.pth')\n",
    "model.load_state_dict(torch.load('kaggle_paddy_timm_multihead.pth', map_location=device))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b291e5f",
   "metadata": {},
   "source": [
    "## 8) Evaluation & Confusion Matrix\n",
    "\n",
    "Evaluate the model on the test_loader.  This is the **only** time the model sees this loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef2af28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (6x6 for PaddyDoctor):\n",
      "[[ 56   0   0   0   0   0   0   1   0   0]\n",
      " [  0  43   0   1   0   0   0   0   0   0]\n",
      " [  0   0  38   0   0   1   0   0   0   0]\n",
      " [  3   0   0 161   0   0   0   0   0   0]\n",
      " [  0   1   0   1  97   0   0   0   0   0]\n",
      " [  0   0   1   0   0 142   0   0   0   0]\n",
      " [  0   0   0   4   0   0  61   0   0   2]\n",
      " [  0   0   0   1   0   0   1 141   2   0]\n",
      " [  0   0   0   3   0   0   0   0 171   0]\n",
      " [  0   0   0   1   0   0   2   1   0 104]]\n",
      "\n",
      "Per-class (precision, recall, f1):\n",
      "bacterial_leaf_blight: P=0.949 R=0.982 F1=0.966\n",
      "bacterial_leaf_streak: P=0.977 R=0.977 F1=0.977\n",
      "bacterial_panicle_blight: P=0.974 R=0.974 F1=0.974\n",
      "     blast: P=0.936 R=0.982 F1=0.958\n",
      "brown_spot: P=1.000 R=0.980 F1=0.990\n",
      "dead_heart: P=0.993 R=0.993 F1=0.993\n",
      "downy_mildew: P=0.953 R=0.910 F1=0.931\n",
      "     hispa: P=0.986 R=0.972 F1=0.979\n",
      "    normal: P=0.988 R=0.983 F1=0.986\n",
      "    tungro: P=0.981 R=0.963 F1=0.972\n",
      "\n",
      "Macro avg: P=0.974 R=0.972 F1=0.973\n",
      "Overall Accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "LOADER=test_loader\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_all_preds_targets(model, loader):\n",
    "    model.eval()\n",
    "    preds, targs = [], []\n",
    "    for xb,_,_,yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "\n",
    "        #just want the label preds\n",
    "        preds.append(logits['label'].argmax(1).cpu().numpy())\n",
    "        targs.append(yb.numpy())\n",
    "    return np.concatenate(preds), np.concatenate(targs)\n",
    "\n",
    "preds, targs = get_all_preds_targets(model, LOADER)\n",
    "\n",
    "# Confusion matrix\n",
    "num_classes = len(class_names)\n",
    "cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "for t, p in zip(targs, preds):\n",
    "    cm[t, p] += 1\n",
    "\n",
    "print('Confusion Matrix (6x6 for PaddyDoctor):')\n",
    "print(cm)\n",
    "\n",
    "# Per-class metrics\n",
    "per_class = []\n",
    "for k in range(num_classes):\n",
    "    TP = cm[k,k]\n",
    "    FP = cm[:,k].sum() - TP\n",
    "    FN = cm[k,:].sum() - TP\n",
    "    TN = cm.sum() - TP - FP - FN\n",
    "    prec = TP/(TP+FP) if (TP+FP)>0 else 0.0\n",
    "    rec  = TP/(TP+FN) if (TP+FN)>0 else 0.0\n",
    "    f1   = (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "    per_class.append((prec, rec, f1))\n",
    "\n",
    "macro_p = float(np.mean([p for p,_,_ in per_class]))\n",
    "macro_r = float(np.mean([r for _,r,_ in per_class]))\n",
    "macro_f = float(np.mean([f for _,_,f in per_class]))\n",
    "overall_acc = float((preds == targs).mean())\n",
    "\n",
    "print('\\nPer-class (precision, recall, f1):')\n",
    "for name,(p,r,f) in zip(class_names, per_class):\n",
    "    print(f'{name:>10s}: P={p:.3f} R={r:.3f} F1={f:.3f}')\n",
    "print(f\"\\nMacro avg: P={macro_p:.3f} R={macro_r:.3f} F1={macro_f:.3f}\")\n",
    "print(f'Overall Accuracy: {overall_acc:.3f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

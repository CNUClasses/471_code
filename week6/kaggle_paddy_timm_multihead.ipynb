{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d6533b",
   "metadata": {},
   "source": [
    "\n",
    "# Paddy Disease Classification — **PyTorch + timm** and Multi‑Head model\n",
    "\n",
    "This notebook demonstrates a **multi‑task** setup using **PyTorch** and and **[timm](https://github.com/huggingface/pytorch-image-models)** (Torch Image Models) for the Kaggle **Paddy** dataset.\n",
    "\n",
    "1) Set up the environment and choose a timm backbone (`convnext_tiny`).  \n",
    "2) Build a **custom Dataset** reading `train.csv` (`image_id`, `label`, `variety`, `age`).  \n",
    "   - Images are stored in subfolders named by **label** (e.g., `train/<label>/<image_id>`).  \n",
    "   - Each sample returns a tuple: **`(image_tensor, variety_idx, age_float, label_idx)`** as requested.  \n",
    "3) Create DataLoaders with timm‑compatible transforms.  \n",
    "4) Define a **multi‑head model**:  \n",
    "   - Head A → **disease label** classification  \n",
    "   - Head B → **variety** classification  \n",
    "   - Head R → **age** regression  \n",
    "5) Train and evaluate with a minimal, well‑commented loop.\n",
    "\n",
    "> **Note:** Point `DATA_DIR` to your local Kaggle Paddy dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e712e2b6",
   "metadata": {},
   "source": [
    "## 1) Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de007fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If timm isn't installed, uncomment:\n",
    "# !pip install timm --quiet\n",
    "\n",
    "import os, random, math, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import timm\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "DATA_DIR = Path(\"./data/\")  \n",
    "TRAIN_CSV = DATA_DIR / 'train.csv'\n",
    "TRAIN_IMG_ROOT = DATA_DIR   / 'train_images'\n",
    "\n",
    "USE_IMAGEFOLDER = False             # set False to use CSV dataset class below\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9cb6c1",
   "metadata": {},
   "source": [
    "## 2) Custom Dataset (returns `(image, variety, age, label)`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02edd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PaddyMultitaskDataset(Dataset):\n",
    "    def __init__(self, csv_path: Path, img_root: Path, transform=None):\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        expected_cols = {'image_id', 'label', 'variety', 'age'}\n",
    "        missing = expected_cols - set(self.df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"CSV is missing columns: {missing}\")\n",
    "        self.img_root = Path(img_root)\n",
    "        self.transform = transform\n",
    "        self.labels = sorted(self.df['label'].astype(str).unique())\n",
    "        self.varieties = sorted(self.df['variety'].astype(str).unique())\n",
    "        self.label_to_idx = {s:i for i,s in enumerate(self.labels)}\n",
    "        self.variety_to_idx = {s:i for i,s in enumerate(self.varieties)}\n",
    "        self.df['age'] = pd.to_numeric(self.df['age'], errors='coerce')\n",
    "        if self.df['age'].isna().any():\n",
    "            med = float(self.df['age'].median())\n",
    "            self.df['age'] = self.df['age'].fillna(med)\n",
    "        self.num_label_classes = len(self.labels)\n",
    "        self.num_variety_classes = len(self.varieties)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_id = str(row['image_id'])\n",
    "        label_name = str(row['label'])\n",
    "        variety_name = str(row['variety'])\n",
    "        age_val = float(row['age'])\n",
    "\n",
    "        img_path = self.img_root / label_name / image_id\n",
    "        if not img_path.exists():\n",
    "            for ext in ('.jpg', '.jpeg', '.png', '.bmp'):\n",
    "                cand = img_path.with_suffix(ext)\n",
    "                if cand.exists():\n",
    "                    img_path = cand\n",
    "                    break\n",
    "        if not img_path.exists():\n",
    "            raise FileNotFoundError(f\"Image not found for row {idx}: {img_path}\")\n",
    "\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        y_label = self.label_to_idx[label_name]\n",
    "        y_var = self.variety_to_idx[variety_name]\n",
    "        y_age = age_val\n",
    "\n",
    "        return img, torch.tensor(y_var, dtype=torch.long), torch.tensor(y_age, dtype=torch.float32), torch.tensor(y_label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6049e",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Create datasets with timm transforms \n",
    "\n",
    "Not going to use ImageFolder dataset.  Instead use PaddyMultitaskDataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a14265f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# MODEL_NAME = \"resnet18\"  # try: 'efficientnet_b0', 'convnext_tiny', 'mobilenetv3_large_100', ...\n",
    "MODEL_NAME = 'convnext_tiny'\n",
    "config = timm.data.resolve_data_config({}, model=MODEL_NAME)\n",
    "train_tfms = timm.data.create_transform(**config, is_training=True, hflip=0.5, auto_augment=None)\n",
    "valid_tfms = timm.data.create_transform(**config, is_training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0600bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 8327, Valid: 1040, Test: 1040\n",
      "Label classes: ['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro']\n",
      "Variety classes: ['ADT45', 'AndraPonni', 'AtchayaPonni', 'IR20', 'KarnatakaPonni', 'Onthanel', 'Ponni', 'RR', 'Surya', 'Zonal']\n",
      "Train/Valid/test sizes: 8327 1040 1040\n"
     ]
    }
   ],
   "source": [
    "#split the dataset into train/validation/test sets\n",
    "df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "indices = np.arange(len(df))\n",
    "np.random.shuffle(indices) #shuffle the indexes before splitting\n",
    "ten_percent = int(len(indices) * 0.1)\n",
    "\n",
    "tst_idx = indices[ :ten_percent]\n",
    "val_idx = indices[ten_percent:2*ten_percent]\n",
    "trn_idx = indices[2*ten_percent: ]\n",
    "print(f\"Train: {len(trn_idx)}, Valid: {len(val_idx)}, Test: {len(tst_idx)}\")\n",
    "\n",
    "train_csv_tmp = DATA_DIR / 'train_split.csv'\n",
    "val_csv_tmp   = DATA_DIR / 'valid_split.csv'\n",
    "test_csv_tmp  = DATA_DIR / 'test_split.csv'\n",
    "df.iloc[tst_idx].to_csv(test_csv_tmp,  index=False)\n",
    "df.iloc[trn_idx].to_csv(train_csv_tmp, index=False)\n",
    "df.iloc[val_idx].to_csv(val_csv_tmp,   index=False)\n",
    "\n",
    "train_ds = PaddyMultitaskDataset(train_csv_tmp, TRAIN_IMG_ROOT, transform=train_tfms)\n",
    "valid_ds = PaddyMultitaskDataset(val_csv_tmp,   TRAIN_IMG_ROOT, transform=valid_tfms)\n",
    "test_ds  = PaddyMultitaskDataset(test_csv_tmp,  TRAIN_IMG_ROOT, transform=valid_tfms)\n",
    "\n",
    "#should be stratified split, but close enough for now\n",
    "# train_ds, valid_ds = random_split(full_ds, [n_train, n_valid], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print('Label classes:', train_ds.labels)\n",
    "print('Variety classes:', train_ds.varieties)\n",
    "print('Train/Valid/test sizes:', len(train_ds), len(valid_ds), len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e93f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=train_ds.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f6870",
   "metadata": {},
   "source": [
    "## 3) Create Dataloaders\n",
    "\n",
    "- Shuffle the training loader; keep validation loader deterministic.  \n",
    "- Adjust `BATCH_SIZE` to fit your GPU/CPU memory.\n",
    "\n",
    "This is a datascience competition:<br>\n",
    "the train_images folder contains images with class membership info (in the train.csv file).<br>\n",
    "the test_images folder contains images that your model infers membership on.  These inferences are bundled into a file (see sample_submission.csv) which is submitted for ranking  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0afefd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes: torch.Size([256, 3, 224, 224]) torch.Size([256]) torch.Size([256]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "xb, y_var, y_age, y_lbl = next(iter(train_loader))\n",
    "print('Batch shapes:', xb.shape, y_var.shape, y_age.shape, y_lbl.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567303c6",
   "metadata": {},
   "source": [
    "## 4) Load Multi‑head timm Model (Transfer Learning with **timm**)\n",
    "\n",
    "- Create a **pretrained** model with 3 heads, one to predict label, one to predict variety, and one to predict age\n",
    "- **Warm-up:** freeze backbone; train the classifier head first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "816197a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params (heads only): 16149\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MultiHeadNet(nn.Module):\n",
    "    def __init__(self, model_name: str, num_label_classes: int, num_variety_classes: int, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n",
    "        feat_dim = self.backbone.num_features\n",
    "        self.head_label   = nn.Linear(feat_dim, num_label_classes)\n",
    "        self.head_variety = nn.Linear(feat_dim, num_variety_classes)\n",
    "        self.head_age     = nn.Linear(feat_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        logits_label   = self.head_label(feats)\n",
    "        logits_variety = self.head_variety(feats)\n",
    "        age_pred       = self.head_age(feats).squeeze(1)\n",
    "        return {'label': logits_label, 'variety': logits_variety, 'age': age_pred}\n",
    "\n",
    "model = MultiHeadNet(MODEL_NAME, train_ds.num_label_classes, train_ds.num_variety_classes, pretrained=True).to(device)\n",
    "\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "print('Trainable params (heads only):', sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a411ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): ConvNeXtStage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (4): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (5): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (6): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (7): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (8): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_pre): Identity()\n",
       "  (head): NormMlpClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (pre_logits): Identity()\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee8d2803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name=backbone.stem.0.weight,p.shape=torch.Size([96, 3, 4, 4]), p.requires_grad = False\n",
      "Name=backbone.stem.0.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stem.1.weight,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stem.1.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.0.gamma,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.0.conv_dw.weight,p.shape=torch.Size([96, 1, 7, 7]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.0.conv_dw.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.0.norm.weight,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.0.norm.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.0.mlp.fc1.weight,p.shape=torch.Size([384, 96]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.0.mlp.fc1.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.0.mlp.fc2.weight,p.shape=torch.Size([96, 384]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.0.mlp.fc2.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.1.gamma,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.1.conv_dw.weight,p.shape=torch.Size([96, 1, 7, 7]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.1.conv_dw.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.1.norm.weight,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.1.norm.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.1.mlp.fc1.weight,p.shape=torch.Size([384, 96]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.1.mlp.fc1.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.1.mlp.fc2.weight,p.shape=torch.Size([96, 384]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.1.mlp.fc2.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.2.gamma,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.2.conv_dw.weight,p.shape=torch.Size([96, 1, 7, 7]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.2.conv_dw.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.2.norm.weight,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.2.norm.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.2.mlp.fc1.weight,p.shape=torch.Size([384, 96]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.2.mlp.fc1.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.2.mlp.fc2.weight,p.shape=torch.Size([96, 384]), p.requires_grad = False\n",
      "Name=backbone.stages.0.blocks.2.mlp.fc2.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stages.1.downsample.0.weight,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stages.1.downsample.0.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=backbone.stages.1.downsample.1.weight,p.shape=torch.Size([192, 96, 2, 2]), p.requires_grad = False\n",
      "Name=backbone.stages.1.downsample.1.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.0.gamma,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.0.conv_dw.weight,p.shape=torch.Size([192, 1, 7, 7]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.0.conv_dw.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.0.norm.weight,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.0.norm.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.0.mlp.fc1.weight,p.shape=torch.Size([768, 192]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.0.mlp.fc1.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.0.mlp.fc2.weight,p.shape=torch.Size([192, 768]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.0.mlp.fc2.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.1.gamma,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.1.conv_dw.weight,p.shape=torch.Size([192, 1, 7, 7]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.1.conv_dw.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.1.norm.weight,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.1.norm.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.1.mlp.fc1.weight,p.shape=torch.Size([768, 192]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.1.mlp.fc1.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.1.mlp.fc2.weight,p.shape=torch.Size([192, 768]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.1.mlp.fc2.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.2.gamma,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.2.conv_dw.weight,p.shape=torch.Size([192, 1, 7, 7]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.2.conv_dw.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.2.norm.weight,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.2.norm.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.2.mlp.fc1.weight,p.shape=torch.Size([768, 192]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.2.mlp.fc1.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.2.mlp.fc2.weight,p.shape=torch.Size([192, 768]), p.requires_grad = False\n",
      "Name=backbone.stages.1.blocks.2.mlp.fc2.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=backbone.stages.2.downsample.0.weight,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=backbone.stages.2.downsample.0.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=backbone.stages.2.downsample.1.weight,p.shape=torch.Size([384, 192, 2, 2]), p.requires_grad = False\n",
      "Name=backbone.stages.2.downsample.1.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.0.gamma,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.0.conv_dw.weight,p.shape=torch.Size([384, 1, 7, 7]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.0.conv_dw.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.0.norm.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.0.norm.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.0.mlp.fc1.weight,p.shape=torch.Size([1536, 384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.0.mlp.fc1.bias,p.shape=torch.Size([1536]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.0.mlp.fc2.weight,p.shape=torch.Size([384, 1536]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.0.mlp.fc2.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.1.gamma,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.1.conv_dw.weight,p.shape=torch.Size([384, 1, 7, 7]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.1.conv_dw.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.1.norm.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.1.norm.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.1.mlp.fc1.weight,p.shape=torch.Size([1536, 384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.1.mlp.fc1.bias,p.shape=torch.Size([1536]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.1.mlp.fc2.weight,p.shape=torch.Size([384, 1536]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.1.mlp.fc2.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.2.gamma,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.2.conv_dw.weight,p.shape=torch.Size([384, 1, 7, 7]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.2.conv_dw.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.2.norm.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.2.norm.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.2.mlp.fc1.weight,p.shape=torch.Size([1536, 384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.2.mlp.fc1.bias,p.shape=torch.Size([1536]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.2.mlp.fc2.weight,p.shape=torch.Size([384, 1536]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.2.mlp.fc2.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.3.gamma,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.3.conv_dw.weight,p.shape=torch.Size([384, 1, 7, 7]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.3.conv_dw.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.3.norm.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.3.norm.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.3.mlp.fc1.weight,p.shape=torch.Size([1536, 384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.3.mlp.fc1.bias,p.shape=torch.Size([1536]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.3.mlp.fc2.weight,p.shape=torch.Size([384, 1536]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.3.mlp.fc2.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.4.gamma,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.4.conv_dw.weight,p.shape=torch.Size([384, 1, 7, 7]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.4.conv_dw.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.4.norm.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.4.norm.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.4.mlp.fc1.weight,p.shape=torch.Size([1536, 384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.4.mlp.fc1.bias,p.shape=torch.Size([1536]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.4.mlp.fc2.weight,p.shape=torch.Size([384, 1536]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.4.mlp.fc2.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.5.gamma,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.5.conv_dw.weight,p.shape=torch.Size([384, 1, 7, 7]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.5.conv_dw.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.5.norm.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.5.norm.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.5.mlp.fc1.weight,p.shape=torch.Size([1536, 384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.5.mlp.fc1.bias,p.shape=torch.Size([1536]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.5.mlp.fc2.weight,p.shape=torch.Size([384, 1536]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.5.mlp.fc2.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.6.gamma,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.6.conv_dw.weight,p.shape=torch.Size([384, 1, 7, 7]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.6.conv_dw.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.6.norm.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.6.norm.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.6.mlp.fc1.weight,p.shape=torch.Size([1536, 384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.6.mlp.fc1.bias,p.shape=torch.Size([1536]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.6.mlp.fc2.weight,p.shape=torch.Size([384, 1536]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.6.mlp.fc2.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.7.gamma,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.7.conv_dw.weight,p.shape=torch.Size([384, 1, 7, 7]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.7.conv_dw.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.7.norm.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.7.norm.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.7.mlp.fc1.weight,p.shape=torch.Size([1536, 384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.7.mlp.fc1.bias,p.shape=torch.Size([1536]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.7.mlp.fc2.weight,p.shape=torch.Size([384, 1536]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.7.mlp.fc2.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.8.gamma,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.8.conv_dw.weight,p.shape=torch.Size([384, 1, 7, 7]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.8.conv_dw.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.8.norm.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.8.norm.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.8.mlp.fc1.weight,p.shape=torch.Size([1536, 384]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.8.mlp.fc1.bias,p.shape=torch.Size([1536]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.8.mlp.fc2.weight,p.shape=torch.Size([384, 1536]), p.requires_grad = False\n",
      "Name=backbone.stages.2.blocks.8.mlp.fc2.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.3.downsample.0.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.3.downsample.0.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=backbone.stages.3.downsample.1.weight,p.shape=torch.Size([768, 384, 2, 2]), p.requires_grad = False\n",
      "Name=backbone.stages.3.downsample.1.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.0.gamma,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.0.conv_dw.weight,p.shape=torch.Size([768, 1, 7, 7]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.0.conv_dw.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.0.norm.weight,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.0.norm.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.0.mlp.fc1.weight,p.shape=torch.Size([3072, 768]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.0.mlp.fc1.bias,p.shape=torch.Size([3072]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.0.mlp.fc2.weight,p.shape=torch.Size([768, 3072]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.0.mlp.fc2.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.1.gamma,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.1.conv_dw.weight,p.shape=torch.Size([768, 1, 7, 7]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.1.conv_dw.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.1.norm.weight,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.1.norm.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.1.mlp.fc1.weight,p.shape=torch.Size([3072, 768]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.1.mlp.fc1.bias,p.shape=torch.Size([3072]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.1.mlp.fc2.weight,p.shape=torch.Size([768, 3072]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.1.mlp.fc2.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.2.gamma,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.2.conv_dw.weight,p.shape=torch.Size([768, 1, 7, 7]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.2.conv_dw.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.2.norm.weight,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.2.norm.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.2.mlp.fc1.weight,p.shape=torch.Size([3072, 768]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.2.mlp.fc1.bias,p.shape=torch.Size([3072]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.2.mlp.fc2.weight,p.shape=torch.Size([768, 3072]), p.requires_grad = False\n",
      "Name=backbone.stages.3.blocks.2.mlp.fc2.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.head.norm.weight,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=backbone.head.norm.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=head_label.weight,p.shape=torch.Size([10, 768]), p.requires_grad = True\n",
      "Name=head_label.bias,p.shape=torch.Size([10]), p.requires_grad = True\n",
      "Name=head_variety.weight,p.shape=torch.Size([10, 768]), p.requires_grad = True\n",
      "Name=head_variety.bias,p.shape=torch.Size([10]), p.requires_grad = True\n",
      "Name=head_age.weight,p.shape=torch.Size([1, 768]), p.requires_grad = True\n",
      "Name=head_age.bias,p.shape=torch.Size([1]), p.requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "#make sure just training the last layer\n",
    "for name, p in model.named_parameters():\n",
    "    print (f'Name={name},p.shape={p.shape}, p.requires_grad = {p.requires_grad}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51e84886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadNet(\n",
      "  (backbone): ConvNeXt(\n",
      "    (stem): Sequential(\n",
      "      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "    (stages): Sequential(\n",
      "      (0): ConvNeXtStage(\n",
      "        (downsample): Identity()\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (3): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (4): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (5): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (6): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (7): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (8): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_pre): Identity()\n",
      "    (head): NormMlpClassifierHead(\n",
      "      (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
      "      (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (pre_logits): Identity()\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "      (fc): Identity()\n",
      "    )\n",
      "  )\n",
      "  (head_label): Linear(in_features=768, out_features=10, bias=True)\n",
      "  (head_variety): Linear(in_features=768, out_features=10, bias=True)\n",
      "  (head_age): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#stopped here 9/29/25\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cdb817",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Optimizer & loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36a239fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=2e-3\n",
    "criterion = {\n",
    "    'label':   nn.CrossEntropyLoss(),\n",
    "    'variety': nn.CrossEntropyLoss(),\n",
    "    'age':     nn.MSELoss()\n",
    "}\n",
    "loss_weights = {'label': 1.0, 'variety': 0.7, 'age': 0.5}\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c23d6",
   "metadata": {},
   "source": [
    "# 6) Training & validation loop\n",
    "\n",
    "- Log train/valid **loss** and **accuracy** per epoch.  \n",
    "- warm up heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "087df5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss=1153.2969 tr_acc_label=0.443 tr_acc_variety=0.680 | valid loss=382.1500 va_acc_label=0.622 va_acc_variety=0.714 va_mae_age=25.305\n",
      "Epoch 02 | train loss=198.7945 tr_acc_label=0.596 tr_acc_variety=0.752 | valid loss=81.0375 va_acc_label=0.643 va_acc_variety=0.751 va_mae_age=9.989\n",
      "Epoch 03 | train loss=75.3589 tr_acc_label=0.628 tr_acc_variety=0.779 | valid loss=69.9145 va_acc_label=0.714 va_acc_variety=0.802 va_mae_age=9.131\n",
      "Epoch 04 | train loss=63.7352 tr_acc_label=0.654 tr_acc_variety=0.797 | valid loss=60.4969 va_acc_label=0.736 va_acc_variety=0.809 va_mae_age=8.443\n",
      "Epoch 05 | train loss=58.4966 tr_acc_label=0.670 tr_acc_variety=0.808 | valid loss=54.5497 va_acc_label=0.761 va_acc_variety=0.826 va_mae_age=7.993\n",
      "Epoch 06 | train loss=54.2428 tr_acc_label=0.685 tr_acc_variety=0.819 | valid loss=50.6365 va_acc_label=0.775 va_acc_variety=0.840 va_mae_age=7.685\n",
      "Epoch 07 | train loss=52.6000 tr_acc_label=0.692 tr_acc_variety=0.823 | valid loss=48.3230 va_acc_label=0.774 va_acc_variety=0.852 va_mae_age=7.490\n",
      "Epoch 08 | train loss=50.8445 tr_acc_label=0.704 tr_acc_variety=0.827 | valid loss=46.4956 va_acc_label=0.793 va_acc_variety=0.855 va_mae_age=7.339\n",
      "Epoch 09 | train loss=49.1280 tr_acc_label=0.712 tr_acc_variety=0.837 | valid loss=44.0459 va_acc_label=0.792 va_acc_variety=0.856 va_mae_age=7.162\n",
      "Epoch 10 | train loss=47.5367 tr_acc_label=0.713 tr_acc_variety=0.841 | valid loss=43.2746 va_acc_label=0.804 va_acc_variety=0.869 va_mae_age=7.067\n",
      "Epoch 11 | train loss=47.0926 tr_acc_label=0.714 tr_acc_variety=0.844 | valid loss=41.8722 va_acc_label=0.810 va_acc_variety=0.875 va_mae_age=6.958\n",
      "Epoch 12 | train loss=46.2643 tr_acc_label=0.719 tr_acc_variety=0.850 | valid loss=40.7182 va_acc_label=0.812 va_acc_variety=0.876 va_mae_age=6.869\n",
      "Epoch 13 | train loss=44.2754 tr_acc_label=0.724 tr_acc_variety=0.848 | valid loss=39.4992 va_acc_label=0.821 va_acc_variety=0.879 va_mae_age=6.778\n",
      "Epoch 14 | train loss=43.7631 tr_acc_label=0.733 tr_acc_variety=0.850 | valid loss=38.8479 va_acc_label=0.807 va_acc_variety=0.870 va_mae_age=6.714\n",
      "Epoch 15 | train loss=43.1973 tr_acc_label=0.731 tr_acc_variety=0.855 | valid loss=37.9537 va_acc_label=0.809 va_acc_variety=0.876 va_mae_age=6.640\n",
      "Epoch 16 | train loss=42.7841 tr_acc_label=0.733 tr_acc_variety=0.862 | valid loss=37.2275 va_acc_label=0.824 va_acc_variety=0.876 va_mae_age=6.581\n",
      "Epoch 17 | train loss=41.6184 tr_acc_label=0.737 tr_acc_variety=0.861 | valid loss=36.8855 va_acc_label=0.812 va_acc_variety=0.891 va_mae_age=6.538\n",
      "Epoch 18 | train loss=40.9078 tr_acc_label=0.742 tr_acc_variety=0.869 | valid loss=35.8716 va_acc_label=0.818 va_acc_variety=0.890 va_mae_age=6.471\n",
      "Epoch 19 | train loss=40.0722 tr_acc_label=0.739 tr_acc_variety=0.860 | valid loss=35.4352 va_acc_label=0.812 va_acc_variety=0.895 va_mae_age=6.425\n",
      "Epoch 20 | train loss=40.2220 tr_acc_label=0.741 tr_acc_variety=0.857 | valid loss=34.5878 va_acc_label=0.786 va_acc_variety=0.870 va_mae_age=6.364\n",
      "CPU times: user 2min 41s, sys: 29.5 s, total: 3min 10s\n",
      "Wall time: 13min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "def compute_multitask_loss(outputs, targets, criteria, weights=None):\n",
    "    total = 0.0\n",
    "    for k in ['label', 'variety', 'age']:\n",
    "        w = 1.0 if (weights is None or k not in weights) else weights[k]\n",
    "        total = total + w * criteria[k](outputs[k], targets[k])\n",
    "    return total\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criteria, device, weights=None):\n",
    "    model.train()\n",
    "    run_loss= 0.0\n",
    "    correct_label, correct_variety, total = 0, 0, 0\n",
    "    for images, y_var, y_age, y_lbl in loader:\n",
    "        images = images.to(device); y_lbl = y_lbl.to(device); y_var = y_var.to(device); y_age = y_age.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = model(images)  #only feeding in images\n",
    "        loss = compute_multitask_loss(outputs, {'label': y_lbl, 'variety': y_var, 'age': y_age}, criteria, weights)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        bs = images.size(0)\n",
    "        run_loss += loss.item() * bs;  \n",
    "        total += bs\n",
    "        correct_label   += (outputs['label'].argmax(1) == y_lbl).sum().item()\n",
    "        correct_variety += (outputs['variety'].argmax(1) == y_var).sum().item()\n",
    "    return run_loss / total, correct_label / total, correct_variety / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criteria, device, weights=None):\n",
    "    model.eval()\n",
    "    run_loss= 0.0\n",
    "    correct_label, correct_variety, total, mae_age = 0, 0, 0, 0.0\n",
    "    for images, y_var, y_age, y_lbl in loader:\n",
    "        images = images.to(device); y_lbl = y_lbl.to(device); y_var = y_var.to(device); y_age = y_age.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = compute_multitask_loss(outputs, {'label': y_lbl, 'variety': y_var, 'age': y_age}, criteria, weights)\n",
    "        bs = images.size(0)\n",
    "        run_loss += loss.item() * bs; total += bs\n",
    "        correct_label   += (outputs['label'].argmax(1) == y_lbl).sum().item()\n",
    "        correct_variety += (outputs['variety'].argmax(1) == y_var).sum().item()\n",
    "        mae_age         += torch.abs(outputs['age'] - y_age).sum().item()\n",
    "    return run_loss / total, correct_label / total, correct_variety / total, mae_age / total\n",
    "\n",
    "def train_and_evaluate(model, train_loader, valid_loader, optimizer, criterion, loss_weights=None, num_epochs=10):\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Train for one epoch\n",
    "        tr_loss, tr_acc_lbl, tr_acc_var = train_one_epoch(model, train_loader, optimizer, criterion, device, weights=loss_weights)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        va_loss, va_acc_lbl, va_acc_var, va_mae_age = evaluate(model, valid_loader, criterion, device, weights=loss_weights)\n",
    "        print(f\"Epoch {epoch:02d} | train loss={tr_loss:.4f} tr_acc_label={tr_acc_lbl:.3f} tr_acc_variety={tr_acc_var:.3f} | valid loss={va_loss:.4f} va_acc_label={va_acc_lbl:.3f} va_acc_variety={va_acc_var:.3f} va_mae_age={va_mae_age:.3f}\")\n",
    "\n",
    "train_and_evaluate(model, train_loader, valid_loader, optimizer, criterion, loss_weights=loss_weights, num_epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72f395f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=34.8115 acc_label=0.809 acc_variety=0.870\n"
     ]
    }
   ],
   "source": [
    "def eval(loader=test_loader):\n",
    "    run_loss, correct_label, correct_variety, mae_age = evaluate(model, loader, criterion, device, weights=loss_weights)\n",
    "    print(f\"loss={run_loss:.4f} acc_label={correct_label:.3f} acc_variety={correct_variety:.3f}\")\n",
    "\n",
    "eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b06063",
   "metadata": {},
   "source": [
    "### (Optional) Fine-tune the whole network\n",
    "\n",
    "After warming up the head, unfreeze the backbone and fine-tune at a **smaller LR**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b3ec264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss=39.3526 tr_acc_label=0.751 tr_acc_variety=0.874 | valid loss=34.4629 va_acc_label=0.828 va_acc_variety=0.896 va_mae_age=6.360\n",
      "Epoch 02 | train loss=38.7105 tr_acc_label=0.761 tr_acc_variety=0.870 | valid loss=34.4670 va_acc_label=0.830 va_acc_variety=0.895 va_mae_age=6.355\n",
      "Epoch 03 | train loss=39.0011 tr_acc_label=0.759 tr_acc_variety=0.876 | valid loss=34.4646 va_acc_label=0.837 va_acc_variety=0.897 va_mae_age=6.351\n",
      "Epoch 04 | train loss=39.3526 tr_acc_label=0.758 tr_acc_variety=0.875 | valid loss=34.4237 va_acc_label=0.829 va_acc_variety=0.902 va_mae_age=6.347\n",
      "Epoch 05 | train loss=39.0868 tr_acc_label=0.755 tr_acc_variety=0.871 | valid loss=34.3939 va_acc_label=0.835 va_acc_variety=0.906 va_mae_age=6.342\n",
      "Epoch 06 | train loss=38.5538 tr_acc_label=0.759 tr_acc_variety=0.878 | valid loss=34.3860 va_acc_label=0.838 va_acc_variety=0.897 va_mae_age=6.338\n",
      "Epoch 07 | train loss=39.4844 tr_acc_label=0.761 tr_acc_variety=0.875 | valid loss=34.3677 va_acc_label=0.833 va_acc_variety=0.902 va_mae_age=6.334\n",
      "Epoch 08 | train loss=38.9655 tr_acc_label=0.757 tr_acc_variety=0.875 | valid loss=34.2683 va_acc_label=0.835 va_acc_variety=0.900 va_mae_age=6.327\n",
      "Epoch 09 | train loss=38.5377 tr_acc_label=0.762 tr_acc_variety=0.874 | valid loss=34.2652 va_acc_label=0.834 va_acc_variety=0.901 va_mae_age=6.324\n",
      "Epoch 10 | train loss=38.8148 tr_acc_label=0.761 tr_acc_variety=0.873 | valid loss=34.2063 va_acc_label=0.835 va_acc_variety=0.902 va_mae_age=6.318\n",
      "Epoch 11 | train loss=38.7759 tr_acc_label=0.756 tr_acc_variety=0.876 | valid loss=34.1363 va_acc_label=0.838 va_acc_variety=0.895 va_mae_age=6.313\n",
      "Epoch 12 | train loss=38.6859 tr_acc_label=0.765 tr_acc_variety=0.884 | valid loss=34.0755 va_acc_label=0.838 va_acc_variety=0.902 va_mae_age=6.307\n",
      "Epoch 13 | train loss=38.2720 tr_acc_label=0.765 tr_acc_variety=0.878 | valid loss=33.8639 va_acc_label=0.837 va_acc_variety=0.898 va_mae_age=6.297\n",
      "Epoch 14 | train loss=38.5748 tr_acc_label=0.765 tr_acc_variety=0.877 | valid loss=33.7329 va_acc_label=0.841 va_acc_variety=0.909 va_mae_age=6.290\n",
      "Epoch 15 | train loss=39.2814 tr_acc_label=0.761 tr_acc_variety=0.881 | valid loss=33.7521 va_acc_label=0.838 va_acc_variety=0.902 va_mae_age=6.285\n",
      "Epoch 16 | train loss=38.1852 tr_acc_label=0.762 tr_acc_variety=0.877 | valid loss=33.7598 va_acc_label=0.838 va_acc_variety=0.901 va_mae_age=6.281\n",
      "Epoch 17 | train loss=37.9252 tr_acc_label=0.766 tr_acc_variety=0.877 | valid loss=33.6436 va_acc_label=0.837 va_acc_variety=0.901 va_mae_age=6.274\n",
      "Epoch 18 | train loss=38.4563 tr_acc_label=0.765 tr_acc_variety=0.878 | valid loss=33.5070 va_acc_label=0.840 va_acc_variety=0.904 va_mae_age=6.267\n",
      "Epoch 19 | train loss=38.5052 tr_acc_label=0.763 tr_acc_variety=0.876 | valid loss=33.4916 va_acc_label=0.834 va_acc_variety=0.903 va_mae_age=6.262\n",
      "Epoch 20 | train loss=37.4069 tr_acc_label=0.763 tr_acc_variety=0.880 | valid loss=33.4401 va_acc_label=0.838 va_acc_variety=0.909 va_mae_age=6.256\n",
      "CPU times: user 8min 34s, sys: 29.8 s, total: 9min 4s\n",
      "Wall time: 14min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# train the whole thing (unfreeze all layers)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "#make lr smaller for fine-tuning\n",
    "lr1=lr/10\n",
    "\n",
    "#change learning rate for fine-tuning\n",
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = lr1\n",
    "\n",
    "train_and_evaluate(model, train_loader, valid_loader, optimizer, criterion, loss_weights=loss_weights, num_epochs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a433be9f",
   "metadata": {},
   "source": [
    "## 10) Save / Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "829b6ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to multitask_convnext_tiny_paddy.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.save(model.state_dict(), 'multitask_convnext_tiny_paddy.pth')\n",
    "print('Saved to multitask_convnext_tiny_paddy.pth')\n",
    "# model.load_state_dict(torch.load('multitask_convnext_tiny_paddy.pth', map_location=device))\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9025eae",
   "metadata": {},
   "source": [
    "## 7) Evaluation & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef2af28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (6x6 for PaddyDoctor):\n",
      "[[ 21   1   0   4   2   0   1   6   1  10]\n",
      " [  1  34   0   1   1   0   1   4   1   1]\n",
      " [  0   0  21   1   0   1   0   0   0   0]\n",
      " [  0   1   0 136   4   0   3   5   4   8]\n",
      " [  3   2   1  10  75   0   4   1   0   0]\n",
      " [  0   0   1   0   0 166   0   0   0   1]\n",
      " [  3   0   0   5   3   0  38   6   1   5]\n",
      " [  0   0   0   8   0   0   5 139  13   5]\n",
      " [  0   0   2   0   1   0   0   3 171   3]\n",
      " [  4   0   0   6   2   0   4   4   1  70]]\n",
      "\n",
      "Per-class (precision, recall, f1):\n",
      "bacterial_leaf_blight: P=0.656 R=0.457 F1=0.538\n",
      "bacterial_leaf_streak: P=0.895 R=0.773 F1=0.829\n",
      "bacterial_panicle_blight: P=0.840 R=0.913 F1=0.875\n",
      "     blast: P=0.795 R=0.845 F1=0.819\n",
      "brown_spot: P=0.852 R=0.781 F1=0.815\n",
      "dead_heart: P=0.994 R=0.988 F1=0.991\n",
      "downy_mildew: P=0.679 R=0.623 F1=0.650\n",
      "     hispa: P=0.827 R=0.818 F1=0.822\n",
      "    normal: P=0.891 R=0.950 F1=0.919\n",
      "    tungro: P=0.680 R=0.769 F1=0.722\n",
      "\n",
      "Macro avg: P=0.811 R=0.792 F1=0.798\n",
      "Overall Accuracy: 0.838\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def get_all_preds_targets(model, loader):\n",
    "    model.eval()\n",
    "    preds, targs = [], []\n",
    "    for xb,_,_,yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "\n",
    "        #just want the label preds\n",
    "        preds.append(logits['label'].argmax(1).cpu().numpy())\n",
    "        targs.append(yb.numpy())\n",
    "    return np.concatenate(preds), np.concatenate(targs)\n",
    "\n",
    "preds, targs = get_all_preds_targets(model, valid_loader)\n",
    "\n",
    "# Confusion matrix\n",
    "num_classes = len(class_names)\n",
    "cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "for t, p in zip(targs, preds):\n",
    "    cm[t, p] += 1\n",
    "\n",
    "print('Confusion Matrix (6x6 for PaddyDoctor):')\n",
    "print(cm)\n",
    "\n",
    "# Per-class metrics\n",
    "per_class = []\n",
    "for k in range(num_classes):\n",
    "    TP = cm[k,k]\n",
    "    FP = cm[:,k].sum() - TP\n",
    "    FN = cm[k,:].sum() - TP\n",
    "    TN = cm.sum() - TP - FP - FN\n",
    "    prec = TP/(TP+FP) if (TP+FP)>0 else 0.0\n",
    "    rec  = TP/(TP+FN) if (TP+FN)>0 else 0.0\n",
    "    f1   = (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "    per_class.append((prec, rec, f1))\n",
    "\n",
    "macro_p = float(np.mean([p for p,_,_ in per_class]))\n",
    "macro_r = float(np.mean([r for _,r,_ in per_class]))\n",
    "macro_f = float(np.mean([f for _,_,f in per_class]))\n",
    "overall_acc = float((preds == targs).mean())\n",
    "\n",
    "print('\\nPer-class (precision, recall, f1):')\n",
    "for name,(p,r,f) in zip(class_names, per_class):\n",
    "    print(f'{name:>10s}: P={p:.3f} R={r:.3f} F1={f:.3f}')\n",
    "print(f\"\\nMacro avg: P={macro_p:.3f} R={macro_r:.3f} F1={macro_f:.3f}\")\n",
    "print(f'Overall Accuracy: {overall_acc:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736da112",
   "metadata": {},
   "source": [
    "## 8) (Optional) Inference on Test Set + `submission.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fb11c44",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'argmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m     ids, labels \u001b[38;5;241m=\u001b[39m predict_folder_images(test_dir_A, valid_tfms, class_names)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m test_dir_B\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m---> 24\u001b[0m     ids, labels \u001b[38;5;241m=\u001b[39m predict_folder_images(test_dir_B, valid_tfms, class_names)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     ids, labels \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/p311/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m, in \u001b[0;36mpredict_folder_images\u001b[0;34m(img_dir, transform, class_names)\u001b[0m\n\u001b[1;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m transform(img)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m---> 12\u001b[0m pred \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     13\u001b[0m ids\u001b[38;5;241m.\u001b[39mappend(p\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m     14\u001b[0m labels\u001b[38;5;241m.\u001b[39mappend(class_names[pred])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'argmax'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_folder_images(img_dir, transform, class_names):\n",
    "    paths = sorted([p for p in Path(img_dir).glob(\"*.*\") if p.suffix.lower() in {\".jpg\",\".jpeg\",\".png\",\".bmp\"}])\n",
    "    ids, labels = [], []\n",
    "    for p in paths:\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        x = transform(img).unsqueeze(0).to(device)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(1).item()\n",
    "        ids.append(p.name)\n",
    "        labels.append(class_names[pred])\n",
    "    return ids, labels\n",
    "\n",
    "test_dir_A = DATA_DIR / \"test\"\n",
    "test_dir_B = DATA_DIR / \"test_images\"\n",
    "sub_path   = DATA_DIR / \"submission.csv\"\n",
    "\n",
    "if test_dir_A.exists():\n",
    "    ids, labels = predict_folder_images(test_dir_A, valid_tfms, class_names)\n",
    "elif test_dir_B.exists():\n",
    "    ids, labels = predict_folder_images(test_dir_B, valid_tfms, class_names)\n",
    "else:\n",
    "    ids, labels = [], []\n",
    "    print(\"No test directory found; skipping submission.csv.\")\n",
    "\n",
    "if ids:\n",
    "    df = pd.DataFrame({\"image_id\": ids, \"label\": labels})\n",
    "    df.to_csv(sub_path, index=False)\n",
    "    print(\"Saved:\", sub_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630fe097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_id,label\n",
      "200001.jpg,hispa\n",
      "200002.jpg,normal\n",
      "200003.jpg,downy_mildew\n",
      "200004.jpg,blast\n",
      "200005.jpg,blast\n",
      "200006.jpg,brown_spot\n",
      "200007.jpg,dead_heart\n",
      "200008.jpg,brown_spot\n",
      "200009.jpg,hispa\n",
      "cat: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!cat ./data/submission.csv | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bbc52c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Notes for Beginners\n",
    "\n",
    "- **Why `timm`?** Lots of pretrained models + convenient transforms. Switching `MODEL_NAME` is an easy way to try stronger backbones.\n",
    "- **Transforms:** Using `timm.data.create_transform` keeps preprocessing consistent with the chosen model.\n",
    "- **Training recipe:** Freeze → train head → unfreeze → fine-tune at smaller LR.\n",
    "- **OOM tips:** Lower `BATCH_SIZE` or try a smaller model (e.g., `efficientnet_b0`, `mobilenetv3_large_100`).\n",
    "- **Save/load:** `torch.save(model.state_dict(), \"model.pth\")`, then `model.load_state_dict(torch.load(\"model.pth\", map_location=device))`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

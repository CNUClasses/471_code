{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d6533b",
   "metadata": {},
   "source": [
    "\n",
    "# Paddy Disease Classification — **PyTorch + timm**\n",
    "\n",
    "This is a **minimal, beginner-friendly** image classification pipeline using **PyTorch** and **[timm](https://github.com/huggingface/pytorch-image-models)** (Torch Image Models).  \n",
    "Typical steps:\n",
    "\n",
    "1) Setup & configuration  \n",
    "2) Dataset & transforms (train/valid split)  \n",
    "3) Model (transfer learning with `timm.create_model`)  \n",
    "4) Training loop (loss/optimizer)  \n",
    "5) Evaluation & confusion matrix  \n",
    "6) (Optional) Inference on test set + `submission.csv`\n",
    "\n",
    "> **Note:** Point `DATA_DIR` to your local Kaggle Paddy dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e712e2b6",
   "metadata": {},
   "source": [
    "## 1) Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de007fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If timm isn't installed, uncomment:\n",
    "# !pip install timm --quiet\n",
    "\n",
    "import os, random, math, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import timm\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "DATA_DIR = Path(\"./data/\")  \n",
    "DATA_DIR_TRAIN = Path(\"./data/train_images/\")\n",
    "USE_IMAGEFOLDER = False             # set False to use CSV dataset class below\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9cb6c1",
   "metadata": {},
   "source": [
    "## 1) Get Data\n",
    "\n",
    "Data comes from <strong><a href=\"https://www.kaggle.com/competitions/paddy-disease-classification\">Paddy Doctor: Paddy Disease Classification</a></strong>.  A Kaggle competition whose goal is to identify the type of disease present in rice paddy leaf images. You can download it directly after signing up for a <a href=\"https://www.kaggle.com/\">Kaggle</a> account.<br>\n",
    "\n",
    "### (Optional)\n",
    "A better way to get the data is through the <strong><a href=\"https://www.kaggle.com/docs/api\">Kaggle CLI (command line interface)</a></strong>.<br>\n",
    "It lets you programatically interact with Kaggle (get/browse datasets and competitions and submit results)<br>\n",
    "BTW- you need to have an API key in order to use the CLI, to get one:<br>\n",
    "Scroll to the API section in your Account settings and click the Create New API Token button.<br>\n",
    "Kaggle will generate a JSON file named kaggle.json and prompt you to save the file to your computer.<br>\n",
    "Put this file in the ~/.kaggle directory on your machine, make sure its only readable by you (chmod 600 /root/.kaggle/kaggle.json)<br>\n",
    "The CLI looks in this place for your key.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade kaggle  #upgrade or install kaggle CLI\n",
    "# !mkdir -p ~/.kaggle  #create a place for kaggle.json file\n",
    "# get the key from above, download it and place it in ~/.kaggle/kaggle.json\n",
    "\n",
    "# the \\\" below ensures the \" marks are written to the file\n",
    "# echo \"{\\\"username\\\":\\\"youruserid\\\",\\\"key\\\":\\\"yourkey\\\"}\" > ~/.kaggle/kaggle.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f13dfe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets list #active competitions\n",
    "# !kaggle datasets list -s paddy-disease-classification  #the one we want\n",
    "# !kaggle competitions download -c paddy-disease-classification #get it\n",
    "# !unzip paddy-disease-classification.zip -d ./data/  #unzip it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6049e",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Create datasets with timm transforms \n",
    "\n",
    "If your data follows the Kaggle layout (`train_images/` and `train.csv` with `image_id,label`), use this dataset class.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a14265f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved input size: (224, 224)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# MODEL_NAME = \"resnet18\"  # try: 'efficientnet_b0', 'convnext_tiny', 'mobilenetv3_large_100', ...\n",
    "MODEL_NAME='convnext_tiny'\n",
    "\n",
    "config = timm.data.resolve_data_config({}, model=MODEL_NAME)\n",
    "if 'input_size' in config and len(config['input_size']) == 3:\n",
    "    _, H, W = config['input_size']\n",
    "else:\n",
    "    H = W = 224\n",
    "\n",
    "train_tfms = timm.data.create_transform(\n",
    "    **config,\n",
    "    is_training=True,\n",
    "    hflip=0.5,\n",
    "    color_jitter=None,\n",
    "    auto_augment=None\n",
    ")\n",
    "\n",
    "valid_tfms = timm.data.create_transform(\n",
    "    **config,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "print(\"Resolved input size:\", (H, W))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0600bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_ds = datasets.ImageFolder(DATA_DIR_TRAIN, transform=train_tfms)\n",
    "n_total = len(full_ds)\n",
    "n_valid = int(0.1 * n_total)\n",
    "n_train = n_total - n_valid\n",
    "\n",
    "#should be stratified split, but close enough for now\n",
    "train_ds, valid_ds = random_split(full_ds, [n_train, n_valid], generator=torch.Generator().manual_seed(42))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f6870",
   "metadata": {},
   "source": [
    "## 3) Create Dataloaders\n",
    "\n",
    "- Shuffle the training loader; keep validation loader deterministic.  \n",
    "- Adjust `BATCH_SIZE` to fit your GPU/CPU memory.\n",
    "\n",
    "This is a datascience competition:<br>\n",
    "the train_images folder contains images with class membership info (in the train.csv file).<br>\n",
    "the test_images folder contains images that your model infers membership on.  These inferences are bundled into a file (see sample_submission.csv) which is submitted for ranking  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0afefd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid sizes: 9367 1040\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# lr=0.01 start with this one (its too high see lr finder below)\n",
    "lr=2e-3 #lr finer approved this one\n",
    "\n",
    "train_images = DATA_DIR / \"train_images\"\n",
    "train_csv    = DATA_DIR / \"train.csv\"\n",
    "class_names = list(full_ds.class_to_idx.keys())\n",
    "num_classes = len(class_names)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# print(f\"Classes ({num_classes}):\", class_names[:10], \"...\" if num_classes>10 else \"\")\n",
    "print(\"Train/Valid sizes:\", len(train_ds), len(valid_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567303c6",
   "metadata": {},
   "source": [
    "## 4) Load timm Model (Transfer Learning with **timm**)\n",
    "\n",
    "- Create a **pretrained** model with `num_classes` = 10 (PaddyDoctor classes).  \n",
    "- **Warm-up:** freeze backbone; train the classifier head first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d11c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier layer: Linear(in_features=768, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def make_model(num_classes=num_classes, device=device, verbose=True):\n",
    "    #pre trained =True or you get random weights\n",
    "    model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=num_classes)\n",
    "    if verbose: print(\"Classifier layer:\", model.get_classifier())\n",
    "\n",
    "    # Freeze backbone; train classifier head first\n",
    "    for name, p in model.named_parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Unfreeze classifier / head only (name depends on model family; get via get_classifier())\n",
    "    clf_name = model.get_classifier()\n",
    "\n",
    "    #make sure the last claFC layer is trainable\n",
    "    for name, p in clf_name.named_parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "model=make_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee8d2803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name=stem.0.weight,p.shape=torch.Size([96, 3, 4, 4]), p.requires_grad = False\n",
      "Name=stem.0.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stem.1.weight,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stem.1.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stages.0.blocks.0.gamma,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stages.0.blocks.0.conv_dw.weight,p.shape=torch.Size([96, 1, 7, 7]), p.requires_grad = False\n",
      "Name=stages.0.blocks.0.conv_dw.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stages.0.blocks.0.norm.weight,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stages.0.blocks.0.norm.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stages.0.blocks.0.mlp.fc1.weight,p.shape=torch.Size([384, 96]), p.requires_grad = False\n",
      "Name=stages.0.blocks.0.mlp.fc1.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.0.blocks.0.mlp.fc2.weight,p.shape=torch.Size([96, 384]), p.requires_grad = False\n",
      "Name=stages.0.blocks.0.mlp.fc2.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stages.0.blocks.1.gamma,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stages.0.blocks.1.conv_dw.weight,p.shape=torch.Size([96, 1, 7, 7]), p.requires_grad = False\n",
      "Name=stages.0.blocks.1.conv_dw.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stages.0.blocks.1.norm.weight,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stages.0.blocks.1.norm.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stages.0.blocks.1.mlp.fc1.weight,p.shape=torch.Size([384, 96]), p.requires_grad = False\n",
      "Name=stages.0.blocks.1.mlp.fc1.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.0.blocks.1.mlp.fc2.weight,p.shape=torch.Size([96, 384]), p.requires_grad = False\n",
      "Name=stages.0.blocks.1.mlp.fc2.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stages.0.blocks.2.gamma,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stages.0.blocks.2.conv_dw.weight,p.shape=torch.Size([96, 1, 7, 7]), p.requires_grad = False\n",
      "Name=stages.0.blocks.2.conv_dw.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stages.0.blocks.2.norm.weight,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stages.0.blocks.2.norm.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stages.0.blocks.2.mlp.fc1.weight,p.shape=torch.Size([384, 96]), p.requires_grad = False\n",
      "Name=stages.0.blocks.2.mlp.fc1.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.0.blocks.2.mlp.fc2.weight,p.shape=torch.Size([96, 384]), p.requires_grad = False\n",
      "Name=stages.0.blocks.2.mlp.fc2.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stages.1.downsample.0.weight,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stages.1.downsample.0.bias,p.shape=torch.Size([96]), p.requires_grad = False\n",
      "Name=stages.1.downsample.1.weight,p.shape=torch.Size([192, 96, 2, 2]), p.requires_grad = False\n",
      "Name=stages.1.downsample.1.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=stages.1.blocks.0.gamma,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=stages.1.blocks.0.conv_dw.weight,p.shape=torch.Size([192, 1, 7, 7]), p.requires_grad = False\n",
      "Name=stages.1.blocks.0.conv_dw.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=stages.1.blocks.0.norm.weight,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=stages.1.blocks.0.norm.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=stages.1.blocks.0.mlp.fc1.weight,p.shape=torch.Size([768, 192]), p.requires_grad = False\n",
      "Name=stages.1.blocks.0.mlp.fc1.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=stages.1.blocks.0.mlp.fc2.weight,p.shape=torch.Size([192, 768]), p.requires_grad = False\n",
      "Name=stages.1.blocks.0.mlp.fc2.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=stages.1.blocks.1.gamma,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=stages.1.blocks.1.conv_dw.weight,p.shape=torch.Size([192, 1, 7, 7]), p.requires_grad = False\n",
      "Name=stages.1.blocks.1.conv_dw.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=stages.1.blocks.1.norm.weight,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=stages.1.blocks.1.norm.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=stages.1.blocks.1.mlp.fc1.weight,p.shape=torch.Size([768, 192]), p.requires_grad = False\n",
      "Name=stages.1.blocks.1.mlp.fc1.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=stages.1.blocks.1.mlp.fc2.weight,p.shape=torch.Size([192, 768]), p.requires_grad = False\n",
      "Name=stages.1.blocks.1.mlp.fc2.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=stages.1.blocks.2.gamma,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=stages.1.blocks.2.conv_dw.weight,p.shape=torch.Size([192, 1, 7, 7]), p.requires_grad = False\n",
      "Name=stages.1.blocks.2.conv_dw.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=stages.1.blocks.2.norm.weight,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=stages.1.blocks.2.norm.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=stages.1.blocks.2.mlp.fc1.weight,p.shape=torch.Size([768, 192]), p.requires_grad = False\n",
      "Name=stages.1.blocks.2.mlp.fc1.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=stages.1.blocks.2.mlp.fc2.weight,p.shape=torch.Size([192, 768]), p.requires_grad = False\n",
      "Name=stages.1.blocks.2.mlp.fc2.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=stages.2.downsample.0.weight,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=stages.2.downsample.0.bias,p.shape=torch.Size([192]), p.requires_grad = False\n",
      "Name=stages.2.downsample.1.weight,p.shape=torch.Size([384, 192, 2, 2]), p.requires_grad = False\n",
      "Name=stages.2.downsample.1.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.0.gamma,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.0.conv_dw.weight,p.shape=torch.Size([384, 1, 7, 7]), p.requires_grad = False\n",
      "Name=stages.2.blocks.0.conv_dw.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.0.norm.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.0.norm.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.0.mlp.fc1.weight,p.shape=torch.Size([1536, 384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.0.mlp.fc1.bias,p.shape=torch.Size([1536]), p.requires_grad = False\n",
      "Name=stages.2.blocks.0.mlp.fc2.weight,p.shape=torch.Size([384, 1536]), p.requires_grad = False\n",
      "Name=stages.2.blocks.0.mlp.fc2.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.1.gamma,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.1.conv_dw.weight,p.shape=torch.Size([384, 1, 7, 7]), p.requires_grad = False\n",
      "Name=stages.2.blocks.1.conv_dw.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.1.norm.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.1.norm.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.1.mlp.fc1.weight,p.shape=torch.Size([1536, 384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.1.mlp.fc1.bias,p.shape=torch.Size([1536]), p.requires_grad = False\n",
      "Name=stages.2.blocks.1.mlp.fc2.weight,p.shape=torch.Size([384, 1536]), p.requires_grad = False\n",
      "Name=stages.2.blocks.1.mlp.fc2.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.2.gamma,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.2.conv_dw.weight,p.shape=torch.Size([384, 1, 7, 7]), p.requires_grad = False\n",
      "Name=stages.2.blocks.2.conv_dw.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.2.norm.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.2.norm.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.2.mlp.fc1.weight,p.shape=torch.Size([1536, 384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.2.mlp.fc1.bias,p.shape=torch.Size([1536]), p.requires_grad = False\n",
      "Name=stages.2.blocks.2.mlp.fc2.weight,p.shape=torch.Size([384, 1536]), p.requires_grad = False\n",
      "Name=stages.2.blocks.2.mlp.fc2.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.3.gamma,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.3.conv_dw.weight,p.shape=torch.Size([384, 1, 7, 7]), p.requires_grad = False\n",
      "Name=stages.2.blocks.3.conv_dw.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.3.norm.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.3.norm.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.3.mlp.fc1.weight,p.shape=torch.Size([1536, 384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.3.mlp.fc1.bias,p.shape=torch.Size([1536]), p.requires_grad = False\n",
      "Name=stages.2.blocks.3.mlp.fc2.weight,p.shape=torch.Size([384, 1536]), p.requires_grad = False\n",
      "Name=stages.2.blocks.3.mlp.fc2.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.4.gamma,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.4.conv_dw.weight,p.shape=torch.Size([384, 1, 7, 7]), p.requires_grad = False\n",
      "Name=stages.2.blocks.4.conv_dw.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.4.norm.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.4.norm.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.4.mlp.fc1.weight,p.shape=torch.Size([1536, 384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.4.mlp.fc1.bias,p.shape=torch.Size([1536]), p.requires_grad = False\n",
      "Name=stages.2.blocks.4.mlp.fc2.weight,p.shape=torch.Size([384, 1536]), p.requires_grad = False\n",
      "Name=stages.2.blocks.4.mlp.fc2.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.5.gamma,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.5.conv_dw.weight,p.shape=torch.Size([384, 1, 7, 7]), p.requires_grad = False\n",
      "Name=stages.2.blocks.5.conv_dw.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.5.norm.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.5.norm.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.5.mlp.fc1.weight,p.shape=torch.Size([1536, 384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.5.mlp.fc1.bias,p.shape=torch.Size([1536]), p.requires_grad = False\n",
      "Name=stages.2.blocks.5.mlp.fc2.weight,p.shape=torch.Size([384, 1536]), p.requires_grad = False\n",
      "Name=stages.2.blocks.5.mlp.fc2.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.6.gamma,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.6.conv_dw.weight,p.shape=torch.Size([384, 1, 7, 7]), p.requires_grad = False\n",
      "Name=stages.2.blocks.6.conv_dw.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.6.norm.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.6.norm.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.6.mlp.fc1.weight,p.shape=torch.Size([1536, 384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.6.mlp.fc1.bias,p.shape=torch.Size([1536]), p.requires_grad = False\n",
      "Name=stages.2.blocks.6.mlp.fc2.weight,p.shape=torch.Size([384, 1536]), p.requires_grad = False\n",
      "Name=stages.2.blocks.6.mlp.fc2.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.7.gamma,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.7.conv_dw.weight,p.shape=torch.Size([384, 1, 7, 7]), p.requires_grad = False\n",
      "Name=stages.2.blocks.7.conv_dw.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.7.norm.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.7.norm.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.7.mlp.fc1.weight,p.shape=torch.Size([1536, 384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.7.mlp.fc1.bias,p.shape=torch.Size([1536]), p.requires_grad = False\n",
      "Name=stages.2.blocks.7.mlp.fc2.weight,p.shape=torch.Size([384, 1536]), p.requires_grad = False\n",
      "Name=stages.2.blocks.7.mlp.fc2.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.8.gamma,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.8.conv_dw.weight,p.shape=torch.Size([384, 1, 7, 7]), p.requires_grad = False\n",
      "Name=stages.2.blocks.8.conv_dw.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.8.norm.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.8.norm.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.8.mlp.fc1.weight,p.shape=torch.Size([1536, 384]), p.requires_grad = False\n",
      "Name=stages.2.blocks.8.mlp.fc1.bias,p.shape=torch.Size([1536]), p.requires_grad = False\n",
      "Name=stages.2.blocks.8.mlp.fc2.weight,p.shape=torch.Size([384, 1536]), p.requires_grad = False\n",
      "Name=stages.2.blocks.8.mlp.fc2.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.3.downsample.0.weight,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.3.downsample.0.bias,p.shape=torch.Size([384]), p.requires_grad = False\n",
      "Name=stages.3.downsample.1.weight,p.shape=torch.Size([768, 384, 2, 2]), p.requires_grad = False\n",
      "Name=stages.3.downsample.1.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=stages.3.blocks.0.gamma,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=stages.3.blocks.0.conv_dw.weight,p.shape=torch.Size([768, 1, 7, 7]), p.requires_grad = False\n",
      "Name=stages.3.blocks.0.conv_dw.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=stages.3.blocks.0.norm.weight,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=stages.3.blocks.0.norm.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=stages.3.blocks.0.mlp.fc1.weight,p.shape=torch.Size([3072, 768]), p.requires_grad = False\n",
      "Name=stages.3.blocks.0.mlp.fc1.bias,p.shape=torch.Size([3072]), p.requires_grad = False\n",
      "Name=stages.3.blocks.0.mlp.fc2.weight,p.shape=torch.Size([768, 3072]), p.requires_grad = False\n",
      "Name=stages.3.blocks.0.mlp.fc2.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=stages.3.blocks.1.gamma,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=stages.3.blocks.1.conv_dw.weight,p.shape=torch.Size([768, 1, 7, 7]), p.requires_grad = False\n",
      "Name=stages.3.blocks.1.conv_dw.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=stages.3.blocks.1.norm.weight,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=stages.3.blocks.1.norm.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=stages.3.blocks.1.mlp.fc1.weight,p.shape=torch.Size([3072, 768]), p.requires_grad = False\n",
      "Name=stages.3.blocks.1.mlp.fc1.bias,p.shape=torch.Size([3072]), p.requires_grad = False\n",
      "Name=stages.3.blocks.1.mlp.fc2.weight,p.shape=torch.Size([768, 3072]), p.requires_grad = False\n",
      "Name=stages.3.blocks.1.mlp.fc2.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=stages.3.blocks.2.gamma,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=stages.3.blocks.2.conv_dw.weight,p.shape=torch.Size([768, 1, 7, 7]), p.requires_grad = False\n",
      "Name=stages.3.blocks.2.conv_dw.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=stages.3.blocks.2.norm.weight,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=stages.3.blocks.2.norm.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=stages.3.blocks.2.mlp.fc1.weight,p.shape=torch.Size([3072, 768]), p.requires_grad = False\n",
      "Name=stages.3.blocks.2.mlp.fc1.bias,p.shape=torch.Size([3072]), p.requires_grad = False\n",
      "Name=stages.3.blocks.2.mlp.fc2.weight,p.shape=torch.Size([768, 3072]), p.requires_grad = False\n",
      "Name=stages.3.blocks.2.mlp.fc2.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=head.norm.weight,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=head.norm.bias,p.shape=torch.Size([768]), p.requires_grad = False\n",
      "Name=head.fc.weight,p.shape=torch.Size([10, 768]), p.requires_grad = True\n",
      "Name=head.fc.bias,p.shape=torch.Size([10]), p.requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "#make sure just training the last layer\n",
    "for name, p in model.named_parameters():\n",
    "    print (f'Name={name},p.shape={p.shape}, p.requires_grad = {p.requires_grad}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51e84886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNeXt(\n",
      "  (stem): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (stages): Sequential(\n",
      "    (0): ConvNeXtStage(\n",
      "      (downsample): Identity()\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): ConvNeXtStage(\n",
      "      (downsample): Sequential(\n",
      "        (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ConvNeXtStage(\n",
      "      (downsample): Sequential(\n",
      "        (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (3): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (4): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (5): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (6): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (7): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (8): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): ConvNeXtStage(\n",
      "      (downsample): Sequential(\n",
      "        (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm_pre): Identity()\n",
      "  (head): NormMlpClassifierHead(\n",
      "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
      "    (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (pre_logits): Identity()\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (fc): Linear(in_features=768, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cdb817",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Optimizer & loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04bbc80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4af36",
   "metadata": {},
   "source": [
    "### (Optional) Create a learning rate finder to help set initial Learning Rate\n",
    "\n",
    "For a sweep of learning rates, reinitialize the model for each rate, train for a **few batches**, and save (learning rate, loss). <br>\n",
    "When done plot the loss versus learning rate, then choose the largest learning rate on the descending slope—just before the loss starts rising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4be4f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class LearningRateFinder:\n",
    "    def __init__(self, model_fn, criterion, device):\n",
    "        self.model_fn = model_fn\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.results = []\n",
    "\n",
    "    def _reinit(self):\n",
    "        return self.model_fn().to(self.device)\n",
    "\n",
    "    def run(self, lr_list, train_loader, num_batches=5):\n",
    "        self.results.clear()\n",
    "        for lr in lr_list:\n",
    "            model = self._reinit()\n",
    "            optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "#            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            # optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            model.train()\n",
    "            it = iter(train_loader); losses = []\n",
    "            for _ in range(num_batches):\n",
    "                try:\n",
    "                    xb, yb = next(it)\n",
    "                except StopIteration:\n",
    "                    it = iter(train_loader); xb, yb = next(it)\n",
    "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                logits = model(xb)\n",
    "                loss = self.criterion(logits, yb)\n",
    "                loss.backward(); optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "            avg_loss = float(np.mean(losses)) if losses else float('nan')\n",
    "            self.results.append((float(lr), avg_loss))\n",
    "        return self.results\n",
    "\n",
    "    def plot(self):\n",
    "        if not self.results:\n",
    "            print(\"No results. Run .run() first.\"); return\n",
    "        lrs, losses = zip(*self.results)\n",
    "        plt.figure()\n",
    "        plt.plot(lrs, losses, marker='o')\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel(\"Learning Rate\"); plt.ylabel(\"Average Loss (few batches)\")\n",
    "        plt.title(\"LR Finder — Loss vs Learning Rate\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5361190a",
   "metadata": {},
   "source": [
    "### Use the LR Finder to select an appropriate LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac533ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier layer: Linear(in_features=768, out_features=10, bias=True)\n",
      "Classifier layer: Linear(in_features=768, out_features=10, bias=True)\n",
      "Classifier layer: Linear(in_features=768, out_features=10, bias=True)\n",
      "Classifier layer: Linear(in_features=768, out_features=10, bias=True)\n",
      "Classifier layer: Linear(in_features=768, out_features=10, bias=True)\n",
      "Classifier layer: Linear(in_features=768, out_features=10, bias=True)\n",
      "Classifier layer: Linear(in_features=768, out_features=10, bias=True)\n",
      "Classifier layer: Linear(in_features=768, out_features=10, bias=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.0007196856730011522, 2.2901888608932497)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lr_candidates = np.logspace(-6, -1, num=8)\n",
    "finder = LearningRateFinder(make_model, criterion, device)\n",
    "results = finder.run(lr_candidates, train_loader, num_batches=5)\n",
    "finder.plot()\n",
    "\n",
    "best_lr, best_loss = min(results, key=lambda t: t[1])\n",
    "best_lr, best_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c23d6",
   "metadata": {},
   "source": [
    "# 6) Training & validation loop\n",
    "\n",
    "- Log train/valid **loss** and **accuracy** per epoch.  \n",
    "- Keep code short and well-commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "087df5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss 0.6094 acc 0.797 | valid loss 0.7097 acc 0.774\n",
      "Epoch 02 | train loss 0.6131 acc 0.797 | valid loss 0.7005 acc 0.768\n",
      "Epoch 03 | train loss 0.6105 acc 0.802 | valid loss 0.6664 acc 0.789\n",
      "Epoch 04 | train loss 0.6059 acc 0.798 | valid loss 0.6949 acc 0.771\n",
      "Epoch 05 | train loss 0.6119 acc 0.801 | valid loss 0.6617 acc 0.777\n",
      "CPU times: user 2min 46s, sys: 9.26 s, total: 2min 56s\n",
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb) #loss is the avearege over the batch (so multiply by batch size below)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)  #loss is the avearege over the batch (so multiply by batch size below)\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "def train_and_evaluate(model, train_loader, valid_loader, optimizer, criterion, num_epochs=10):\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Train for one epoch\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        va_loss, va_acc = evaluate(model, valid_loader, criterion)\n",
    "        print(f\"Epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.3f} | valid loss {va_loss:.4f} acc {va_acc:.3f}\")\n",
    "\n",
    "train_and_evaluate(model, train_loader, valid_loader, optimizer, criterion, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b06063",
   "metadata": {},
   "source": [
    "### (Optional) Fine-tune the whole network\n",
    "\n",
    "After warming up the head, unfreeze the backbone and fine-tune at a **smaller LR**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b3ec264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss 0.6603 acc 0.781 | valid loss 0.6833 acc 0.782\n",
      "Epoch 02 | train loss 0.6443 acc 0.787 | valid loss 0.6672 acc 0.784\n",
      "Epoch 03 | train loss 0.6325 acc 0.793 | valid loss 0.6734 acc 0.768\n",
      "Epoch 04 | train loss 0.6306 acc 0.788 | valid loss 0.7326 acc 0.760\n",
      "Epoch 05 | train loss 0.6232 acc 0.796 | valid loss 0.6841 acc 0.776\n",
      "CPU times: user 2min 45s, sys: 9.9 s, total: 2min 55s\n",
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# train the whole thing (unfreeze all layers)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "#make lr smaller for fine-tuning\n",
    "lr1=lr/10\n",
    "\n",
    "#change learning rate for fine-tuning\n",
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = lr1\n",
    "\n",
    "train_and_evaluate(model, train_loader, valid_loader, optimizer, criterion, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9025eae",
   "metadata": {},
   "source": [
    "## 7) Evaluation & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef2af28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (6x6 for PaddyDoctor):\n",
      "[[ 31   2   0   8   7   0   1   6   3   4]\n",
      " [  0  26   0   1   3   0   1   4   0   1]\n",
      " [  0   0  32   0   0   6   0   0   3   0]\n",
      " [  2   0   0 138   9   1   6   7   6   7]\n",
      " [  3   1   0   8  68   0   5   3   4   3]\n",
      " [  0   1   1   1   1 138   0   3   3   2]\n",
      " [  3   0   0   5   5   0  25  11   1   5]\n",
      " [  0   0   0   7   7   0   3 129  13   0]\n",
      " [  0   0   1   2   6   0   0  10 146   1]\n",
      " [  0   0   0   7   5   1   3  10   4  70]]\n",
      "\n",
      "Per-class (precision, recall, f1):\n",
      "bacterial_leaf_blight: P=0.795 R=0.500 F1=0.614\n",
      "bacterial_leaf_streak: P=0.867 R=0.722 F1=0.788\n",
      "bacterial_panicle_blight: P=0.941 R=0.780 F1=0.853\n",
      "     blast: P=0.780 R=0.784 F1=0.782\n",
      "brown_spot: P=0.613 R=0.716 F1=0.660\n",
      "dead_heart: P=0.945 R=0.920 F1=0.932\n",
      "downy_mildew: P=0.568 R=0.455 F1=0.505\n",
      "     hispa: P=0.705 R=0.811 F1=0.754\n",
      "    normal: P=0.798 R=0.880 F1=0.837\n",
      "    tungro: P=0.753 R=0.700 F1=0.725\n",
      "\n",
      "Macro avg: P=0.776 R=0.727 F1=0.745\n",
      "Overall Accuracy: 0.772\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def get_all_preds_targets(model, loader):\n",
    "    model.eval()\n",
    "    preds, targs = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        preds.append(logits.argmax(1).cpu().numpy())\n",
    "        targs.append(yb.numpy())\n",
    "    return np.concatenate(preds), np.concatenate(targs)\n",
    "\n",
    "preds, targs = get_all_preds_targets(model, valid_loader)\n",
    "\n",
    "# Confusion matrix\n",
    "num_classes = len(class_names)\n",
    "cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "for t, p in zip(targs, preds):\n",
    "    cm[t, p] += 1\n",
    "\n",
    "print('Confusion Matrix (6x6 for PaddyDoctor):')\n",
    "print(cm)\n",
    "\n",
    "# Per-class metrics\n",
    "per_class = []\n",
    "for k in range(num_classes):\n",
    "    TP = cm[k,k]\n",
    "    FP = cm[:,k].sum() - TP\n",
    "    FN = cm[k,:].sum() - TP\n",
    "    TN = cm.sum() - TP - FP - FN\n",
    "    prec = TP/(TP+FP) if (TP+FP)>0 else 0.0\n",
    "    rec  = TP/(TP+FN) if (TP+FN)>0 else 0.0\n",
    "    f1   = (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "    per_class.append((prec, rec, f1))\n",
    "\n",
    "macro_p = float(np.mean([p for p,_,_ in per_class]))\n",
    "macro_r = float(np.mean([r for _,r,_ in per_class]))\n",
    "macro_f = float(np.mean([f for _,_,f in per_class]))\n",
    "overall_acc = float((preds == targs).mean())\n",
    "\n",
    "print('\\nPer-class (precision, recall, f1):')\n",
    "for name,(p,r,f) in zip(class_names, per_class):\n",
    "    print(f'{name:>10s}: P={p:.3f} R={r:.3f} F1={f:.3f}')\n",
    "print(f\"\\nMacro avg: P={macro_p:.3f} R={macro_r:.3f} F1={macro_f:.3f}\")\n",
    "print(f'Overall Accuracy: {overall_acc:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736da112",
   "metadata": {},
   "source": [
    "## 8) (Optional) Inference on Test Set + `submission.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fb11c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/submission.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_folder_images(img_dir, transform, class_names):\n",
    "    paths = sorted([p for p in Path(img_dir).glob(\"*.*\") if p.suffix.lower() in {\".jpg\",\".jpeg\",\".png\",\".bmp\"}])\n",
    "    ids, labels = [], []\n",
    "    for p in paths:\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        x = transform(img).unsqueeze(0).to(device)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(1).item()\n",
    "        ids.append(p.name)\n",
    "        labels.append(class_names[pred])\n",
    "    return ids, labels\n",
    "\n",
    "test_dir_A = DATA_DIR / \"test\"\n",
    "test_dir_B = DATA_DIR / \"test_images\"\n",
    "sub_path   = DATA_DIR / \"submission.csv\"\n",
    "\n",
    "if test_dir_A.exists():\n",
    "    ids, labels = predict_folder_images(test_dir_A, valid_tfms, class_names)\n",
    "elif test_dir_B.exists():\n",
    "    ids, labels = predict_folder_images(test_dir_B, valid_tfms, class_names)\n",
    "else:\n",
    "    ids, labels = [], []\n",
    "    print(\"No test directory found; skipping submission.csv.\")\n",
    "\n",
    "if ids:\n",
    "    df = pd.DataFrame({\"image_id\": ids, \"label\": labels})\n",
    "    df.to_csv(sub_path, index=False)\n",
    "    print(\"Saved:\", sub_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "630fe097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_id,label\n",
      "200001.jpg,hispa\n",
      "200002.jpg,normal\n",
      "200003.jpg,hispa\n",
      "200004.jpg,blast\n",
      "200005.jpg,blast\n",
      "200006.jpg,brown_spot\n",
      "200007.jpg,dead_heart\n",
      "200008.jpg,brown_spot\n",
      "200009.jpg,hispa\n"
     ]
    }
   ],
   "source": [
    "!cat ./data/submission.csv | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bbc52c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Notes for Beginners\n",
    "\n",
    "- **Why `timm`?** Lots of pretrained models + convenient transforms. Switching `MODEL_NAME` is an easy way to try stronger backbones.\n",
    "- **Transforms:** Using `timm.data.create_transform` keeps preprocessing consistent with the chosen model.\n",
    "- **Training recipe:** Freeze → train head → unfreeze → fine-tune at smaller LR.\n",
    "- **OOM tips:** Lower `BATCH_SIZE` or try a smaller model (e.g., `efficientnet_b0`, `mobilenetv3_large_100`).\n",
    "- **Save/load:** `torch.save(model.state_dict(), \"model.pth\")`, then `model.load_state_dict(torch.load(\"model.pth\", map_location=device))`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

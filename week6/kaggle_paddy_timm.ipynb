{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d6533b",
   "metadata": {},
   "source": [
    "\n",
    "# Paddy Disease Classification — **PyTorch + timm**\n",
    "\n",
    "This is a **minimal, beginner-friendly** image classification pipeline using **PyTorch** and **[timm](https://github.com/huggingface/pytorch-image-models)** (Torch Image Models).  \n",
    "It mirrors a typical structure with clear comments:\n",
    "\n",
    "1) Setup & configuration  \n",
    "2) Dataset & transforms (train/valid split)  \n",
    "3) Model (transfer learning with `timm.create_model`)  \n",
    "4) Training loop (loss/optimizer)  \n",
    "5) Evaluation & confusion matrix  \n",
    "6) (Optional) Inference on test set + `submission.csv`\n",
    "\n",
    "> **Note:** Point `DATA_DIR` to your local Kaggle Paddy dataset. This notebook avoids any fastai dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e712e2b6",
   "metadata": {},
   "source": [
    "## 1) Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de007fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If timm isn't installed, uncomment:\n",
    "# !pip install timm --quiet\n",
    "\n",
    "import os, random, math, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import timm\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "DATA_DIR = Path(\"/path/to/paddy\")  # <-- change me\n",
    "USE_IMAGEFOLDER = True             # set False to use CSV dataset class below\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9cb6c1",
   "metadata": {},
   "source": [
    "### Get the Data \n",
    "\n",
    "Data comes from <strong><a href=\"https://www.kaggle.com/competitions/paddy-disease-classification\">Paddy Doctor: Paddy Disease Classification</a></strong>.  A Kaggle competition whose goal is to identify the type of disease present in rice paddy leaf images. You can download it directly after signing up for a <a href=\"https://www.kaggle.com/\">Kaggle</a> account.<br>\n",
    "\n",
    "### (Optional)\n",
    "A better way to get the data is through the <strong><a href=\"https://www.kaggle.com/docs/api\">Kaggle CLI (command line interface)</a></strong>.<br>\n",
    "It lets you programatically interact with Kaggle (get/browse datasets and competitions and submit results)<br>\n",
    "BTW- you need to have an API key in order to use the CLI, to get one:<br>\n",
    "Scroll to the API section in your Account settings and click the Create New API Token button.<br>\n",
    "Kaggle will generate a JSON file named kaggle.json and prompt you to save the file to your computer.<br>\n",
    "Put this file in the ~/.kaggle directory on your machine, make sure its only readable by you (chmod 600 /root/.kaggle/kaggle.json)<br>\n",
    "BTW you are probably going to use this again and again, its a good idea to put it in /storage/cfg then symlink it in setup.sh<br>\n",
    "The CLI looks in this place for your key.<br>\n",
    "\t\t\t\t\n",
    "\n",
    " <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82da9457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in ./anaconda3/envs/p311/lib/python3.11/site-packages (1.6.17)\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.7.4.5-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: bleach in ./anaconda3/envs/p311/lib/python3.11/site-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in ./anaconda3/envs/p311/lib/python3.11/site-packages (from kaggle) (2024.6.2)\n",
      "Requirement already satisfied: charset-normalizer in ./anaconda3/envs/p311/lib/python3.11/site-packages (from kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna in ./anaconda3/envs/p311/lib/python3.11/site-packages (from kaggle) (3.7)\n",
      "Requirement already satisfied: protobuf in ./anaconda3/envs/p311/lib/python3.11/site-packages (from kaggle) (5.29.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./anaconda3/envs/p311/lib/python3.11/site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in ./anaconda3/envs/p311/lib/python3.11/site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in ./anaconda3/envs/p311/lib/python3.11/site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in ./anaconda3/envs/p311/lib/python3.11/site-packages (from kaggle) (69.5.1)\n",
      "Requirement already satisfied: six>=1.10 in ./anaconda3/envs/p311/lib/python3.11/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: text-unidecode in ./anaconda3/envs/p311/lib/python3.11/site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in ./anaconda3/envs/p311/lib/python3.11/site-packages (from kaggle) (4.66.4)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in ./anaconda3/envs/p311/lib/python3.11/site-packages (from kaggle) (2.3.0)\n",
      "Requirement already satisfied: webencodings in ./anaconda3/envs/p311/lib/python3.11/site-packages (from kaggle) (0.5.1)\n",
      "Downloading kaggle-1.7.4.5-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kaggle\n",
      "  Attempting uninstall: kaggle\n",
      "    Found existing installation: kaggle 1.6.17\n",
      "    Uninstalling kaggle-1.6.17:\n",
      "      Successfully uninstalled kaggle-1.6.17\n",
      "Successfully installed kaggle-1.7.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f13dfe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref                                                        title                                                  size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
      "---------------------------------------------------------  -----------------------------------------------  ----------  --------------------------  -------------  ---------  ---------------  \n",
      "mosapabdelghany/medical-insurance-cost-dataset             Medical Insurance Cost Dataset                        16425  2025-08-24 11:54:36.533000          18350        367  1.0              \n",
      "zadafiyabhrami/global-crocodile-species-dataset            Global Crocodile Species Dataset                      57473  2025-08-26 08:46:11.950000          12036        325  1.0              \n",
      "codebynadiia/gdp-per-country-20202025                      GDP per Country 2020–2025                              5677  2025-09-04 14:37:43.563000           9667        180  1.0              \n",
      "minahilfatima12328/lifestyle-and-sleep-patterns            Lifestyle and Sleep Patterns                           2604  2025-09-18 14:30:08.190000           1781         38  1.0              \n",
      "saadaliyaseen/analyzing-student-academic-trends            Analyzing Student Academic Trends                      2430  2025-09-10 15:19:31.970000           5481        133  1.0              \n",
      "yashdevladdha/uber-ride-analytics-dashboard                Uber Data Analytics Dashboard                      17324552  2025-08-08 11:13:42.920000          56645       1197  1.0              \n",
      "nabihazahid/spotify-dataset-for-churn-analysis             Spotify Analysis Dataset 2025                         99163  2025-08-28 23:44:35.173000           4649         58  1.0              \n",
      "vedikagupta0/youtube-top-100-songs-2025-dataset            YouTube Top 100 Songs 2025 Dataset                    78598  2025-09-22 11:32:29.517000           1203         25  0.8235294        \n",
      "mjshubham21/movie-dataset-for-analytics-and-visualization  Movie Dataset for Analytics & Visualization        65136772  2025-09-08 12:13:22.823000           2663         36  1.0              \n",
      "emanfatima2025/student-academic-performance-trends         Student Academic Performance Trends                    2430  2025-09-15 16:16:51.340000            798         24  1.0              \n",
      "ghost5612/novel-covid-19-dataset                           Novel Covid-19 Dataset                              8928752  2025-09-18 06:23:52.250000            892         24  0.9411765        \n",
      "msnbehdani/mock-dataset-of-second-hand-car-sales           Car Sales Dataset: Model, Features, and Pricing      501188  2025-08-20 17:47:58.207000           9165        126  1.0              \n",
      "tarekmasryo/generative-ai-tools-and-platforms-2025         Generative AI Tools & Platforms 2025                   3501  2025-09-18 14:05:21.400000            623         27  1.0              \n",
      "adharshinikumar/screentime-vs-mentalwellness-survey-2025   Screen Time vs Mental Wellness Survey - 2025          10214  2025-09-14 15:49:10.683000           2198         33  1.0              \n",
      "ayeshaimran123/academic-stress-level-maintenance-dataset   Academic Stress Level                                  2104  2025-09-12 07:46:59.777000           1697         40  1.0              \n",
      "mosapabdelghany/adult-income-prediction-dataset            Adult Income Prediction Dataset                      460936  2025-08-28 11:56:47.150000           2525         26  1.0              \n",
      "nabeelqureshitiii/student-performance-dataset              Student Performance Dataset                         9364133  2025-08-27 11:28:12.570000           4146         65  1.0              \n",
      "sidraaazam/bmw-global-sales-analysis                       BMW Global Sales Analysis                            853348  2025-09-13 02:59:47.453000           1210         23  1.0              \n",
      "chik0di/health-and-lifestyle-dataset                       Health & Lifestyle Dataset                          2204934  2025-09-17 12:46:52.680000           1490         32  0.9411765        \n",
      "wardabilal/salary-prediction-dataset                       Salary Prediction Dataset                             17048  2025-09-06 14:14:03.303000           2568         32  1.0              \n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets list "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6049e",
   "metadata": {},
   "source": [
    "\n",
    "### (Optional) CSV-based Dataset\n",
    "\n",
    "If your data follows the Kaggle layout (`train_images/` and `train.csv` with `image_id,label`), use this dataset class.  \n",
    "If you rearranged files into class folders, keep `USE_IMAGEFOLDER=True` to use `ImageFolder` instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2339fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import csv\n",
    "\n",
    "class PaddyCSVDataset(Dataset):\n",
    "    def __init__(self, images_dir, csv_path, transform=None, class_to_idx=None):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.transform  = transform\n",
    "        rows = []\n",
    "        with open(csv_path, newline=\"\") as f:\n",
    "            rd = csv.DictReader(f)\n",
    "            if rd.fieldnames is None or 'image_id' not in rd.fieldnames:\n",
    "                f.seek(0)\n",
    "                rd = csv.reader(f)\n",
    "                for r in rd:\n",
    "                    if len(r) >= 2:\n",
    "                        rows.append((r[0], r[1]))\n",
    "            else:\n",
    "                for r in rd:\n",
    "                    rows.append((r['image_id'], r['label']))\n",
    "        self.items = rows\n",
    "        labels = sorted(set(label for _, label in self.items))\n",
    "        self.class_to_idx = class_to_idx or {c:i for i,c in enumerate(labels)}\n",
    "        self.idx_to_class = {v:k for k,v in self.class_to_idx.items()}\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, idx):\n",
    "        img_id, label_name = self.items[idx]\n",
    "        img = Image.open(self.images_dir / img_id).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        y = self.class_to_idx[label_name]\n",
    "        return img, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58329c15",
   "metadata": {},
   "source": [
    "## 2) Transforms (match the timm model’s expectations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a14265f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved input size: (224, 224)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL_NAME = \"resnet18\"  # try: 'efficientnet_b0', 'convnext_tiny', 'mobilenetv3_large_100', ...\n",
    "\n",
    "config = timm.data.resolve_data_config({}, model=MODEL_NAME)\n",
    "if 'input_size' in config and len(config['input_size']) == 3:\n",
    "    _, H, W = config['input_size']\n",
    "else:\n",
    "    H = W = 224\n",
    "\n",
    "train_tfms = timm.data.create_transform(\n",
    "    **config,\n",
    "    is_training=True,\n",
    "    hflip=0.5,\n",
    "    color_jitter=None,\n",
    "    auto_augment=None\n",
    ")\n",
    "\n",
    "valid_tfms = timm.data.create_transform(\n",
    "    **config,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "print(\"Resolved input size:\", (H, W))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f6870",
   "metadata": {},
   "source": [
    "## 3) Datasets & Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0afefd74",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/paddy/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     valid_ds \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(valid_dir, transform\u001b[38;5;241m=\u001b[39mvalid_tfms)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     full_ds \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(train_dir, transform\u001b[38;5;241m=\u001b[39mtrain_tfms)\n\u001b[1;32m     13\u001b[0m     n_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(full_ds)\n\u001b[1;32m     14\u001b[0m     n_valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m n_total)\n",
      "File \u001b[0;32m~/anaconda3/envs/p311/lib/python3.11/site-packages/torchvision/datasets/folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    321\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    327\u001b[0m ):\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    329\u001b[0m         root,\n\u001b[1;32m    330\u001b[0m         loader,\n\u001b[1;32m    331\u001b[0m         IMG_EXTENSIONS \u001b[38;5;28;01mif\u001b[39;00m is_valid_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    332\u001b[0m         transform\u001b[38;5;241m=\u001b[39mtransform,\n\u001b[1;32m    333\u001b[0m         target_transform\u001b[38;5;241m=\u001b[39mtarget_transform,\n\u001b[1;32m    334\u001b[0m         is_valid_file\u001b[38;5;241m=\u001b[39mis_valid_file,\n\u001b[1;32m    335\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[1;32m    336\u001b[0m     )\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m~/anaconda3/envs/p311/lib/python3.11/site-packages/torchvision/datasets/folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_classes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n\u001b[1;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[1;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m~/anaconda3/envs/p311/lib/python3.11/site-packages/torchvision/datasets/folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find_classes(directory)\n",
      "File \u001b[0;32m~/anaconda3/envs/p311/lib/python3.11/site-packages/torchvision/datasets/folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mscandir(directory) \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/paddy/train'"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "if USE_IMAGEFOLDER:\n",
    "    train_dir = DATA_DIR / \"train\"\n",
    "    valid_dir = DATA_DIR / \"valid\"\n",
    "    test_dir  = DATA_DIR / \"test\"\n",
    "\n",
    "    if valid_dir.exists():\n",
    "        train_ds = datasets.ImageFolder(train_dir, transform=train_tfms)\n",
    "        valid_ds = datasets.ImageFolder(valid_dir, transform=valid_tfms)\n",
    "    else:\n",
    "        full_ds = datasets.ImageFolder(train_dir, transform=train_tfms)\n",
    "        n_total = len(full_ds)\n",
    "        n_valid = int(0.1 * n_total)\n",
    "        n_train = n_total - n_valid\n",
    "        train_ds, valid_ds = random_split(full_ds, [n_train, n_valid], generator=torch.Generator().manual_seed(42))\n",
    "        train_ds.dataset = full_ds\n",
    "        valid_ds.dataset = full_ds\n",
    "\n",
    "    class_names = train_ds.dataset.classes if hasattr(train_ds, 'dataset') else train_ds.classes\n",
    "    num_classes = len(class_names)\n",
    "else:\n",
    "    train_images = DATA_DIR / \"train_images\"\n",
    "    train_csv    = DATA_DIR / \"train.csv\"\n",
    "    full_ds = PaddyCSVDataset(train_images, train_csv, transform=train_tfms)\n",
    "    n_total = len(full_ds)\n",
    "    n_valid = int(0.1 * n_total)\n",
    "    n_train = n_total - n_valid\n",
    "    train_ds, valid_ds = random_split(full_ds, [n_train, n_valid], generator=torch.Generator().manual_seed(42))\n",
    "    class_names = list(full_ds.class_to_idx.keys())\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Classes ({num_classes}):\", class_names[:10], \"...\" if num_classes>10 else \"\")\n",
    "print(\"Train/Valid sizes:\", len(train_ds), len(valid_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567303c6",
   "metadata": {},
   "source": [
    "## 4) Model (Transfer Learning with **timm**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d11c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=num_classes)\n",
    "print(\"Classifier layer:\", model.get_classifier())\n",
    "\n",
    "# Freeze backbone; train classifier head first\n",
    "for name, p in model.named_parameters():\n",
    "    p.requires_grad = False\n",
    "clf_name = model.get_classifier()\n",
    "for name, p in model.named_parameters():\n",
    "    if clf_name in name:\n",
    "        p.requires_grad = True\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c23d6",
   "metadata": {},
   "source": [
    "## 5) Training Loop (minimal, commented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087df5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "EPOCHS = 5\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    va_loss, va_acc = evaluate(model, valid_loader, criterion)\n",
    "    print(f\"epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.3f} | valid loss {va_loss:.4f} acc {va_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b06063",
   "metadata": {},
   "source": [
    "### (Optional) Fine-tune the whole network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3ec264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "EPOCHS_FT = 3\n",
    "for epoch in range(1, EPOCHS_FT+1):\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    va_loss, va_acc = evaluate(model, valid_loader, criterion)\n",
    "    print(f\"[FT] epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.3f} | valid loss {va_loss:.4f} acc {va_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9025eae",
   "metadata": {},
   "source": [
    "## 6) Evaluation & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2af28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_all_preds_targets(model, loader):\n",
    "    model.eval()\n",
    "    preds, targs = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        preds.append(logits.argmax(1).cpu().numpy())\n",
    "        targs.append(yb.numpy())\n",
    "    return np.concatenate(preds), np.concatenate(targs)\n",
    "\n",
    "preds, targs = get_all_preds_targets(model, valid_loader)\n",
    "\n",
    "num_classes = len(class_names)\n",
    "cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "for t, p in zip(targs, preds):\n",
    "    cm[t, p] += 1\n",
    "\n",
    "print(\"Confusion Matrix (truncated up to 10×10):\")\n",
    "show_n = min(10, num_classes)\n",
    "print(cm[:show_n, :show_n])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736da112",
   "metadata": {},
   "source": [
    "## 7) (Optional) Inference on Test Set + `submission.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb11c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_folder_images(img_dir, transform, class_names):\n",
    "    paths = sorted([p for p in Path(img_dir).glob(\"*.*\") if p.suffix.lower() in {\".jpg\",\".jpeg\",\".png\",\".bmp\"}])\n",
    "    ids, labels = [], []\n",
    "    for p in paths:\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        x = transform(img).unsqueeze(0).to(device)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(1).item()\n",
    "        ids.append(p.name)\n",
    "        labels.append(class_names[pred])\n",
    "    return ids, labels\n",
    "\n",
    "test_dir_A = DATA_DIR / \"test\"\n",
    "test_dir_B = DATA_DIR / \"test_images\"\n",
    "sub_path   = DATA_DIR / \"submission.csv\"\n",
    "\n",
    "if test_dir_A.exists():\n",
    "    ids, labels = predict_folder_images(test_dir_A, valid_tfms, class_names)\n",
    "elif test_dir_B.exists():\n",
    "    ids, labels = predict_folder_images(test_dir_B, valid_tfms, class_names)\n",
    "else:\n",
    "    ids, labels = [], []\n",
    "    print(\"No test directory found; skipping submission.csv.\")\n",
    "\n",
    "if ids:\n",
    "    df = pd.DataFrame({\"image_id\": ids, \"label\": labels})\n",
    "    df.to_csv(sub_path, index=False)\n",
    "    print(\"Saved:\", sub_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bbc52c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Notes for Beginners\n",
    "\n",
    "- **Why `timm`?** Lots of pretrained models + convenient transforms. Switching `MODEL_NAME` is an easy way to try stronger backbones.\n",
    "- **Transforms:** Using `timm.data.create_transform` keeps preprocessing consistent with the chosen model.\n",
    "- **Training recipe:** Freeze → train head → unfreeze → fine-tune at smaller LR.\n",
    "- **OOM tips:** Lower `BATCH_SIZE` or try a smaller model (e.g., `efficientnet_b0`, `mobilenetv3_large_100`).\n",
    "- **Save/load:** `torch.save(model.state_dict(), \"model.pth\")`, then `model.load_state_dict(torch.load(\"model.pth\", map_location=device))`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

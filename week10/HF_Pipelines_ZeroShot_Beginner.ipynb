{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6e37e80",
   "metadata": {},
   "source": [
    "\n",
    "# Hugging Face `pipeline`: Beginner-Friendly Tour (with Zero‚ÄëShot Demos)\n",
    "\n",
    "**Audience:** 4th‚Äëyear CS students ‚Äî beginner friendly  \n",
    "**Goal:** Learn how to use ü§ó Transformers **`pipeline`** to run **pretrained** models quickly (no fine‚Äëtuning) and\n",
    "see where **zero‚Äëshot** pipelines are powerful.\n",
    "\n",
    "> This follows the spirit of HF LLM Course Chapter 1.3 (Pipelines) and adds gentle explanations, tips, and exercises.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb38d4",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup (Install, Seeds, Device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e637c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed, uncomment to install dependencies:\n",
    "# %pip install -U transformers torch accelerate sentencepiece sacremoses datasets\n",
    "\n",
    "import os, random, time\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# --- Reproducibility (mostly affects sampling-based generators) ---\n",
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# --- Device selection for pipelines: device=0 means CUDA GPU 0; -1 means CPU ---\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available(), \"| pipeline(device) =\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93d8c5e",
   "metadata": {},
   "source": [
    "\n",
    "## 1) What *is* `pipeline`?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fdbe39",
   "metadata": {},
   "source": [
    "\n",
    "`pipeline(task, model=..., tokenizer=...)` is a **high‚Äëlevel helper** that hides the usual steps:\n",
    "1. **Tokenize** raw inputs (text ‚Üí token IDs)  \n",
    "2. **Run the model** forward pass  \n",
    "3. **Post‚Äëprocess** outputs into friendly Python objects (labels, scores, text, etc.)\n",
    "\n",
    "If you **omit `model=`**, it will pick a sensible default from the Hub for that task.  \n",
    "You can always **override** with a specific model ID later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fae4e9",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Minimal example: Sentiment Analysis (zero‚Äëshot pretrained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aebc55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Default pretrained model for sentiment analysis (zero-shot use: we don't fine-tune anything here)\n",
    "sentiment = pipeline(\"sentiment-analysis\", device=device)\n",
    "result = sentiment(\"I absolutely love using Transformers‚Äîit's so convenient!\")\n",
    "print(result)  # [{'label': 'POSITIVE', 'score': ...}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a20a7d",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 Batch inputs (faster than looped single calls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922769bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts = [\n",
    "    \"Today is an amazing day üòÑ\",\n",
    "    \"The interface is confusing and slow.\",\n",
    "    \"Meh, it's fine but could be better.\",\n",
    "]\n",
    "for out in sentiment(texts):\n",
    "    print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8d6bb",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3 Under the hood\n",
    "- Picks / loads a **tokenizer** and **model** (from the Hub cache after first download)  \n",
    "- Handles **padding/truncation** defaults (configurable)  \n",
    "- Converts logits ‚Üí **labels/scores**  \n",
    "- Runs on **GPU** if available, else CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b9da6e",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Zero‚ÄëShot Text Classification (No Task‚ÄëSpecific Training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82d156e",
   "metadata": {},
   "source": [
    "\n",
    "**Problem:** Classify a sentence into **labels you choose on the fly** (e.g., `\"bug report\"`, `\"feature request\"`), even if the model was **not trained** specifically for those labels.\n",
    "\n",
    "**How:** Use a Natural Language Inference (NLI) model (e.g., `facebook/bart-large-mnli`) under the hood.  \n",
    "The pipeline scores how much the text **entails** each label hypothesis (e.g., ‚ÄúThis text is about *a bug report*‚Äù).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zs = pipeline(\n",
    "    task=\"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "candidate_labels = [\"customer support\", \"bug report\", \"feature request\", \"pricing\", \"complaint\"]\n",
    "text = \"The app keeps crashing whenever I try to upload a photo. Please fix it.\"\n",
    "zs(text, candidate_labels=candidate_labels)  # labels with probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2b9796",
   "metadata": {},
   "source": [
    "\n",
    "**Batch example:** Pass several texts to save time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce90cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_texts = [\n",
    "    \"Could you add SSO with Okta?\",\n",
    "    \"My credit card was charged twice.\",\n",
    "    \"New release works flawlessly. Kudos to the team!\",\n",
    "]\n",
    "zs(batch_texts, candidate_labels=candidate_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907b4c43",
   "metadata": {},
   "source": [
    "\n",
    "**Tip:** Ask for **all scores** to see probabilities for every label (useful for multi‚Äëlabel cases).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65886112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zs(text, candidate_labels=candidate_labels, multi_label=True)  # treats labels as independent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8597f52e",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Masked Language Modeling ‚Äî `fill-mask` (Pretrained, Zero‚ÄëShot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19cf58a",
   "metadata": {},
   "source": [
    "\n",
    "Given a sentence with a **mask token**, predict the missing word(s) using a pretrained masked‚ÄëLM (`bert-base-uncased`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8fcad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fill = pipeline(\"fill-mask\", model=\"bert-base-uncased\", device=device)\n",
    "fill(\"Transformers are the most [MASK] library for NLP.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a95374",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Named Entity Recognition (NER) ‚Äî Token Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76a8858",
   "metadata": {},
   "source": [
    "\n",
    "Extract entities (people, organizations, locations) from text.  \n",
    "`aggregation_strategy=\"simple\"` merges word‚Äëpieces into full tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ed48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ner = pipeline(\"token-classification\", aggregation_strategy=\"simple\", device=device)\n",
    "ner(\"Hugging Face is based in Paris and New York, and Google is one of its partners.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc89c4f",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Extractive Question Answering (QA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48aa140",
   "metadata": {},
   "source": [
    "\n",
    "Given a **context** and a **question**, extract the answer span.  \n",
    "We use a pretrained QA model (often fine‚Äëtuned on SQuAD) as‚Äëis (zero‚Äëshot on your text).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f613a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa = pipeline(\"question-answering\", device=device)\n",
    "context = \\\"\\\"\\\"\n",
    "The Transformer architecture, introduced in 2017, replaced recurrent networks for many NLP tasks.\n",
    "It relies entirely on attention mechanisms to draw global dependencies between input and output.\n",
    "\\\"\\\"\\\"\n",
    "qa({\"question\": \"What architecture replaced recurrent networks?\", \"context\": context})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3a1758",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Summarization (Seq2Seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63802e8f",
   "metadata": {},
   "source": [
    "\n",
    "Abstractive summarization with a pretrained seq2seq model. Keep `max_length` small for quick demos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123f1324",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summarizer = pipeline(\"summarization\", device=device)\n",
    "article = (\n",
    "    \"Transformers have revolutionized natural language processing by enabling \"\n",
    "    \"parallel training and capturing long-range dependencies efficiently. \"\n",
    "    \"Libraries like Hugging Face Transformers provide user-friendly APIs for \"\n",
    "    \"inference and fine-tuning, accelerating research and production adoption.\"\n",
    ")\n",
    "summarizer(article, max_length=40, min_length=10, do_sample=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43716e81",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Translation (EN ‚Üí ES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb097a3e",
   "metadata": {},
   "source": [
    "\n",
    "Pretrained machine translation models can translate out‚Äëof‚Äëthe‚Äëbox.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b7a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\", device=device)\n",
    "translator(\"Transformers make transfer learning straightforward and effective.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e529eae0",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Text Generation (Causal LM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a7624",
   "metadata": {},
   "source": [
    "\n",
    "Generate continuations with an autoregressive model (e.g., `gpt2`). Use a small `max_length` for speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\", device=device)\n",
    "prompt = \"In a future where AI assists every developer,\"\n",
    "generator(prompt, max_length=40, num_return_sequences=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43de8452",
   "metadata": {},
   "source": [
    "\n",
    "## 9) (Optional) Zero‚ÄëShot **Image** Classification with CLIP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f14c01",
   "metadata": {},
   "source": [
    "\n",
    "Zero‚Äëshot also works for images using **text prompts** as labels.  \n",
    "Below is commented code (needs internet to fetch an image). Uncomment to try.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a3804",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from transformers import pipeline\n",
    "# import requests\n",
    "# from PIL import Image\n",
    "# from io import BytesIO\n",
    "# \n",
    "# img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/image_classification.jpeg\"\n",
    "# image = Image.open(BytesIO(requests.get(img_url).content))\n",
    "# labels = [\"a cat\", \"a dog\", \"a bird\", \"a car\"]\n",
    "# clip = pipeline(\"zero-shot-image-classification\", model=\"openai/clip-vit-base-patch32\", device=device)\n",
    "# clip(image, candidate_labels=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187cfe79",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Tiny Timing Utility (CPU vs GPU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6faf107",
   "metadata": {},
   "source": [
    "\n",
    "Measure average runtime to compare CPU vs GPU or different models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26224b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def time_call(fn, *args, repeat=3, **kwargs):\n",
    "    # Return average runtime (s) of calling fn(*args, **kwargs)\n",
    "    times = []\n",
    "    import time as _t\n",
    "    for _ in range(repeat):\n",
    "        t0 = _t.time()\n",
    "        _ = fn(*args, **kwargs)\n",
    "        times.append(_t.time() - t0)\n",
    "    return sum(times) / len(times)\n",
    "\n",
    "avg = time_call(sentiment, texts, repeat=2)\n",
    "print(f\"Avg runtime over 2 runs: {avg:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2301eaf",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Exercises (Try It Yourself)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb36dbe",
   "metadata": {},
   "source": [
    "\n",
    "1) **Zero‚ÄëShot Labels:** Change `candidate_labels` in Section 2 (e.g., [\"how‚Äëto\", \"billing\", \"outage\", \"praise\"]).  \n",
    "2) **Domain Shift:** Paste your own domain text into QA / Summarization; compare outputs.  \n",
    "3) **Model Cards:** In the Hub, open the model page used by a pipeline (e.g., `facebook/bart-large-mnli`). Skim the **Model Card** for intended use & limitations.  \n",
    "4) **Batching:** Time single‚Äëinput vs list‚Äëinput for sentiment. Which is faster?  \n",
    "5) **Multilingual:** Try `xlm-roberta-large` with NER or zero‚Äëshot and test on non‚ÄëEnglish text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc288cf1",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Troubleshooting Tips\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfbe877",
   "metadata": {},
   "source": [
    "\n",
    "- **Downloads are slow / blocked:** Pre‚Äëdownload models or set a local HF cache (`HF_HOME`).  \n",
    "- **Out of memory:** Use smaller models (e.g., `distilbert-base-uncased`), smaller `max_length`, or switch to CPU.  \n",
    "- **Mismatched mask token:** For `fill-mask`, ensure your input uses the right mask (e.g., `[MASK]` for BERT).  \n",
    "- **Reproducibility:** Generation may vary slightly; set seeds and keep sequences short in demos.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73a259f4",
   "metadata": {},
   "source": [
    "\n",
    "# Sentence Classification (Sentiment) — Linear Probe vs Full Fine-Tuning on **TweetEval: Sentiment** (PyTorch Training Loop)\n",
    "\n",
    "**Audience:** 4th-year Computer Science students  \n",
    "**Task:** Multi-class sentiment classification (**negative / neutral / positive**) on the **TweetEval** benchmark (subset: `sentiment`).\n",
    "\n",
    "This version is identical to the previous notebook **except** the training sections now use a **traditional PyTorch loop** (no `Trainer`).\n",
    "\n",
    "You'll build and compare two approaches using a Hugging Face encoder:\n",
    "1. **Linear Probe (Frozen Encoder):** Freeze the transformer encoder and train only a small classification head.\n",
    "2. **Full Fine-Tuning:** Unfreeze the encoder and fine-tune end-to-end.\n",
    "\n",
    "We'll evaluate both on the same test set and visualize improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17385139",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup & Reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed, uncomment to install:\n",
    "# %pip install -U transformers datasets accelerate evaluate scikit-learn matplotlib\n",
    "\n",
    "import os, random, time, json, math\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, get_linear_schedule_with_warmup)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00697ba",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a7d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONFIG = {\n",
    "    \"dataset_name\": \"tweet_eval\",\n",
    "    \"dataset_subset\": \"sentiment\",   # 3-way: negative(0), neutral(1), positive(2)\n",
    "    \"text_col\": \"text\",\n",
    "    \"label_col\": \"label\",\n",
    "    \"labels\": [\"negative\", \"neutral\", \"positive\"],\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "    \"max_length\": 128,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 32,\n",
    "    \"epochs_probe\": 2,           # linear-probe training epochs\n",
    "    \"epochs_finetune\": 3,        # full finetune epochs\n",
    "    \"learning_rate_probe\": 5e-4, # higher since only head trains\n",
    "    \"learning_rate_finetune\": 2e-5,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_ratio\": 0.06,\n",
    "    \"subset_fraction\": 0.3,      # None for full data; use fraction like 0.3 for speed\n",
    "    \"output_dir\": \"checkpoints_tweeteval_sentiment_ptloop\"\n",
    "}\n",
    "print(json.dumps(CONFIG, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fafe01",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Load the **TweetEval: Sentiment** Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a382984",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw = load_dataset(CONFIG[\"dataset_name\"], CONFIG[\"dataset_subset\"])\n",
    "\n",
    "# Optionally downsample for a quick demo run\n",
    "subset_fraction = CONFIG[\"subset_fraction\"]\n",
    "if subset_fraction is not None and 0 < subset_fraction < 1:\n",
    "    def take_fraction(dset, frac):\n",
    "        n = max(30, int(len(dset) * frac))  # keep a minimum\n",
    "        return dset.shuffle(seed=SEED).select(range(n))\n",
    "    raw = DatasetDict({\n",
    "        \"train\": take_fraction(raw[\"train\"], subset_fraction),\n",
    "        \"validation\": take_fraction(raw[\"validation\"], subset_fraction),\n",
    "        \"test\": raw[\"test\"]  # keep full test for better generalization measurement\n",
    "    })\n",
    "\n",
    "raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be3888c",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f284cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"], use_fast=True)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[CONFIG[\"text_col\"]], truncation=True, max_length=CONFIG[\"max_length\"])\n",
    "\n",
    "remove_cols = [c for c in raw[\"train\"].column_names if c not in (CONFIG[\"text_col\"], CONFIG[\"label_col\"])]\n",
    "tokenized = raw.map(tokenize_fn, batched=True, remove_columns=remove_cols)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "num_labels = len(CONFIG[\"labels\"])\n",
    "label_names = CONFIG[\"labels\"]\n",
    "\n",
    "# Set format for PyTorch Dataloaders\n",
    "columns = [\"input_ids\", \"attention_mask\", CONFIG[\"label_col\"]]\n",
    "tokenized = tokenized.with_format(type=\"torch\", columns=columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e6245e",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab7fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics_from_preds(preds, labels):\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"macro_f1\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394276e3",
   "metadata": {},
   "source": [
    "\n",
    "### Helper: Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9ad191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\", labels=None):\n",
    "    if labels is None:\n",
    "        labels = [str(i) for i in sorted(np.unique(y_true))]\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(labels))))\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm)  # default colormap (no custom colors)\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_yticks(range(len(labels)))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Annotate counts\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fdf0f8",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Model Builder (unchanged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7031d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(freeze_encoder=True):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        CONFIG[\"model_name\"], num_labels=len(label_names)\n",
    "    )\n",
    "    if freeze_encoder:\n",
    "        if hasattr(model, \"distilbert\"):\n",
    "            for p in model.distilbert.parameters():\n",
    "                p.requires_grad = False\n",
    "        else:\n",
    "            base = getattr(model, \"bert\", None) or getattr(model, \"roberta\", None) or getattr(model, \"deberta\", None)\n",
    "            if base is not None:\n",
    "                for p in base.parameters():\n",
    "                    p.requires_grad = False\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1951e3f6",
   "metadata": {},
   "source": [
    "\n",
    "## 6) PyTorch Training Utilities (New)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07d6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def make_dataloaders(tokenized, split, batch_size, shuffle, collate_fn):\n",
    "    return DataLoader(tokenized[split], batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            labels = batch[CONFIG[\"label_col\"]].to(device)\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if k != CONFIG[\"label_col\"]}\n",
    "            outputs = model(**batch, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    metrics = compute_metrics_from_preds(all_preds, all_labels)\n",
    "    return avg_loss, metrics, all_preds, all_labels\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs, lr, weight_decay, warmup_ratio):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                  lr=lr, weight_decay=weight_decay)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    warmup_steps = max(1, int(warmup_ratio * total_steps))\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "    best_state = None\n",
    "    best_macro_f1 = -1.0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            labels = batch[CONFIG[\"label_col\"]].to(device)\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if k != CONFIG[\"label_col\"]}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        val_loss, val_metrics, _, _ = evaluate_model(model, val_loader, device)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | Train Loss {train_loss:.4f} | Val Loss {val_loss:.4f} | \"\n",
    "              f\"Val Acc {val_metrics['accuracy']:.4f} | Val Macro-F1 {val_metrics['macro_f1']:.4f}\")\n",
    "\n",
    "        if val_metrics[\"macro_f1\"] > best_macro_f1:\n",
    "            best_macro_f1 = val_metrics[\"macro_f1\"]\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5afc8e2",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Baseline: **Linear Probe** (Frozen Encoder) — PyTorch Loop (Changed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436a7b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "probe_model = build_model(freeze_encoder=True)\n",
    "\n",
    "train_loader = make_dataloaders(tokenized, \"train\", CONFIG[\"per_device_train_batch_size\"], True, data_collator)\n",
    "val_loader   = make_dataloaders(tokenized, \"validation\", CONFIG[\"per_device_eval_batch_size\"], False, data_collator)\n",
    "test_loader  = make_dataloaders(tokenized, \"test\", CONFIG[\"per_device_eval_batch_size\"], False, data_collator)\n",
    "\n",
    "t0 = time.time()\n",
    "probe_model = train_model(\n",
    "    probe_model, train_loader, val_loader, device,\n",
    "    epochs=CONFIG[\"epochs_probe\"],\n",
    "    lr=CONFIG[\"learning_rate_probe\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"]\n",
    ")\n",
    "probe_train_time = time.time() - t0\n",
    "\n",
    "probe_test_loss, probe_test_metrics, probe_preds, y_test = evaluate_model(probe_model, test_loader, device)\n",
    "\n",
    "probe_test = {\"eval_accuracy\": probe_test_metrics[\"accuracy\"], \"eval_macro_f1\": probe_test_metrics[\"macro_f1\"]}\n",
    "print(\"Probe Test:\", probe_test, \"| Train time (s):\", round(probe_train_time, 2))\n",
    "\n",
    "plot_confusion_matrix(y_test, probe_preds, title=\"Frozen Encoder (Linear Probe) — Test\", labels=label_names)\n",
    "print(classification_report(y_test, probe_preds, target_names=label_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e577ad",
   "metadata": {},
   "source": [
    "\n",
    "## 8) **Full Fine-Tuning** (Encoder + Head) — PyTorch Loop (Changed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec12244",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ft_model = build_model(freeze_encoder=False)\n",
    "\n",
    "t0 = time.time()\n",
    "ft_model = train_model(\n",
    "    ft_model, train_loader, val_loader, device,\n",
    "    epochs=CONFIG[\"epochs_finetune\"],\n",
    "    lr=CONFIG[\"learning_rate_finetune\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"]\n",
    ")\n",
    "ft_train_time = time.time() - t0\n",
    "\n",
    "ft_test_loss, ft_test_metrics, ft_preds, y_test2 = evaluate_model(ft_model, test_loader, device)\n",
    "\n",
    "ft_test = {\"eval_accuracy\": ft_test_metrics[\"accuracy\"], \"eval_macro_f1\": ft_test_metrics[\"macro_f1\"]}\n",
    "print(\"Finetune Test:\", ft_test, \"| Train time (s):\", round(ft_train_time, 2))\n",
    "\n",
    "plot_confusion_matrix(y_test2, ft_preds, title=\"Full Fine-Tuned — Test\", labels=label_names)\n",
    "print(classification_report(y_test2, ft_preds, target_names=label_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62553026",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Compare Results (unchanged logic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c9c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def metric(d, key):\n",
    "    return float(d.get(key, \"nan\"))\n",
    "\n",
    "probe_acc = metric(probe_test, \"eval_accuracy\")\n",
    "probe_f1m = metric(probe_test, \"eval_macro_f1\")\n",
    "ft_acc = metric(ft_test, \"eval_accuracy\")\n",
    "ft_f1m = metric(ft_test, \"eval_macro_f1\")\n",
    "\n",
    "print(f\"Probe — Test Accuracy: {probe_acc:.4f} | Macro F1: {probe_f1m:.4f}\")\n",
    "print(f\"FT    — Test Accuracy: {ft_acc:.4f} | Macro F1: {ft_f1m:.4f}\")\n",
    "print(f\"Δ Accuracy: {ft_acc - probe_acc:+.4f}\")\n",
    "print(f\"Δ Macro F1: {ft_f1m - probe_f1m:+.4f}\")\n",
    "\n",
    "labels_disp = [\"Probe (Frozen)\", \"Finetuned\"]\n",
    "accs = [probe_acc, ft_acc]\n",
    "f1s = [probe_f1m, ft_f1m]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(labels_disp, accs)\n",
    "plt.title(\"Test Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(labels_disp, f1s)\n",
    "plt.title(\"Test Macro-F1\")\n",
    "plt.ylabel(\"Macro-F1\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c2b40",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Discussion & Extensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173096c3",
   "metadata": {},
   "source": [
    "\n",
    "- **Try other datasets:** `imdb`, `amazon_polarity`, `yelp_polarity`, or other `tweet_eval` tasks.\n",
    "- **Try other encoders:** `bert-base-uncased`, `roberta-base`, `google/electra-small-discriminator`.\n",
    "- **Compute budget:** Adjust `subset_fraction` for CPU demos vs. full GPU runs.\n",
    "- **PEFT:** Explore LoRA/adapters to approach full-FT accuracy with less compute.\n",
    "- **Error analysis:** Inspect misclassifications; per-class precision/recall; calibration.\n",
    "- **Robustness:** Evaluate on different time slices or domains.\n",
    "\n",
    "> ✍️ **Short write-up prompt:** Explain why full fine-tuning improves performance vs. a frozen encoder. Relate to representation learning and task/domain adaptation.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

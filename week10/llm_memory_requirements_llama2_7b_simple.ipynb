{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e9d4b2",
   "metadata": {},
   "source": [
    "\n",
    "# Simplified LLM Memory Requirements — Transformer Decoder (LLaMA‑2 7B Example)\n",
    "\n",
    "*Generated on 2025-10-28 19:21:59*\n",
    "\n",
    "This tutorial estimates **GPU memory requirements** for a **decoder‑only Transformer** such as **LLaMA‑2 7B**,  \n",
    "using simple **rules of thumb** for both **training** and **inference**.\n",
    "\n",
    "We assume:\n",
    "- FP16/BF16 precision (2 bytes / float)\n",
    "- AdamW optimizer\n",
    "- No activation checkpointing\n",
    "- Context length = 4096 tokens  \n",
    "- Batch size = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a8b922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# Define variables (edit these)\n",
    "# ==============================\n",
    "\n",
    "# Model architecture\n",
    "num_params   = 6.74e9   # total parameters\n",
    "num_layers   = 32       # number of transformer layers\n",
    "hidden_size  = 4096     # model hidden dimension\n",
    "num_heads    = 32       # attention heads\n",
    "head_dim     = hidden_size // num_heads  # dimension per head (128)\n",
    "\n",
    "# Data / batch\n",
    "batch_size   = 4        # batch size (examples)\n",
    "seq_len      = 4096     # context length (tokens)\n",
    "\n",
    "# Precision and constants\n",
    "bytes_per_float = 2     # FP16/BF16 precision\n",
    "adam_overhead   = 12    # AdamW optimizer overhead (m, v, master weights)\n",
    "activation_scale = 7.0  # rule of thumb for activations per token per layer\n",
    "kv_bytes_per_elem = 2   # FP16 KV cache element size\n",
    "\n",
    "# Conversion constant\n",
    "BYTES_PER_GIB = 1.073741824e9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d32479cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training total memory: 128.4 GiB\n",
      "  Parameters + optimizer: 100.4 GiB\n",
      "  Activations: 28.0 GiB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================\n",
    "# Training memory estimate\n",
    "# ==============================\n",
    "\n",
    "def estimate_training_memory(params, layers, hidden, batch, seq,\n",
    "                             bytes_per_float=2, adam_overhead=12, activation_scale=7.0):\n",
    "    \"\"\"Estimate training memory (GiB).\"\"\"\n",
    "    # Model weights + gradients + optimizer\n",
    "    param_bytes = params * (bytes_per_float + bytes_per_float + adam_overhead)\n",
    "    # Activations\n",
    "    act_bytes = batch * seq * layers * hidden * bytes_per_float * activation_scale\n",
    "    total_bytes = param_bytes + act_bytes\n",
    "    return total_bytes / BYTES_PER_GIB, param_bytes / BYTES_PER_GIB, act_bytes / BYTES_PER_GIB\n",
    "\n",
    "train_total, train_params, train_acts = estimate_training_memory(\n",
    "    num_params, num_layers, hidden_size, batch_size, seq_len,\n",
    "    bytes_per_float, adam_overhead, activation_scale)\n",
    "\n",
    "print(f\"Training total memory: {train_total:.1f} GiB\")\n",
    "print(f\"  Parameters + optimizer: {train_params:.1f} GiB\")\n",
    "print(f\"  Activations: {train_acts:.1f} GiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c70eb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference total (prefill): 20.6 GiB\n",
      "  Weights: 12.6 GiB\n",
      "  KV cache: 8.0 GiB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================\n",
    "# Inference memory estimate (prefill)\n",
    "# ==============================\n",
    "\n",
    "def kv_cache_bytes(layers, batch, seq, heads, head_dim, bytes_per_elem=2):\n",
    "    \"\"\"Approximate KV cache size in bytes.\"\"\"\n",
    "    return layers * batch * heads * seq * head_dim * bytes_per_elem * 2  # K and V\n",
    "\n",
    "def estimate_inference_memory(params, layers, batch, seq, heads, head_dim,\n",
    "                              bytes_per_float=2, kv_bytes_per_elem=2):\n",
    "    \"\"\"Estimate inference memory (GiB).\"\"\"\n",
    "    weights = params * bytes_per_float\n",
    "    kv_cache = kv_cache_bytes(layers, batch, seq, heads, head_dim, kv_bytes_per_elem)\n",
    "    total = weights + kv_cache\n",
    "    return total / BYTES_PER_GIB, weights / BYTES_PER_GIB, kv_cache / BYTES_PER_GIB\n",
    "\n",
    "infer_total, infer_weights, infer_kv = estimate_inference_memory(\n",
    "    num_params, num_layers, batch_size, seq_len, num_heads, head_dim,\n",
    "    bytes_per_float, kv_bytes_per_elem)\n",
    "\n",
    "print(f\"Inference total (prefill): {infer_total:.1f} GiB\")\n",
    "print(f\"  Weights: {infer_weights:.1f} GiB\")\n",
    "print(f\"  KV cache: {infer_kv:.1f} GiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30a527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# KV cache scaling with context length\n",
    "# ==============================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seq_values = np.array([512, 1024, 2048, 4096, 8192, 16384])\n",
    "kv_gib = [\n",
    "    kv_cache_bytes(num_layers, batch_size, s, num_heads, head_dim, kv_bytes_per_elem) / BYTES_PER_GIB\n",
    "    for s in seq_values\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(7, 4.5))\n",
    "plt.plot(seq_values, kv_gib)\n",
    "plt.xlabel(\"Context length (tokens)\")\n",
    "plt.ylabel(\"KV cache memory (GiB)\")\n",
    "plt.title(\"LLaMA‑2 7B — KV Cache Memory vs Context Length\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43284153",
   "metadata": {},
   "source": [
    "\n",
    "## Simple Rules of Thumb\n",
    "\n",
    "- **Training memory ≈ 16 bytes / parameter + activation cost.**  \n",
    "  Activations scale as `batch × seq × layers × hidden × 2 bytes × ~7`.\n",
    "\n",
    "- **Inference memory ≈ model weights + KV cache.**  \n",
    "  KV cache scales *linearly* with sequence length, batch size, and layers.\n",
    "\n",
    "- **For LLaMA‑2 7B (FP16):**\n",
    "  - Training ≈ 30 – 35 GiB / GPU (before activation sharding)\n",
    "  - Inference (batch 4, 4 k context) ≈ 25 GiB\n",
    "  - Doubling context ≈ doubles inference memory\n",
    "  - Quantizing weights (INT8 / 4‑bit) reduces weight term by ½ – ¼\n",
    "  - GQA reduces KV cache linearly with fewer KV heads\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

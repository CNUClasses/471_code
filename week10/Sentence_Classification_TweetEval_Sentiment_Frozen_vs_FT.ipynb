{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf3ad3e1",
   "metadata": {},
   "source": [
    "\n",
    "# Sentence Classification (Sentiment) — Linear Probe vs Full Fine-Tuning on **TweetEval: Sentiment**\n",
    "\n",
    "**Audience:** 4th-year Computer Science students  \n",
    "**Task:** Multi-class sentiment classification (**negative / neutral / positive**) on the **TweetEval** benchmark (subset: `sentiment`).\n",
    "\n",
    "You'll build and compare two approaches using a Hugging Face encoder:\n",
    "1. **Linear Probe (Frozen Encoder):** Freeze the transformer encoder and train only a small classification head.\n",
    "2. **Full Fine-Tuning:** Unfreeze the encoder and fine-tune end-to-end.\n",
    "\n",
    "We'll evaluate both on the same test set and visualize improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab567d4c",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup & Reproducibility\n",
    "\n",
    "Run this cell to (optionally) install dependencies and set the random seed.  \n",
    "If running on a managed environment (e.g., Colab) uncomment the `pip` line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6b9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed, uncomment to install:\n",
    "# %pip install -U transformers datasets accelerate evaluate scikit-learn matplotlib\n",
    "\n",
    "import os, random, time, json\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36495d08",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Configuration\n",
    "\n",
    "Tweak hyperparameters here. To make a quick run on CPU, use a **subset_fraction** like `0.3`. Set to `None` for the full dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2868eb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONFIG = {\n",
    "    \"dataset_name\": \"tweet_eval\",\n",
    "    \"dataset_subset\": \"sentiment\",   # 3-way: negative(0), neutral(1), positive(2)\n",
    "    \"text_col\": \"text\",\n",
    "    \"label_col\": \"label\",\n",
    "    \"labels\": [\"negative\", \"neutral\", \"positive\"],\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "    \"max_length\": 128,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 32,\n",
    "    \"epochs_probe\": 2,           # linear-probe training epochs\n",
    "    \"epochs_finetune\": 3,        # full finetune epochs\n",
    "    \"learning_rate_probe\": 5e-4, # higher since only head trains\n",
    "    \"learning_rate_finetune\": 2e-5,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_ratio\": 0.06,\n",
    "    \"subset_fraction\": 0.3,      # None for full data; use fraction like 0.3 for speed\n",
    "    \"output_dir\": \"checkpoints_tweeteval_sentiment\"\n",
    "}\n",
    "print(json.dumps(CONFIG, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec6a58a",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Load the **TweetEval: Sentiment** Dataset\n",
    "\n",
    "We use the **TweetEval** benchmark (not GLUE). The `sentiment` subset has labels: 0=negative, 1=neutral, 2=positive.  \n",
    "Splits: `train`, `validation`, `test`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de96eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw = load_dataset(CONFIG[\"dataset_name\"], CONFIG[\"dataset_subset\"])\n",
    "\n",
    "# Optionally downsample for a quick demo run\n",
    "subset_fraction = CONFIG[\"subset_fraction\"]\n",
    "if subset_fraction is not None and 0 < subset_fraction < 1:\n",
    "    def take_fraction(dset, frac):\n",
    "        n = max(30, int(len(dset) * frac))  # keep a minimum\n",
    "        return dset.shuffle(seed=SEED).select(range(n))\n",
    "    raw = DatasetDict({\n",
    "        \"train\": take_fraction(raw[\"train\"], subset_fraction),\n",
    "        \"validation\": take_fraction(raw[\"validation\"], subset_fraction),\n",
    "        \"test\": raw[\"test\"]  # keep full test for better generalization measurement\n",
    "    })\n",
    "\n",
    "raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a08962",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Tokenization\n",
    "\n",
    "We use the tokenizer associated with the chosen encoder. Tweets are short; we cap `max_length` to keep it efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b26454",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"], use_fast=True)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[CONFIG[\"text_col\"]], truncation=True, max_length=CONFIG[\"max_length\"])\n",
    "\n",
    "remove_cols = [c for c in raw[\"train\"].column_names if c not in (CONFIG[\"text_col\"], CONFIG[\"label_col\"])]\n",
    "tokenized = raw.map(tokenize_fn, batched=True, remove_columns=remove_cols)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "num_labels = len(CONFIG[\"labels\"])\n",
    "label_names = CONFIG[\"labels\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab64e48",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Metrics\n",
    "\n",
    "We report **accuracy** and **macro-F1** (averages F1 across classes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"macro_f1\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0652dc3f",
   "metadata": {},
   "source": [
    "\n",
    "### Helper: Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c47f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\", labels=None):\n",
    "    if labels is None:\n",
    "        labels = [str(i) for i in sorted(np.unique(y_true))]\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(labels))))\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm)  # default colormap\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_yticks(range(len(labels)))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b2390",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Model Builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e7a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(freeze_encoder=True):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        CONFIG[\"model_name\"], num_labels=num_labels\n",
    "    )\n",
    "    if freeze_encoder:\n",
    "        if hasattr(model, \"distilbert\"):\n",
    "            for p in model.distilbert.parameters():\n",
    "                p.requires_grad = False\n",
    "        else:\n",
    "            base = getattr(model, \"bert\", None) or getattr(model, \"roberta\", None) or getattr(model, \"deberta\", None)\n",
    "            if base is not None:\n",
    "                for p in base.parameters():\n",
    "                    p.requires_grad = False\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad24509",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Baseline: **Linear Probe** (Frozen Encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93228790",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "probe_model = build_model(freeze_encoder=True)\n",
    "\n",
    "args_probe = TrainingArguments(\n",
    "    output_dir=os.path.join(CONFIG[\"output_dir\"], \"probe\"),\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],\n",
    "    learning_rate=CONFIG[\"learning_rate_probe\"],\n",
    "    num_train_epochs=CONFIG[\"epochs_probe\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer_probe = Trainer(\n",
    "    model=probe_model,\n",
    "    args=args_probe,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "trainer_probe.train()\n",
    "probe_train_time = time.time() - t0\n",
    "\n",
    "probe_val = trainer_probe.evaluate(tokenized[\"validation\"])\n",
    "probe_test = trainer_probe.evaluate(tokenized[\"test\"])\n",
    "\n",
    "print(\"Probe Validation:\", probe_val)\n",
    "print(\"Probe Test:\", probe_test, \"| Train time (s):\", round(probe_train_time, 2))\n",
    "\n",
    "probe_logits, _, _ = trainer_probe.predict(tokenized[\"test\"])\n",
    "probe_preds = np.argmax(probe_logits, axis=-1)\n",
    "y_test = np.array(tokenized[\"test\"][CONFIG[\"label_col\"]])\n",
    "\n",
    "plot_confusion_matrix(y_test, probe_preds, title=\"Frozen Encoder (Linear Probe) — Test\", labels=label_names)\n",
    "print(classification_report(y_test, probe_preds, target_names=label_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd4bdb1",
   "metadata": {},
   "source": [
    "\n",
    "## 7) **Full Fine-Tuning** (Encoder + Head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942130cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ft_model = build_model(freeze_encoder=False)\n",
    "\n",
    "args_ft = TrainingArguments(\n",
    "    output_dir=os.path.join(CONFIG[\"output_dir\"], \"finetune\"),\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],\n",
    "    learning_rate=CONFIG[\"learning_rate_finetune\"],\n",
    "    num_train_epochs=CONFIG[\"epochs_finetune\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer_ft = Trainer(\n",
    "    model=ft_model,\n",
    "    args=args_ft,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "trainer_ft.train()\n",
    "ft_train_time = time.time() - t0\n",
    "\n",
    "ft_val = trainer_ft.evaluate(tokenized[\"validation\"])\n",
    "ft_test = trainer_ft.evaluate(tokenized[\"test\"])\n",
    "\n",
    "print(\"Finetune Validation:\", ft_val)\n",
    "print(\"Finetune Test:\", ft_test, \"| Train time (s):\", round(ft_train_time, 2))\n",
    "\n",
    "ft_logits, _, _ = trainer_ft.predict(tokenized[\"test\"])\n",
    "ft_preds = np.argmax(ft_logits, axis=-1)\n",
    "\n",
    "plot_confusion_matrix(y_test, ft_preds, title=\"Full Fine-Tuned — Test\", labels=label_names)\n",
    "print(classification_report(y_test, ft_preds, target_names=label_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eb9659",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Compare Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247250fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def metric(d, key):\n",
    "    return float(d.get(key, \"nan\"))\n",
    "\n",
    "probe_acc = metric(probe_test, \"eval_accuracy\")\n",
    "probe_f1m = metric(probe_test, \"eval_macro_f1\")\n",
    "ft_acc = metric(ft_test, \"eval_accuracy\")\n",
    "ft_f1m = metric(ft_test, \"eval_macro_f1\")\n",
    "\n",
    "print(f\"Probe — Test Accuracy: {probe_acc:.4f} | Macro F1: {probe_f1m:.4f}\")\n",
    "print(f\"FT    — Test Accuracy: {ft_acc:.4f} | Macro F1: {ft_f1m:.4f}\")\n",
    "print(f\"Δ Accuracy: {ft_acc - probe_acc:+.4f}\")\n",
    "print(f\"Δ Macro F1: {ft_f1m - probe_f1m:+.4f}\")\n",
    "\n",
    "labels_disp = [\"Probe (Frozen)\", \"Finetuned\"]\n",
    "accs = [probe_acc, ft_acc]\n",
    "f1s = [probe_f1m, ft_f1m]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(labels_disp, accs)\n",
    "plt.title(\"Test Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(labels_disp, f1s)\n",
    "plt.title(\"Test Macro-F1\")\n",
    "plt.ylabel(\"Macro-F1\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c2cd70",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Discussion & Extensions\n",
    "\n",
    "- **Try other datasets:** `imdb`, `amazon_polarity`, `yelp_polarity`, or other `tweet_eval` tasks.\n",
    "- **Try other encoders:** `bert-base-uncased`, `roberta-base`, `google/electra-small-discriminator`.\n",
    "- **Compute budget:** Adjust `subset_fraction` for CPU demos vs. full GPU runs.\n",
    "- **PEFT:** Explore LoRA/adapters to approach full-FT accuracy with less compute.\n",
    "- **Error analysis:** Inspect misclassifications; per-class precision/recall; calibration.\n",
    "- **Robustness:** Evaluate on different time slices or domains.\n",
    "\n",
    "> ✍️ **Short write-up prompt:** Explain why full fine-tuning improves performance vs. a frozen encoder. Relate to representation learning and task/domain adaptation.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

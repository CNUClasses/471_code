{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf3ad3e1",
   "metadata": {},
   "source": [
    "\n",
    "# Sentence Classification (Sentiment) â€” Linear Probe vs Full Fine-Tuning on **TweetEval: Sentiment**\n",
    "\n",
    "**Task:** Multi-class sentiment classification (**negative / neutral / positive**) on the **TweetEval** benchmark (subset: `sentiment`).\n",
    "\n",
    "You'll build and compare two approaches using a Hugging Face encoder:\n",
    "1. **Head only fine tuned encoder (Frozen Encoder):** Freeze the transformer encoder and train only a small classification head.\n",
    "2. **Full Fine-Tuning:** Unfreeze the encoder and fine-tune end-to-end.\n",
    "\n",
    "We'll evaluate both on the same test set and visualize improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab567d4c",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup & Reproducibility\n",
    "\n",
    "Run this cell to (optionally) install dependencies and set the random seed.  \n",
    "If running on a managed environment (e.g., Colab) uncomment the `pip` line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff6b9769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.8.0+cu128 | CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If needed, uncomment to install:\n",
    "# %pip install -U transformers datasets accelerate evaluate scikit-learn matplotlib\n",
    "\n",
    "import os, random, time, json\n",
    "import numpy as np\n",
    "\n",
    "# Set the environment variable to use the first GPU (otherwise trainer uses them all)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36495d08",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Configuration\n",
    "\n",
    "Tweak hyperparameters here. To make a quick run on CPU, use a **subset_fraction** like `0.3`. Set to `None` for the full dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2868eb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"dataset_name\": \"tweet_eval\",\n",
      "  \"dataset_subset\": \"sentiment\",\n",
      "  \"text_col\": \"text\",\n",
      "  \"label_col\": \"label\",\n",
      "  \"labels\": [\n",
      "    \"negative\",\n",
      "    \"neutral\",\n",
      "    \"positive\"\n",
      "  ],\n",
      "  \"model_name\": \"distilbert-base-uncased\",\n",
      "  \"max_length\": 128,\n",
      "  \"per_device_train_batch_size\": 512,\n",
      "  \"per_device_eval_batch_size\": 1024,\n",
      "  \"epochs_probe\": 2,\n",
      "  \"epochs_finetune\": 3,\n",
      "  \"learning_rate_probe\": 0.0005,\n",
      "  \"learning_rate_finetune\": 2e-05,\n",
      "  \"weight_decay\": 0.01,\n",
      "  \"warmup_ratio\": 0.06,\n",
      "  \"subset_fraction\": 0.3,\n",
      "  \"output_dir\": \"checkpoints_tweeteval_sentiment\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CONFIG = {\n",
    "    \"dataset_name\": \"tweet_eval\",\n",
    "    \"dataset_subset\": \"sentiment\",   # 3-way: negative(0), neutral(1), positive(2)\n",
    "    \"text_col\": \"text\",\n",
    "    \"label_col\": \"label\",\n",
    "    \"labels\": [\"negative\", \"neutral\", \"positive\"],\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "    \"max_length\": 128,\n",
    "    \"per_device_train_batch_size\": 512,\n",
    "    \"per_device_eval_batch_size\": 1024,\n",
    "    \"epochs_probe\": 2,           # linear-probe training epochs\n",
    "    \"epochs_finetune\": 3,        # full finetune epochs\n",
    "    \"learning_rate_probe\": 5e-4, # higher since only head trains\n",
    "    \"learning_rate_finetune\": 2e-5,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_ratio\": 0.06,\n",
    "    \"subset_fraction\": .3,      # None for full data; use fraction like 0.3 for speed\n",
    "    \"output_dir\": \"checkpoints_tweeteval_sentiment\"\n",
    "}\n",
    "print(json.dumps(CONFIG, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec6a58a",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Load the **TweetEval: Sentiment** Dataset\n",
    "\n",
    "We use the **TweetEval** benchmark (not GLUE). The `sentiment` subset has labels: 0=negative, 1=neutral, 2=positive.  \n",
    "Splits: `train`, `validation`, `test`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de96eb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 13684\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 12284\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "raw = load_dataset(CONFIG[\"dataset_name\"], CONFIG[\"dataset_subset\"])\n",
    "\n",
    "# Optionally downsample for a quick demo run\n",
    "subset_fraction = CONFIG[\"subset_fraction\"]\n",
    "if subset_fraction is not None and 0 < subset_fraction < 1:\n",
    "    def take_fraction(dset, frac):\n",
    "        n = max(30, int(len(dset) * frac))  # keep a minimum\n",
    "        return dset.shuffle(seed=SEED).select(range(n))\n",
    "    raw = DatasetDict({\n",
    "        \"train\": take_fraction(raw[\"train\"], subset_fraction),\n",
    "        \"validation\": take_fraction(raw[\"validation\"], subset_fraction),\n",
    "        \"test\": raw[\"test\"]  # keep full test for better generalization measurement\n",
    "    }) # type: ignore\n",
    "\n",
    "raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "102991ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Few more hours to iPhone 6s launch and im still using the 4th generation ^_^',\n",
       "  \"Last night we were named NZ's 27th fastest growing co. in the Deloitte Fast 50. Our 2nd year making the list and we are totally thrilled!\",\n",
       "  'All the hoes will be out this Saturday at the Chris brown concert.',\n",
       "  'BUENOS AIRES--Argentina late Wednesday approved a law to lower the legal voting age to 16 in a move that could s ...',\n",
       "  '\"Every time I see a runner slide into 1st, I see Kenny Lofton laying on the ground, arm limp like a dead fish.\"'],\n",
       " 'label': [2, 2, 0, 1, 0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw['train'][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a08962",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Tokenization\n",
    "\n",
    "We use the tokenizer associated with the chosen encoder. Tweets are short; we cap `max_length` to keep it efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19b26454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84241d215b644503a1d7fe373d7e687a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13684 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4612e54996c9419b920784c3dedf8a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519e062729534cd184e2312a1dd70072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer for our chosen model (DistilBERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"], use_fast=True)\n",
    "\n",
    "# Define a function to tokenize text batches\n",
    "# This converts raw text into token IDs that the model can process\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[CONFIG[\"text_col\"]], truncation=True, max_length=CONFIG[\"max_length\"])\n",
    "\n",
    "# Remove columns we don't need for training (keep only text and label)\n",
    "# remove_cols = [c for c in raw[\"train\"].column_names if c not in (CONFIG[\"text_col\"], CONFIG[\"label_col\"])]\n",
    "\n",
    "# Apply tokenization to all splits (train, validation, test)\n",
    "# batched=True processes multiple examples at once for efficiency\n",
    "tokenized = raw.map(tokenize_fn, batched=True)# .remove_columns(remove_cols)\n",
    "\n",
    "# Data collator handles padding sequences to the same length within each batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Store number of classes and their names for later use\n",
    "num_labels = len(CONFIG[\"labels\"])  # 3 classes: negative, neutral, positive\n",
    "label_names = CONFIG[\"labels\"]  # Human-readable names for confusion matrix and reports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab64e48",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Metrics\n",
    "\n",
    "We report **accuracy** and **macro-F1** (averages F1 across classes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16ab427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load evaluation metrics from the evaluate library\n",
    "# accuracy: measures the proportion of correct predictions\n",
    "# f1: measures the harmonic mean of precision and recall\n",
    "accuracy = evaluate.load(\"accuracy\")        #type accuracy in a seperate cell to see options\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for model predictions.\n",
    "    \n",
    "    This function is called by the Trainer during evaluation to calculate\n",
    "    how well the model is performing.\n",
    "    \n",
    "    Args:\n",
    "        eval_pred: A tuple containing (logits, labels)\n",
    "            - logits: raw model outputs (shape: [num_samples, num_classes])\n",
    "            - labels: true labels (shape: [num_samples])\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with accuracy and macro-averaged F1 score\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    # Convert logits to predicted class labels\n",
    "    # argmax finds the index of the highest score for each sample\n",
    "    # e.g., [0.1, 0.7, 0.2] -> 1 (neutral)\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    return {\n",
    "        # Accuracy: percentage of correct predictions\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \n",
    "        # Macro F1: average F1 score across all classes (treats each class equally)\n",
    "        # Good for imbalanced datasets where we care about all classes\n",
    "        \"macro_f1\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd9cbd",
   "metadata": {},
   "source": [
    "## We are using Accuracy.  A fine metric if dataset is not unbalanced.\n",
    "\n",
    "<mark>Rule of thumb: a ratio of 1 to 10 or higher between the largest and smallest class means dataset is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b85eedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN split:\n",
      "  Total samples: 13684\n",
      "  negative:  2129 (15.56%)\n",
      "   neutral:  6261 (45.75%)\n",
      "  positive:  5294 (38.69%)\n",
      "\n",
      "VALIDATION split:\n",
      "  Total samples: 600\n",
      "  negative:    90 (15.00%)\n",
      "   neutral:   249 (41.50%)\n",
      "  positive:   261 (43.50%)\n",
      "\n",
      "TEST split:\n",
      "  Total samples: 12284\n",
      "  negative:  3972 (32.33%)\n",
      "   neutral:  5937 (48.33%)\n",
      "  positive:  2375 (19.33%)\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution across all splits\n",
    "for split_name in [\"train\", \"validation\", \"test\"]:\n",
    "    labels = tokenized[split_name][CONFIG[\"label_col\"]]\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    print(f\"\\n{split_name.upper()} split:\")\n",
    "    print(f\"  Total samples: {len(labels)}\")\n",
    "    for label_id, count in zip(unique, counts):\n",
    "        percentage = (count / len(labels)) * 100\n",
    "        print(f\"  {label_names[label_id]:>8}: {count:>5} ({percentage:>5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b2390",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Model Builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca3e7a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(model=None, encoder_requires_grad=False):\n",
    "    \"\"\"\n",
    "    Build or reuse a sequence classification model and (un)freeze its encoder.\n",
    "\n",
    "    Args:\n",
    "        model: An existing Hugging Face sequence classification model. If None,\n",
    "               a model is loaded from CONFIG[\"model_name\"] with `num_labels`.\n",
    "        encoder_requires_grad (bool): If False, freeze the encoder (linear probe).\n",
    "                                      If True, unfreeze the encoder (full finetune).\n",
    "\n",
    "    Returns:\n",
    "        The model with its encoder parameters' requires_grad set accordingly\n",
    "        (only applied when the backbone is DistilBERT and accessible via\n",
    "        `model.distilbert`).\n",
    "\n",
    "    Notes:\n",
    "        - This function assumes a DistilBERT-based classifier where the encoder\n",
    "          module is exposed as `model.distilbert`.\n",
    "        - If the provided model does not have a `distilbert` attribute, no\n",
    "          parameters are modified.\n",
    "    \"\"\"\n",
    "    # Create a fresh model if none is provided\n",
    "    if model is None:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            CONFIG[\"model_name\"],\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "\n",
    "    # For DistilBERT, the encoder lives under model.distilbert; (un)freeze it\n",
    "    if hasattr(model, \"distilbert\"):\n",
    "        for p in model.distilbert.parameters():\n",
    "            p.requires_grad = encoder_requires_grad\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c13334ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_model(model=None, encoder_requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad24509",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Baseline: **Linear Probe** (Frozen Encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93228790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1621890/2676834322.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_probe = Trainer(\n",
      "/tmp/ipykernel_1621890/2676834322.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_probe = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='54' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [54/54 00:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.918468</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.390629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>0.881011</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.408466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe Test Metrics: {'eval_loss': 0.9080833792686462, 'eval_accuracy': 0.5267828069032888, 'eval_macro_f1': 0.4110568904970793, 'eval_runtime': 4.1561, 'eval_samples_per_second': 2955.679, 'eval_steps_per_second': 2.887, 'epoch': 2.0}\n",
      "Confusion matrix:\n",
      "[[  50 3514  408]\n",
      " [   2 4951  984]\n",
      " [   1  904 1470]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.01      0.02      3972\n",
      "     neutral       0.53      0.83      0.65      5937\n",
      "    positive       0.51      0.62      0.56      2375\n",
      "\n",
      "    accuracy                           0.53     12284\n",
      "   macro avg       0.66      0.49      0.41     12284\n",
      "weighted avg       0.66      0.53      0.43     12284\n",
      "\n",
      "Confusion matrix:\n",
      "[[  50 3514  408]\n",
      " [   2 4951  984]\n",
      " [   1  904 1470]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.01      0.02      3972\n",
      "     neutral       0.53      0.83      0.65      5937\n",
      "    positive       0.51      0.62      0.56      2375\n",
      "\n",
      "    accuracy                           0.53     12284\n",
      "   macro avg       0.66      0.49      0.41     12284\n",
      "weighted avg       0.66      0.53      0.43     12284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TrainingArguments for the linear probe (frozen encoder) experiment.\n",
    "args_probe = TrainingArguments(\n",
    "    output_dir=os.path.join(CONFIG[\"output_dir\"], \"probe\"),              # Directory to save model checkpoints\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],   # Batch size for training\n",
    "    per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],     # Batch size for evaluation\n",
    "    learning_rate=CONFIG[\"learning_rate_probe\"],                         # Learning rate (higher for frozen encoder)\n",
    "    num_train_epochs=CONFIG[\"epochs_probe\"],                             # Number of training epochs\n",
    "    weight_decay=CONFIG[\"weight_decay\"],                                 # Weight decay for regularization\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],                                 # Fraction of steps for learning rate warmup\n",
    "    logging_steps=50,                                                    # Log metrics every N steps\n",
    "    eval_strategy=\"epoch\",                                               # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                                               # Save checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True,                                         # Load the best model based on validation metrics\n",
    "    seed=SEED,                                                           # Random seed for reproducibility\n",
    "    report_to=\"none\"                                                     # Disable reporting to external services\n",
    ")\n",
    "\n",
    "#create frozen encoder model\n",
    "model = build_model(encoder_requires_grad=False)\n",
    "\n",
    "trainer_probe = Trainer(\n",
    "    model=model,                                    # The frozen encoder model\n",
    "    args=args_probe,                                # Training arguments for linear probe\n",
    "    train_dataset=tokenized[\"train\"],               # Training data\n",
    "    eval_dataset=tokenized[\"validation\"],           # Validation data\n",
    "    tokenizer=tokenizer,                            # Tokenizer for text processing\n",
    "    data_collator=data_collator,                    # Handles padding in batches\n",
    "    compute_metrics=compute_metrics                 # Function to compute accuracy and F1\n",
    ")\n",
    "\n",
    "# Train the linear probe (frozen encoder)\n",
    "trainer_probe.train()\n",
    "\n",
    "# Test set evaluation\n",
    "probe_test = trainer_probe.evaluate(tokenized[\"test\"])\n",
    "print(\"Probe Test Metrics:\", probe_test)\n",
    "\n",
    "# Save predictions for analysis\n",
    "probe_logits, _, _ = trainer_probe.predict(tokenized[\"test\"])\n",
    "probe_preds = np.argmax(probe_logits, axis=-1)\n",
    "\n",
    "#get the valid labels\n",
    "y_test = np.array(tokenized[\"test\"][CONFIG[\"label_col\"]])\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, probe_preds))\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, probe_preds, target_names=label_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd4bdb1",
   "metadata": {},
   "source": [
    "\n",
    "## 7) **Full Fine-Tuning** (Encoder + Head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "942130cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1621890/1383617013.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_ft = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.744152</td>\n",
       "      <td>0.655000</td>\n",
       "      <td>0.599401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.732800</td>\n",
       "      <td>0.711859</td>\n",
       "      <td>0.671667</td>\n",
       "      <td>0.634934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.732800</td>\n",
       "      <td>0.706729</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.635720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune Test: {'eval_loss': 0.7125346064567566, 'eval_accuracy': 0.6802344513187887, 'eval_macro_f1': 0.6759480259513638, 'eval_runtime': 4.1456, 'eval_samples_per_second': 2963.133, 'eval_steps_per_second': 2.895, 'epoch': 3.0}\n",
      "Confusion matrix:\n",
      "[[2685 1089  198]\n",
      " [1030 3985  922]\n",
      " [  89  600 1686]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.68      0.69      3972\n",
      "     neutral       0.70      0.67      0.69      5937\n",
      "    positive       0.60      0.71      0.65      2375\n",
      "\n",
      "    accuracy                           0.68     12284\n",
      "   macro avg       0.67      0.69      0.68     12284\n",
      "weighted avg       0.68      0.68      0.68     12284\n",
      "\n",
      "Confusion matrix:\n",
      "[[2685 1089  198]\n",
      " [1030 3985  922]\n",
      " [  89  600 1686]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.68      0.69      3972\n",
      "     neutral       0.70      0.67      0.69      5937\n",
      "    positive       0.60      0.71      0.65      2375\n",
      "\n",
      "    accuracy                           0.68     12284\n",
      "   macro avg       0.67      0.69      0.68     12284\n",
      "weighted avg       0.68      0.68      0.68     12284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#make entire model trainable\n",
    "ft_model = build_model(model=model,encoder_requires_grad=True)\n",
    "\n",
    "args_probe.output_dir=os.path.join(CONFIG[\"output_dir\"], \"finetune\")\n",
    "args_probe.learning_rate=CONFIG[\"learning_rate_finetune\"]   #lower learning rate for full finetune\n",
    "args_probe.num_train_epochs=CONFIG[\"epochs_finetune\"]\n",
    "\n",
    "trainer_ft = Trainer(\n",
    "    model=ft_model,\n",
    "    args=args_probe,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the full finetune model\n",
    "trainer_ft.train()\n",
    "\n",
    "# Test set evaluation\n",
    "ft_test = trainer_ft.evaluate(tokenized[\"test\"])\n",
    "print(\"Finetune Test:\", ft_test)\n",
    "\n",
    "# Save predictions for analysis\n",
    "ft_logits, _, _ = trainer_ft.predict(tokenized[\"test\"])\n",
    "ft_preds = np.argmax(ft_logits, axis=-1)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, ft_preds))\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, ft_preds, target_names=label_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3984657e",
   "metadata": {},
   "source": [
    "\n",
    "## 8) **Save and Reload Model, then test it** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5411e155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model directory: checkpoints_tweeteval_sentiment/finetune_saved\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Save and reload the fine-tuned model + tokenizer, then sanity-check predictions\n",
    "\n",
    "\n",
    "save_dir = os.path.join(CONFIG[\"output_dir\"], \"finetune_saved\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save via the trainer (saves model + config) and tokenizer\n",
    "trainer_ft.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "# Reload model and tokenizer\n",
    "model_reloaded = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "tokenizer_reloaded = AutoTokenizer.from_pretrained(save_dir)\n",
    "\n",
    "# Move to device and eval mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_reloaded.to(device)\n",
    "model_reloaded.eval()\n",
    "\n",
    "print(f\"Saved model directory: {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0ba9142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"@user @user That's coming, but I think the victims are going to be Medicaid recipients.\", 'label': 1, 'input_ids': [101, 1030, 5310, 1030, 5310, 2008, 1005, 1055, 2746, 1010, 2021, 1045, 2228, 1996, 5694, 2024, 2183, 2000, 2022, 19960, 5555, 3593, 15991, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: @user @user That's coming, but I think the victims are going to be Medicaid recipients.\n",
      "\n",
      "Predicted class: 1 (neutral)\n",
      "Actual label: 1 (neutral)\n",
      "\n",
      "Logits: [ 0.42532378  0.65774786 -0.90948796]\n",
      "Probabilities: [0.39606118 0.4996924  0.10424636]\n"
     ]
    }
   ],
   "source": [
    "# Select a single example from the test set\n",
    "single_example = tokenized[\"test\"].select([2])\n",
    "print(single_example[0])\n",
    "\n",
    "# Use the fine-tuned trainer to predict\n",
    "predictions = trainer_ft.predict(single_example)\n",
    "logits = predictions.predictions\n",
    "predicted_class = np.argmax(logits, axis=-1)[0]\n",
    "\n",
    "# Get the actual label\n",
    "actual_label = single_example[\"label\"][0]\n",
    "\n",
    "# Display results\n",
    "print(f\"Text: {raw['test'][2]['text']}\")\n",
    "print(f\"\\nPredicted class: {predicted_class} ({label_names[predicted_class]})\")\n",
    "print(f\"Actual label: {actual_label} ({label_names[actual_label]})\")\n",
    "print(f\"\\nLogits: {logits[0]}\")\n",
    "print(f\"Probabilities: {np.exp(logits[0]) / np.sum(np.exp(logits[0]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4824107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

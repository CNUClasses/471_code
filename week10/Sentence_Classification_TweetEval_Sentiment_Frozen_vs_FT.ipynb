{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf3ad3e1",
   "metadata": {},
   "source": [
    "\n",
    "# Sentence Classification (Sentiment) — Linear Probe vs Full Fine-Tuning on **TweetEval: Sentiment**\n",
    "\n",
    "**Task:** Multi-class sentiment classification (**negative / neutral / positive**) on the **TweetEval** benchmark (subset: `sentiment`).\n",
    "\n",
    "You'll build and compare two approaches using a Hugging Face encoder:\n",
    "1. **Head only fine tuned encoder (Frozen Encoder):** Freeze the transformer encoder and train only a small classification head.\n",
    "2. **Full Fine-Tuning:** Unfreeze the encoder and fine-tune end-to-end.\n",
    "\n",
    "We'll evaluate both on the same test set and visualize improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab567d4c",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup & Reproducibility\n",
    "\n",
    "Run this cell to (optionally) install dependencies and set the random seed.  \n",
    "If running on a managed environment (e.g., Colab) uncomment the `pip` line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff6b9769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kperkins411/anaconda3/envs/ultralytics/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.8.0+cu128 | CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If needed, uncomment to install:\n",
    "# %pip install -U transformers datasets accelerate evaluate scikit-learn matplotlib\n",
    "\n",
    "import os, random, time, json\n",
    "import numpy as np\n",
    "\n",
    "# Set the environment variable to use the first GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36495d08",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Configuration\n",
    "\n",
    "Tweak hyperparameters here. To make a quick run on CPU, use a **subset_fraction** like `0.3`. Set to `None` for the full dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2868eb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"dataset_name\": \"tweet_eval\",\n",
      "  \"dataset_subset\": \"sentiment\",\n",
      "  \"text_col\": \"text\",\n",
      "  \"label_col\": \"label\",\n",
      "  \"labels\": [\n",
      "    \"negative\",\n",
      "    \"neutral\",\n",
      "    \"positive\"\n",
      "  ],\n",
      "  \"model_name\": \"distilbert-base-uncased\",\n",
      "  \"max_length\": 128,\n",
      "  \"per_device_train_batch_size\": 16,\n",
      "  \"per_device_eval_batch_size\": 32,\n",
      "  \"epochs_probe\": 2,\n",
      "  \"epochs_finetune\": 3,\n",
      "  \"learning_rate_probe\": 0.0005,\n",
      "  \"learning_rate_finetune\": 2e-05,\n",
      "  \"weight_decay\": 0.01,\n",
      "  \"warmup_ratio\": 0.06,\n",
      "  \"subset_fraction\": 0.3,\n",
      "  \"output_dir\": \"checkpoints_tweeteval_sentiment\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CONFIG = {\n",
    "    \"dataset_name\": \"tweet_eval\",\n",
    "    \"dataset_subset\": \"sentiment\",   # 3-way: negative(0), neutral(1), positive(2)\n",
    "    \"text_col\": \"text\",\n",
    "    \"label_col\": \"label\",\n",
    "    \"labels\": [\"negative\", \"neutral\", \"positive\"],\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "    \"max_length\": 128,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 32,\n",
    "    \"epochs_probe\": 2,           # linear-probe training epochs\n",
    "    \"epochs_finetune\": 3,        # full finetune epochs\n",
    "    \"learning_rate_probe\": 5e-4, # higher since only head trains\n",
    "    \"learning_rate_finetune\": 2e-5,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_ratio\": 0.06,\n",
    "    \"subset_fraction\": 0.3,      # None for full data; use fraction like 0.3 for speed\n",
    "    \"output_dir\": \"checkpoints_tweeteval_sentiment\"\n",
    "}\n",
    "print(json.dumps(CONFIG, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec6a58a",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Load the **TweetEval: Sentiment** Dataset\n",
    "\n",
    "We use the **TweetEval** benchmark (not GLUE). The `sentiment` subset has labels: 0=negative, 1=neutral, 2=positive.  \n",
    "Splits: `train`, `validation`, `test`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de96eb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 13684\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 12284\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "raw = load_dataset(CONFIG[\"dataset_name\"], CONFIG[\"dataset_subset\"])\n",
    "\n",
    "# Optionally downsample for a quick demo run\n",
    "subset_fraction = CONFIG[\"subset_fraction\"]\n",
    "if subset_fraction is not None and 0 < subset_fraction < 1:\n",
    "    def take_fraction(dset, frac):\n",
    "        n = max(30, int(len(dset) * frac))  # keep a minimum\n",
    "        return dset.shuffle(seed=SEED).select(range(n))\n",
    "    raw = DatasetDict({\n",
    "        \"train\": take_fraction(raw[\"train\"], subset_fraction),\n",
    "        \"validation\": take_fraction(raw[\"validation\"], subset_fraction),\n",
    "        \"test\": raw[\"test\"]  # keep full test for better generalization measurement\n",
    "    }) # type: ignore\n",
    "\n",
    "raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a08962",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Tokenization\n",
    "\n",
    "We use the tokenizer associated with the chosen encoder. Tweets are short; we cap `max_length` to keep it efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b26454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 600/600 [00:00<00:00, 12331.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the tokenizer for our chosen model (DistilBERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"], use_fast=True)\n",
    "\n",
    "# Define a function to tokenize text batches\n",
    "# This converts raw text into token IDs that the model can process\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[CONFIG[\"text_col\"]], truncation=True, max_length=CONFIG[\"max_length\"])\n",
    "\n",
    "# Remove columns we don't need for training (keep only text and label)\n",
    "remove_cols = [c for c in raw[\"train\"].column_names if c not in (CONFIG[\"text_col\"], CONFIG[\"label_col\"])]\n",
    "\n",
    "# Apply tokenization to all splits (train, validation, test)\n",
    "# batched=True processes multiple examples at once for efficiency\n",
    "tokenized = raw.map(tokenize_fn, batched=True, remove_columns=remove_cols)\n",
    "\n",
    "# Data collator handles padding sequences to the same length within each batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Store number of classes and their names for later use\n",
    "num_labels = len(CONFIG[\"labels\"])  # 3 classes: negative, neutral, positive\n",
    "label_names = CONFIG[\"labels\"]  # Human-readable names for confusion matrix and reports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab64e48",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Metrics\n",
    "\n",
    "We report **accuracy** and **macro-F1** (averages F1 across classes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16ab427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load evaluation metrics from the evaluate library\n",
    "# accuracy: measures the proportion of correct predictions\n",
    "# f1: measures the harmonic mean of precision and recall\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for model predictions.\n",
    "    \n",
    "    This function is called by the Trainer during evaluation to calculate\n",
    "    how well the model is performing.\n",
    "    \n",
    "    Args:\n",
    "        eval_pred: A tuple containing (logits, labels)\n",
    "            - logits: raw model outputs (shape: [num_samples, num_classes])\n",
    "            - labels: true labels (shape: [num_samples])\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with accuracy and macro-averaged F1 score\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    # Convert logits to predicted class labels\n",
    "    # argmax finds the index of the highest score for each sample\n",
    "    # e.g., [0.1, 0.7, 0.2] -> 1 (neutral)\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    return {\n",
    "        # Accuracy: percentage of correct predictions\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \n",
    "        # Macro F1: average F1 score across all classes (treats each class equally)\n",
    "        # Good for imbalanced datasets where we care about all classes\n",
    "        \"macro_f1\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd9cbd",
   "metadata": {},
   "source": [
    "## We are using Accuracy.  A fine meric if dataset is not unbalanced.\n",
    "\n",
    "<mark>Rule of thumb: a ratio of 1 to 10 or higher between the largest and smallest class means dataset is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b85eedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN split:\n",
      "  Total samples: 13684\n",
      "  negative:  2129 (15.56%)\n",
      "   neutral:  6261 (45.75%)\n",
      "  positive:  5294 (38.69%)\n",
      "\n",
      "VALIDATION split:\n",
      "  Total samples: 600\n",
      "  negative:    90 (15.00%)\n",
      "   neutral:   249 (41.50%)\n",
      "  positive:   261 (43.50%)\n",
      "\n",
      "TEST split:\n",
      "  Total samples: 12284\n",
      "  negative:  3972 (32.33%)\n",
      "   neutral:  5937 (48.33%)\n",
      "  positive:  2375 (19.33%)\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution across all splits\n",
    "for split_name in [\"train\", \"validation\", \"test\"]:\n",
    "    labels = tokenized[split_name][CONFIG[\"label_col\"]]\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    print(f\"\\n{split_name.upper()} split:\")\n",
    "    print(f\"  Total samples: {len(labels)}\")\n",
    "    for label_id, count in zip(unique, counts):\n",
    "        percentage = (count / len(labels)) * 100\n",
    "        print(f\"  {label_names[label_id]:>8}: {count:>5} ({percentage:>5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b2390",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Model Builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca3e7a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(model=None, encoder_requires_grad=False):\n",
    "    \"\"\"\n",
    "    Build or reuse a sequence classification model and (un)freeze its encoder.\n",
    "\n",
    "    Args:\n",
    "        model: An existing Hugging Face sequence classification model. If None,\n",
    "               a model is loaded from CONFIG[\"model_name\"] with `num_labels`.\n",
    "        encoder_requires_grad (bool): If False, freeze the encoder (linear probe).\n",
    "                                      If True, unfreeze the encoder (full finetune).\n",
    "\n",
    "    Returns:\n",
    "        The model with its encoder parameters' requires_grad set accordingly\n",
    "        (only applied when the backbone is DistilBERT and accessible via\n",
    "        `model.distilbert`).\n",
    "\n",
    "    Notes:\n",
    "        - This function assumes a DistilBERT-based classifier where the encoder\n",
    "          module is exposed as `model.distilbert`.\n",
    "        - If the provided model does not have a `distilbert` attribute, no\n",
    "          parameters are modified.\n",
    "    \"\"\"\n",
    "    # Create a fresh model if none is provided\n",
    "    if model is None:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            CONFIG[\"model_name\"],\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "\n",
    "    # For DistilBERT, the encoder lives under model.distilbert; (un)freeze it\n",
    "    if hasattr(model, \"distilbert\"):\n",
    "        for p in model.distilbert.parameters():\n",
    "            p.requires_grad = encoder_requires_grad\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad24509",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Baseline: **Linear Probe** (Frozen Encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93228790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_568296/1444883652.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_probe = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1712' max='1712' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1712/1712 00:16, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.789700</td>\n",
       "      <td>0.788938</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.556562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.745400</td>\n",
       "      <td>0.782529</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.587171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe Test Metrics: {'eval_loss': 0.7459889650344849, 'eval_accuracy': 0.6471019211983068, 'eval_macro_f1': 0.640696252816535, 'eval_runtime': 4.4274, 'eval_samples_per_second': 2774.569, 'eval_steps_per_second': 86.734, 'epoch': 2.0}\n",
      "Confusion matrix:\n",
      "[[2633 1143  196]\n",
      " [1219 3809  909]\n",
      " [ 113  755 1507]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.66      0.66      0.66      3972\n",
      "     neutral       0.67      0.64      0.65      5937\n",
      "    positive       0.58      0.63      0.60      2375\n",
      "\n",
      "    accuracy                           0.65     12284\n",
      "   macro avg       0.64      0.65      0.64     12284\n",
      "weighted avg       0.65      0.65      0.65     12284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TrainingArguments for the linear probe (frozen encoder) experiment.\n",
    "args_probe = TrainingArguments(\n",
    "    output_dir=os.path.join(CONFIG[\"output_dir\"], \"probe\"),              # Directory to save model checkpoints\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],   # Batch size for training\n",
    "    per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],     # Batch size for evaluation\n",
    "    learning_rate=CONFIG[\"learning_rate_probe\"],                         # Learning rate (higher for frozen encoder)\n",
    "    num_train_epochs=CONFIG[\"epochs_probe\"],                             # Number of training epochs\n",
    "    weight_decay=CONFIG[\"weight_decay\"],                                 # Weight decay for regularization\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],                                 # Fraction of steps for learning rate warmup\n",
    "    logging_steps=50,                                                    # Log metrics every N steps\n",
    "    eval_strategy=\"epoch\",                                               # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                                               # Save checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True,                                         # Load the best model based on validation metrics\n",
    "    seed=SEED,                                                           # Random seed for reproducibility\n",
    "    report_to=\"none\"                                                     # Disable reporting to external services\n",
    ")\n",
    "\n",
    "#create frozen encoder model\n",
    "model = build_model(encoder_requires_grad=False)\n",
    "\n",
    "trainer_probe = Trainer(\n",
    "    model=model,                                    # The frozen encoder model\n",
    "    args=args_probe,                                # Training arguments for linear probe\n",
    "    train_dataset=tokenized[\"train\"],               # Training data\n",
    "    eval_dataset=tokenized[\"validation\"],           # Validation data\n",
    "    tokenizer=tokenizer,                            # Tokenizer for text processing\n",
    "    data_collator=data_collator,                    # Handles padding in batches\n",
    "    compute_metrics=compute_metrics                 # Function to compute accuracy and F1\n",
    ")\n",
    "\n",
    "# Train the linear probe (frozen encoder)\n",
    "trainer_probe.train()\n",
    "\n",
    "# Test set evaluation\n",
    "probe_test = trainer_probe.evaluate(tokenized[\"test\"])\n",
    "print(\"Probe Test Metrics:\", probe_test)\n",
    "\n",
    "# Save predictions for analysis\n",
    "probe_logits, _, _ = trainer_probe.predict(tokenized[\"test\"])\n",
    "probe_preds = np.argmax(probe_logits, axis=-1)\n",
    "y_test = np.array(tokenized[\"test\"][CONFIG[\"label_col\"]])\n",
    "\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, probe_preds))\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, probe_preds, target_names=label_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd4bdb1",
   "metadata": {},
   "source": [
    "\n",
    "## 7) **Full Fine-Tuning** (Encoder + Head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "942130cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_568296/302164541.py:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_ft = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2568' max='2568' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2568/2568 00:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.411900</td>\n",
       "      <td>0.866622</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.664017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.329100</td>\n",
       "      <td>0.975004</td>\n",
       "      <td>0.686667</td>\n",
       "      <td>0.661984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.269500</td>\n",
       "      <td>1.304437</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.676646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune Test: {'eval_loss': 0.8187649250030518, 'eval_accuracy': 0.6767339628785412, 'eval_macro_f1': 0.6737022877236408, 'eval_runtime': 4.4235, 'eval_samples_per_second': 2776.979, 'eval_steps_per_second': 86.809, 'epoch': 3.0}\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'full_ft_model_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m ft_preds = np.argmax(ft_logits, axis=-\u001b[32m1\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mConfusion matrix:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28mprint\u001b[39m(confusion_matrix(y_test, \u001b[43mfull_ft_model_preds\u001b[49m))\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mClassification report:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, full_ft_model_preds, target_names=label_names))\n",
      "\u001b[31mNameError\u001b[39m: name 'full_ft_model_preds' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "ft_model = build_model(model=model,encoder_requires_grad=True)\n",
    "\n",
    "args_probe.output_dir=os.path.join(CONFIG[\"output_dir\"], \"finetune\")\n",
    "args_probe.learning_rate=CONFIG[\"learning_rate_finetune\"]\n",
    "args_probe.num_train_epochs=CONFIG[\"epochs_finetune\"]\n",
    "\n",
    "trainer_ft = Trainer(\n",
    "    model=ft_model,\n",
    "    args=args_probe,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "trainer_ft.train()\n",
    "\n",
    "# Test set evaluation\n",
    "ft_test = trainer_ft.evaluate(tokenized[\"test\"])\n",
    "print(\"Finetune Test:\", ft_test)\n",
    "\n",
    "# Save predictions for analysis\n",
    "ft_logits, _, _ = trainer_ft.predict(tokenized[\"test\"])\n",
    "ft_preds = np.argmax(ft_logits, axis=-1)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, full_ft_model_preds))\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, full_ft_model_preds, target_names=label_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba9142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

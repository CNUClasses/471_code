{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5ad09e",
   "metadata": {},
   "source": [
    "\n",
    "# Hugging Face `pipeline`: Object Detection, Depth Estimation, and Descriptions (Zero-Shot where possible)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126aa065",
   "metadata": {},
   "source": [
    "\n",
    "**Goal:** Use pretrained Hugging Face pipelines to (1) detect objects (zero-shot), (2) estimate relative distance, and (3) describe each object's distance and left/front/right position.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5198e1d9",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %pip install -U transformers torch torchvision accelerate pillow matplotlib numpy\n",
    "\n",
    "import os, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "\n",
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available(), \"| pipeline(device) =\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b5096c",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Load an Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d0ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGE_PATH = \"example.jpg\"  # Replace with your file path\n",
    "assert os.path.exists(IMAGE_PATH), f\"Image not found: {IMAGE_PATH}\"\n",
    "image = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
    "display(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebc617a",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Zero-Shot Object Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6de7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CANDIDATE_LABELS = [\"person\", \"car\", \"bicycle\", \"dog\", \"cat\", \"traffic light\", \"chair\", \"bottle\"]\n",
    "detector = pipeline(\"zero-shot-object-detection\", model=\"google/owlvit-base-patch32\", device=device)\n",
    "detections = detector(image, candidate_labels=CANDIDATE_LABELS)\n",
    "print(\"Num detections:\", len(detections))\n",
    "detections[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74caeb71",
   "metadata": {},
   "source": [
    "\n",
    "### Visualize detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_bboxes(img, dets, score_thresh=0.25):\n",
    "    img = img.copy()\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    W, H = img.size\n",
    "    for d in dets:\n",
    "        if d.get(\"score\", 0) < score_thresh: \n",
    "            continue\n",
    "        box = d[\"box\"]\n",
    "        label = d[\"label\"]\n",
    "        score = d[\"score\"]\n",
    "        xmin = max(0, int(box[\"xmin\"])); ymin = max(0, int(box[\"ymin\"]))\n",
    "        xmax = min(W-1, int(box[\"xmax\"])); ymax = min(H-1, int(box[\"ymax\"]))\n",
    "        draw.rectangle([(xmin, ymin), (xmax, ymax)], outline=\"red\", width=3)\n",
    "        text = f\"{label} ({score:.2f})\"\n",
    "        tw = draw.textlength(text); th = 12\n",
    "        draw.rectangle([(xmin, ymin - th - 4), (xmin + tw + 6, ymin)], fill=\"red\")\n",
    "        draw.text((xmin + 3, ymin - th - 2), text, fill=\"white\")\n",
    "    return img\n",
    "\n",
    "viz = draw_bboxes(image, detections, score_thresh=0.25)\n",
    "display(viz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfad7df9",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Depth Estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e901f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "depth_pipe = pipeline(\"depth-estimation\", model=\"Intel/dpt-large\", device=device)\n",
    "depth_out = depth_pipe(image)\n",
    "depth_map_img = depth_out[\"depth\"]\n",
    "pred_depth = depth_out[\"predicted_depth\"]\n",
    "display(depth_map_img); print(pred_depth.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb1ee7b",
   "metadata": {},
   "source": [
    "\n",
    "### Per-object distances (relative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c6cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_box_depths(dets, depth_tensor, score_thresh=0.25, near_q=0.33, far_q=0.66):\n",
    "    d = depth_tensor.squeeze(0).cpu().numpy()\n",
    "    H, W = d.shape\n",
    "    results, meds = [], []\n",
    "    for item in dets:\n",
    "        if item.get(\"score\", 0) < score_thresh: continue\n",
    "        box = item[\"box\"]\n",
    "        xmin = max(0, int(box[\"xmin\"])); ymin = max(0, int(box[\"ymin\"]))\n",
    "        xmax = min(W-1, int(box[\"xmax\"])); ymax = min(H-1, int(box[\"ymax\"]))\n",
    "        if xmax <= xmin or ymax <= ymin: continue\n",
    "        patch = d[ymin:ymax, xmin:xmax]\n",
    "        if patch.size == 0: continue\n",
    "        med = float(np.median(patch)); meds.append(med)\n",
    "    if not meds: return []\n",
    "    qn, qf = float(np.quantile(meds, near_q)), float(np.quantile(meds, far_q))\n",
    "    for item in dets:\n",
    "        if item.get(\"score\", 0) < score_thresh: continue\n",
    "        label, score, box = item[\"label\"], item[\"score\"], item[\"box\"]\n",
    "        xmin = max(0, int(box[\"xmin\"])); ymin = max(0, int(box[\"ymin\"]))\n",
    "        xmax = min(W-1, int(box[\"xmax\"])); ymax = min(H-1, int(box[\"ymax\"]))\n",
    "        if xmax <= xmin or ymax <= ymin: continue\n",
    "        patch = d[ymin:ymax, xmin:xmax]\n",
    "        if patch.size == 0: continue\n",
    "        med = float(np.median(patch))\n",
    "        if med <= qn: dist_bin = \"near\"\n",
    "        elif med >= qf: dist_bin = \"far\"\n",
    "        else: dist_bin = \"medium\"\n",
    "        results.append({\"label\": label, \"score\": score, \"box\": {\"xmin\": xmin, \"ymin\": ymin, \"xmax\": xmax, \"ymax\": ymax},\n",
    "                        \"median_depth\": med, \"distance_bin\": dist_bin})\n",
    "    return results\n",
    "\n",
    "box_depths = compute_box_depths(detections, pred_depth, score_thresh=0.25)\n",
    "box_depths[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa40fc88",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Left / Front / Right classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77081037",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify_lr_front(box, img_width, center_slice_ratio=0.2):\n",
    "    cx = 0.5 * (box[\"xmin\"] + box[\"xmax\"])\n",
    "    center = img_width * 0.5\n",
    "    half_slice = (img_width * center_slice_ratio) * 0.5\n",
    "    left_edge, right_edge = center - half_slice, center + half_slice\n",
    "    if cx < left_edge: return \"left\"\n",
    "    elif cx > right_edge: return \"right\"\n",
    "    return \"front\"\n",
    "\n",
    "W, H = image.size\n",
    "for obj in box_depths:\n",
    "    obj[\"position_lr\"] = classify_lr_front(obj[\"box\"], W, center_slice_ratio=0.2)\n",
    "\n",
    "box_depths[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e6575c",
   "metadata": {},
   "source": [
    "\n",
    "### Visualization with distance & position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c19946",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import ImageDraw\n",
    "\n",
    "def draw_bboxes_with_extras(img, objs):\n",
    "    img = img.copy()\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    W, H = img.size\n",
    "    center = W * 0.5\n",
    "    left_edge = int(center - 0.1 * W)\n",
    "    right_edge = int(center + 0.1 * W)\n",
    "    draw.line([(left_edge, 0), (left_edge, H)], fill=\"blue\", width=2)\n",
    "    draw.line([(right_edge, 0), (right_edge, H)], fill=\"blue\", width=2)\n",
    "    for o in objs:\n",
    "        box = o[\"box\"]\n",
    "        xmin, ymin, xmax, ymax = box[\"xmin\"], box[\"ymin\"], box[\"xmax\"], box[\"ymax\"]\n",
    "        draw.rectangle([(xmin, ymin), (xmax, ymax)], outline=\"red\", width=3)\n",
    "        text = f\"{o['label']} | {o['distance_bin']} | {o['position_lr']}\"\n",
    "        tw = draw.textlength(text); th = 12\n",
    "        draw.rectangle([(xmin, ymin - th - 4), (xmin + tw + 6, ymin)], fill=\"red\")\n",
    "        draw.text((xmin + 3, ymin - th - 2), text, fill=\"white\")\n",
    "    return img\n",
    "\n",
    "viz2 = draw_bboxes_with_extras(image, box_depths)\n",
    "display(viz2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78e5e75",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Textual Description (rule-based)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e4f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def describe_objects(objects):\n",
    "    if not objects: return \"No confident detections.\"\n",
    "    objs = sorted(objects, key=lambda o: o[\"median_depth\"])\n",
    "    parts = [f\"a {o['label']} that is {o['distance_bin']} and to the {o['position_lr']}\" for o in objs]\n",
    "    if len(parts) == 1: return \"I see \" + parts[0] + \".\"\n",
    "    return \"I see \" + \", \".join(parts[:-1]) + \", and \" + parts[-1] + \".\"\n",
    "\n",
    "summary = describe_objects(box_depths)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e806dff4",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Notes\n",
    "- Zero-shot detection uses `google/owlvit-base-patch32`; expand `CANDIDATE_LABELS` for broader coverage.\n",
    "- Depth is **relative**, not metric meters.\n",
    "- \"Front\" is a vertical slice around the image center; adjust `center_slice_ratio` as needed.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8746ded",
   "metadata": {},
   "source": [
    "\n",
    "# RAG Lab: PDF â†’ Chunking â†’ Embeddings â†’ ChromaDB â†’ Retrieval\n",
    "\n",
    "**Updated:** 2025-11-08\n",
    "\n",
    "Welcome! In this lab you'll build a small Retrieval-Augmented Generation (RAG) data pipeline:\n",
    "1. Load PDFs from a directory\n",
    "2. Chunk the text using either **fixed-size** chunks with **overlap** or **LLM-assisted** segmentation\n",
    "3. Embed the chunks with a compact Sentence-Transformers model\n",
    "4. Store vectors and metadata in a local **ChromaDB** collection\n",
    "5. Run a simple **retrieval** (top-k nearest neighbors) and optionally synthesize a short answer\n",
    "\n",
    "### What you'll learn\n",
    "- Why chunking and overlap improve recall\n",
    "- How to choose and use an embedding model consistently\n",
    "- How a local vector DB (ChromaDB) stores and retrieves embeddings\n",
    "- How to ground an answer with top-k passages\n",
    "\n",
    "### Glossary (quick)\n",
    "- **Chunk**: A small slice of text extracted from documents.\n",
    "- **Overlap**: Repeating some tokens between adjacent chunks to avoid cutting important context.\n",
    "- **Embedding**: A fixed-length vector representation of text.\n",
    "- **Vector DB**: A database that stores vectors and supports similarity search (e.g., top-k retrieval).\n",
    "- **Top-k retrieval**: Return the k most similar chunks to a query (nearest neighbors).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ddd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# âœ… Install minimal dependencies (CPU-friendly). If these are already installed, this cell is a no-op.\n",
    "# We pin versions for classroom reproducibility. Feel free to loosen pins later.\n",
    "%pip -q install \"pymupdf<1.25\" \"chromadb==0.4.24\" \"sentence-transformers==2.5.1\" \"tqdm>=4.66.0\"\n",
    "\n",
    "import sys, platform, importlib\n",
    "print(\"Python:\", sys.version.split()[0], \"| Platform:\", platform.platform())\n",
    "\n",
    "def _ver(pkg):\n",
    "    try:\n",
    "        return importlib.import_module(pkg).__version__\n",
    "    except Exception as e:\n",
    "        return f\"not found ({e})\"\n",
    "\n",
    "print(\"PyMuPDF:\", _ver(\"fitz\"))\n",
    "print(\"chromadb:\", _ver(\"chromadb\"))\n",
    "print(\"sentence_transformers:\", _ver(\"sentence_transformers\"))\n",
    "print(\"tqdm:\", _ver(\"tqdm\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c92b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Imports & configuration ----\n",
    "import os, re, uuid, glob, math, json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- Student-editable configuration ---\n",
    "PDF_DIR = \"./pdfs\"                  # directory with source PDFs\n",
    "PERSIST_DIR = \"./rag_chroma\"        # ChromaDB persistence path (folder will be created)\n",
    "COLLECTION_NAME = \"cnu_rag_lab\"     # collection name\n",
    "CHUNK_WORDS = 220                   # fixed-size chunk length (~words)\n",
    "CHUNK_OVERLAP_WORDS = 40            # overlap between chunks (~words)\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "USE_LLM_CHUNKING = False            # set True to try optional LLM-assisted chunking\n",
    "MAX_PAGES_PER_PDF: Optional[int] = None  # set an int (e.g., 5) to limit pages for demos\n",
    "\n",
    "# Sanity: ensure dirs exist\n",
    "Path(PDF_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(PERSIST_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Helper for neat printing\n",
    "def head(df, n=5):\n",
    "    try:\n",
    "        from caas_jupyter_tools import display_dataframe_to_user\n",
    "        display_dataframe_to_user(\"Preview\", df.head(n))\n",
    "    except Exception:\n",
    "        display(df.head(n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5898da4",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Load PDFs\n",
    "\n",
    "We'll extract text with **PyMuPDF** page-by-page. Empty or whitespace-only pages are dropped.  \n",
    "**Why this matters:** We want a clean, normalized text corpus before chunking to ensure consistent chunk lengths and quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce36dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_ws(text: str) -> str:\n",
    "    '''Normalize whitespace and strip control chars.'''\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\\n\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\n\\s+\", \"\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_pdfs(pdf_dir: str, max_pages: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    '''\n",
    "    Extract text from all PDFs in a directory.\n",
    "    Returns a list of page records: {\"doc_id\",\"source\",\"page\",\"text\"}.\n",
    "    '''\n",
    "    records = []\n",
    "    paths = sorted(glob.glob(os.path.join(pdf_dir, \"*.pdf\")))\n",
    "    if not paths:\n",
    "        print(f\"[WARN] No PDFs found in {pdf_dir}. Place some PDFs there and re-run.\")\n",
    "        return records\n",
    "\n",
    "    for path in paths:\n",
    "        try:\n",
    "            doc = fitz.open(path)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not open {path}: {e}\")\n",
    "            continue\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        page_count = len(doc)\n",
    "        limit = min(page_count, max_pages) if isinstance(max_pages, int) else page_count\n",
    "        for i in range(limit):\n",
    "            try:\n",
    "                text = doc[i].get_text()\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not read page {i} of {path}: {e}\")\n",
    "                continue\n",
    "            text = normalize_ws(text)\n",
    "            if text:\n",
    "                records.append({\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"source\": os.path.basename(path),\n",
    "                    \"page\": i + 1,  # 1-based for humans\n",
    "                    \"text\": text\n",
    "                })\n",
    "        doc.close()\n",
    "    return records\n",
    "\n",
    "pages = load_pdfs(PDF_DIR, MAX_PAGES_PER_PDF)\n",
    "print(f\"Loaded {len(pages)} non-empty pages from {PDF_DIR}.\")\n",
    "if pages:\n",
    "    import pandas as _pd\n",
    "    _df_pages = _pd.DataFrame(pages)\n",
    "    head(_df_pages, 5)\n",
    "else:\n",
    "    print(\"Add PDFs to the directory and re-run this cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54500eb2",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Chunking (Fixed-Size with Overlap)\n",
    "\n",
    "We'll implement a tokenizer-free word splitter and then slide a window with overlap.  \n",
    "**Why overlap?** It preserves context that may straddle chunk boundaries, improving recall during retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfc363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WORD_RE = re.compile(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", flags=re.UNICODE)\n",
    "\n",
    "def words(text: str):\n",
    "    'A lightweight word+punctuation splitter.'\n",
    "    return WORD_RE.findall(text)\n",
    "\n",
    "def chunk_fixed(text: str, size_words: int = CHUNK_WORDS, overlap_words: int = CHUNK_OVERLAP_WORDS):\n",
    "    '''\n",
    "    Split text into overlapping chunks measured in approx words.\n",
    "    Returns a list of chunk strings.\n",
    "    '''\n",
    "    w = words(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(w):\n",
    "        end = min(start + size_words, len(w))\n",
    "        chunk_text = \" \".join(w[start:end])\n",
    "        if chunk_text.strip():\n",
    "            chunks.append(chunk_text)\n",
    "        if end == len(w):\n",
    "            break\n",
    "        start = max(end - overlap_words, 0)\n",
    "    return chunks\n",
    "\n",
    "def chunk_pages_fixed(page_records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    '''\n",
    "    Map pages -> fixed-size chunks with metadata for each chunk.\n",
    "    '''\n",
    "    out = []\n",
    "    for rec in page_records:\n",
    "        for ch in chunk_fixed(rec[\"text\"], CHUNK_WORDS, CHUNK_OVERLAP_WORDS):\n",
    "            out.append({\n",
    "                \"chunk_id\": str(uuid.uuid4()),\n",
    "                \"doc_id\": rec[\"doc_id\"],\n",
    "                \"source\": rec[\"source\"],\n",
    "                \"page\": rec[\"page\"],\n",
    "                \"method\": \"fixed\",\n",
    "                \"text\": ch\n",
    "            })\n",
    "    return out\n",
    "\n",
    "chunk_records_fixed = chunk_pages_fixed(pages) if pages else []\n",
    "print(f\"Fixed-size chunking produced {len(chunk_records_fixed)} chunks.\")\n",
    "if chunk_records_fixed:\n",
    "    import pandas as _pd\n",
    "    _df_chunks_fixed = _pd.DataFrame(chunk_records_fixed)\n",
    "    head(_df_chunks_fixed[ ['chunk_id','source','page','method','text'] ], 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faea1664",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Optional: LLM-Assisted Chunking (Semantic Segmentation)\n",
    "\n",
    "**Idea:** Ask an LLM to segment a page into coherent sections (headings, paragraphs, lists). Then, for any very long segments, re-chunk using the fixed method above so that chunks stay small.\n",
    "\n",
    "> This path is **optional** and requires an API key. If you set `USE_LLM_CHUNKING = True`, make sure `OPENAI_API_KEY` is in your environment. Keep your key private.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d261a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def llm_segment_page(text: str, model: str = \"gpt-4o-mini\") -> List[str]:\n",
    "    '''\n",
    "    Use an LLM to segment the page text into coherent sections.\n",
    "    Returns a list of segments. On any error or short text, returns [text].\n",
    "    '''\n",
    "    text = text.strip()\n",
    "    if len(text.split()) < 80:\n",
    "        return [text]\n",
    "    try:\n",
    "        import os\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            # No key -> no network call; just fall back gracefully\n",
    "            return [text]\n",
    "\n",
    "        # Install only if needed\n",
    "        try:\n",
    "            from openai import OpenAI  # modern SDK\n",
    "        except Exception:\n",
    "            import sys\n",
    "            print(\"[INFO] Installing openai client...\")\n",
    "            !{sys.executable} -m pip -q install --upgrade openai\n",
    "            from openai import OpenAI\n",
    "\n",
    "        client = OpenAI(api_key=api_key)\n",
    "\n",
    "        prompt = (\n",
    "            \"You segment text into coherent sections such as headings and paragraphs. \"\n",
    "            \"Return a JSON list of strings where each string is a segment. \"\n",
    "            \"Keep segments between 150-400 words when possible.\\n\\n\"\n",
    "            f\"TEXT:\\n{text[:8000]}\"\n",
    "        )\n",
    "        resp = client.responses.create(\n",
    "            model=model,\n",
    "            input=[{\"role\":\"user\",\"content\":prompt}],\n",
    "            response_format={\"type\":\"json_object\"}\n",
    "        )\n",
    "        # Extract JSON safely\n",
    "        content = resp.output[0].content[0].text  # SDK shapes can vary; adjust if needed\n",
    "        import json as _json\n",
    "        parsed = _json.loads(content)\n",
    "        segments = parsed.get(\"segments\") or parsed.get(\"data\") or parsed.get(\"list\")\n",
    "        if isinstance(segments, list) and all(isinstance(s, str) for s in segments):\n",
    "            # light cleanup\n",
    "            segments = [s.strip() for s in segments if s.strip()]\n",
    "            return segments or [text]\n",
    "        return [text]\n",
    "    except Exception as e:\n",
    "        # Silent, robust fallback\n",
    "        return [text]\n",
    "\n",
    "def rechunk_if_long(segments: List[str], max_words: int = 600) -> List[str]:\n",
    "    '''If a segment is too long, re-chunk with the fixed method to keep sizes bounded.'''\n",
    "    out = []\n",
    "    for s in segments:\n",
    "        if len(words(s)) > max_words:\n",
    "            out.extend(chunk_fixed(s, CHUNK_WORDS, CHUNK_OVERLAP_WORDS))\n",
    "        else:\n",
    "            out.append(s)\n",
    "    return out\n",
    "\n",
    "def chunk_pages_llm(page_records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    '''\n",
    "    Segment each page via LLM, then bound segment length. Tag method='llm'.\n",
    "    Requires OPENAI_API_KEY for actual LLM calls; otherwise falls back implicitly.\n",
    "    '''\n",
    "    out = []\n",
    "    for rec in tqdm(page_records, desc=\"LLM segmenting pages\"):\n",
    "        segs = llm_segment_page(rec[\"text\"])\n",
    "        segs = rechunk_if_long(segs, max_words=600)\n",
    "        for s in segs:\n",
    "            out.append({\n",
    "                \"chunk_id\": str(uuid.uuid4()),\n",
    "                \"doc_id\": rec[\"doc_id\"],\n",
    "                \"source\": rec[\"source\"],\n",
    "                \"page\": rec[\"page\"],\n",
    "                \"method\": \"llm\",\n",
    "                \"text\": s\n",
    "            })\n",
    "    return out\n",
    "\n",
    "chunk_records_llm = chunk_pages_llm(pages) if (pages and USE_LLM_CHUNKING) else []\n",
    "print(f\"LLM-assisted path produced {len(chunk_records_llm)} chunks (0 if not enabled).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c7fdc",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Embeddings\n",
    "\n",
    "We'll use a compact, high-quality model: `sentence-transformers/all-MiniLM-L6-v2` (384-dim).  \n",
    "**Why this matters:** Consistent embeddings for documents **and** queries are necessary for good retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c049f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose which chunks to use based on the flag\n",
    "chunks = chunk_records_llm if (USE_LLM_CHUNKING and chunk_records_llm) else chunk_records_fixed\n",
    "\n",
    "if not chunks:\n",
    "    raise RuntimeError(\"No chunks available. Add PDFs to PDF_DIR and re-run the earlier cells.\")\n",
    "\n",
    "# Build a DataFrame for convenience\n",
    "df = pd.DataFrame(chunks)\n",
    "print(\"Chunks:\", len(df))\n",
    "head(df[['chunk_id','source','page','method','text']], 5)\n",
    "\n",
    "# Load the embedder and encode in batches\n",
    "embedder = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "print(\"Embedding model loaded:\", EMBED_MODEL_NAME)\n",
    "\n",
    "batch_size = 64\n",
    "embeddings = []\n",
    "for i in tqdm(range(0, len(df), batch_size), desc=\"Embedding chunks\"):\n",
    "    batch_texts = df['text'].iloc[i:i+batch_size].tolist()\n",
    "    batch_vecs = embedder.encode(batch_texts, batch_size=min(32, len(batch_texts)), show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    embeddings.append(batch_vecs)\n",
    "\n",
    "embeddings = np.vstack(embeddings).astype(\"float32\")\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "df[\"embedding\"] = list(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f39f3",
   "metadata": {},
   "source": [
    "\n",
    "## 5) ChromaDB: Create / Persist Collection\n",
    "\n",
    "We'll use a **PersistentClient** so your vectors survive across sessions in `PERSIST_DIR`.  \n",
    "**Why this matters:** Persistence lets you build once and query many times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58672a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "\n",
    "# Create or get the collection. Using upsert allows repeatable runs.\n",
    "try:\n",
    "    collection = client.get_collection(COLLECTION_NAME)\n",
    "except Exception:\n",
    "    collection = client.create_collection(COLLECTION_NAME)\n",
    "\n",
    "# Upsert (safe to re-run)\n",
    "collection.upsert(\n",
    "    ids=df[\"chunk_id\"].tolist(),\n",
    "    embeddings=df[\"embedding\"].tolist(),\n",
    "    metadatas=df[[\"doc_id\", \"source\", \"page\", \"method\"]].to_dict(orient=\"records\"),\n",
    "    documents=df[\"text\"].tolist()\n",
    ")\n",
    "print(f\"Collection '{COLLECTION_NAME}' now has {collection.count()} vectors. Persisted at: {PERSIST_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c3f276",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Retrieval: Topâ€‘k Nearest Chunks\n",
    "\n",
    "We'll embed the user query with the **same model** and query ChromaDB for the nearest chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493b3ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_query(q: str) -> np.ndarray:\n",
    "    '''Embed a query string using the same SentenceTransformer model (normalized).'''\n",
    "    v = embedder.encode([q], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return v[0].astype(\"float32\")\n",
    "\n",
    "def query_topk(question: str, k: int = 5) -> pd.DataFrame:\n",
    "    '''Return a pretty DataFrame with rank, distance (smaller is closer), and metadata.'''\n",
    "    q_emb = embed_query(question)\n",
    "    res = collection.query(\n",
    "        query_embeddings=[q_emb],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\", \"ids\"]\n",
    "    )\n",
    "    # Flatten results\n",
    "    rows = []\n",
    "    for rank, (cid, dist, meta, doc) in enumerate(zip(res[\"ids\"][0], res[\"distances\"][0], res[\"metadatas\"][0], res[\"documents\"][0]), start=1):\n",
    "        snippet = (doc[:240] + \"â€¦\") if len(doc) > 240 else doc\n",
    "        rows.append({\n",
    "            \"rank\": rank,\n",
    "            \"distance\": round(float(dist), 4),\n",
    "            \"source\": meta.get(\"source\"),\n",
    "            \"page\": meta.get(\"page\"),\n",
    "            \"method\": meta.get(\"method\"),\n",
    "            \"chunk_id\": cid,\n",
    "            \"snippet\": snippet\n",
    "        })\n",
    "    out = pd.DataFrame(rows)\n",
    "    return out\n",
    "\n",
    "# Try a query (edit this to your corpus):\n",
    "example_queries = [\n",
    "    \"Summarize the main purpose of this document.\",\n",
    "    \"What are the key definitions introduced?\",\n",
    "    \"List the steps or procedures mentioned in the text.\"\n",
    "]\n",
    "print(\"Try:\", example_queries[0])\n",
    "res_df = query_topk(example_queries[0], k=5)\n",
    "head(res_df, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26969159",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Miniâ€‘RAG: Concise Answer Synthesis (Optional)\n",
    "\n",
    "If you have an API key in your environment (`OPENAI_API_KEY`), we can ask a model for a **short, grounded** answer. Otherwise, we'll just show the retrieved context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214e9744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_context(df_hits: pd.DataFrame) -> str:\n",
    "    ctx_lines = []\n",
    "    for _, row in df_hits.iterrows():\n",
    "        ctx_lines.append(f\"[{row['source']} p.{row['page']} | {row['method']}] {row['snippet']}\")\n",
    "    return \"\\n\".join(ctx_lines)\n",
    "\n",
    "def synthesize_answer(question: str, topk: int = 5, model: str = \"gpt-4o-mini\") -> None:\n",
    "    hits = query_topk(question, k=topk)\n",
    "    ctx = build_context(hits)\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"No OPENAI_API_KEY found. Showing retrieved context instead:\\n\")\n",
    "        print(ctx)\n",
    "        return\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "    except Exception:\n",
    "        import sys\n",
    "        print(\"[INFO] Installing openai client...\")\n",
    "        !{sys.executable} -m pip -q install --upgrade openai\n",
    "        from openai import OpenAI\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    sys_prompt = \"You are a concise teaching assistant. Answer only using the provided evidence. Cite filename and page.\"\n",
    "    user_prompt = f\"Question: {question}\\n\\nEvidence:\\n{ctx}\\n\\nIf insufficient, say you don't know.\"\n",
    "    try:\n",
    "        resp = client.responses.create(model=model, input=[\n",
    "            {\"role\":\"system\",\"content\":sys_prompt},\n",
    "            {\"role\":\"user\",\"content\":user_prompt}\n",
    "        ])\n",
    "        answer = resp.output_text if hasattr(resp, \"output_text\") else str(resp)\n",
    "        print(answer.strip())\n",
    "    except Exception as e:\n",
    "        print(\"LLM call failed; printing context instead.\\nReason:\", e)\n",
    "        print(ctx)\n",
    "\n",
    "# Example (uncomment to try):\n",
    "# synthesize_answer(\"What problem does this document address?\", topk=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9954510d",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Sanity Checks & Simple Stats\n",
    "\n",
    "Quick checks to ensure counts and shapes look reasonable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfac5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_word_count(s: str) -> int:\n",
    "    return len(words(s))\n",
    "\n",
    "stats = df[\"text\"].apply(chunk_word_count).describe()\n",
    "print(\"Chunk length (words) stats:\\n\", stats.to_string())\n",
    "\n",
    "print(\"\\nVectors in collection:\", collection.count())\n",
    "print(\"Unique docs:\", df[\"doc_id\"].nunique())\n",
    "print(\"Chunking method(s):\", df[\"method\"].value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e7ce03",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Wrapâ€‘Up & Next Steps\n",
    "\n",
    "**Key takeaways**\n",
    "- Fixed-size chunks with overlap are simple and reliable; LLM-assisted segmentation can yield cleaner semantic boundaries but requires an API key and careful prompting.\n",
    "- Use the **same** embedding model for both documents and queries.\n",
    "- Persisting to ChromaDB allows fast iteration without re-embedding every run.\n",
    "\n",
    "**Try next**\n",
    "- Tune `CHUNK_WORDS`/`CHUNK_OVERLAP_WORDS`.\n",
    "- Swap the embedder: e.g., `\"BAAI/bge-small-en-v1.5\"` (remember to re-embed queries too).\n",
    "- Add a reranker (e.g., cross-encoder) to re-order top-k hits before synthesis.\n",
    "- Track sources when synthesizing answers (we included filename + page).\n",
    "\n",
    "Happy building! ðŸŽ“\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

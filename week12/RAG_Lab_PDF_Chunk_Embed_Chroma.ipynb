{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8746ded",
   "metadata": {},
   "source": [
    "\n",
    "# RAG Lab: PDF → Chunking → Embeddings → ChromaDB → Retrieval\n",
    "\n",
    "**Updated:** 2025-11-08\n",
    "\n",
    "Welcome! In this lab you'll build a small Retrieval-Augmented Generation (RAG) data pipeline:\n",
    "1. Load PDFs from a directory\n",
    "2. Chunk the text using either **fixed-size** chunks with **overlap** or **LLM-assisted** segmentation\n",
    "3. Embed the chunks with a compact Sentence-Transformers model\n",
    "4. Store vectors and metadata in a local **ChromaDB** collection\n",
    "5. Run a simple **retrieval** (top-k nearest neighbors) and optionally synthesize a short answer\n",
    "\n",
    "### What you'll learn\n",
    "- Why chunking and overlap improve recall\n",
    "- How to choose and use an embedding model consistently\n",
    "- How a local vector DB (ChromaDB) stores and retrieves embeddings\n",
    "- How to ground an answer with top-k passages\n",
    "\n",
    "### Glossary (quick)\n",
    "- **Chunk**: A small slice of text extracted from documents.\n",
    "- **Overlap**: Repeating some tokens between adjacent chunks to avoid cutting important context.\n",
    "- **Embedding**: A fixed-length vector representation of text.\n",
    "- **Vector DB**: A database that stores vectors and supports similarity search (e.g., top-k retrieval).\n",
    "- **Top-k retrieval**: Return the k most similar chunks to a query (nearest neighbors).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b02ea7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "080ddd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.13 | Platform: Linux-5.14.0-570.55.1.el9_6.x86_64-x86_64-with-glibc2.34\n",
      "PyMuPDF: 1.26.6\n",
      "chromadb: 1.3.2\n",
      "sentence_transformers: 5.1.2\n",
      "tqdm: 4.67.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ✅ Install minimal dependencies (CPU-friendly). If these are already installed, this cell is a no-op.\n",
    "# We pin versions for classroom reproducibility. Feel free to loosen pins later.\n",
    "# %pip -q install \"pymupdf<1.25\" \"chromadb==0.4.24\" \"sentence-transformers==2.5.1\" \"tqdm>=4.66.0\"\n",
    "\n",
    "\n",
    "import sys, platform, importlib\n",
    "print(\"Python:\", sys.version.split()[0], \"| Platform:\", platform.platform())\n",
    "\n",
    "def _ver(pkg):\n",
    "    try:\n",
    "        return importlib.import_module(pkg).__version__\n",
    "    except Exception as e:\n",
    "        return f\"not found ({e})\"\n",
    "\n",
    "print(\"PyMuPDF:\", _ver(\"fitz\"))\n",
    "print(\"chromadb:\", _ver(\"chromadb\"))\n",
    "print(\"sentence_transformers:\", _ver(\"sentence_transformers\"))\n",
    "print(\"tqdm:\", _ver(\"tqdm\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08c92b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Imports & configuration ----\n",
    "import os, re, uuid, glob, math, json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- Student-editable configuration ---\n",
    "PDF_DIR = \"./pdfs\"                  # directory with source PDFs\n",
    "PERSIST_DIR = \"./rag_chroma\"        # ChromaDB persistence path (folder will be created)\n",
    "COLLECTION_NAME = \"cnu_rag_lab\"     # collection name\n",
    "CHUNK_WORDS = 400                   # fixed-size chunk length (~words)\n",
    "CHUNK_OVERLAP_WORDS = 40            # overlap between chunks (~words)\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/msmarco-distilbert-cos-v5\"\n",
    "USE_LLM_CHUNKING = False            # set True to try optional LLM-assisted chunking\n",
    "MAX_PAGES_PER_PDF: Optional[int] = None  # set an int (e.g., 5) to limit pages for demos\n",
    "\n",
    "# Sanity: ensure dirs exist\n",
    "Path(PDF_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(PERSIST_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Helper for neat printing\n",
    "def head(df, n=5):\n",
    "    try:\n",
    "        from caas_jupyter_tools import display_dataframe_to_user\n",
    "        display_dataframe_to_user(\"Preview\", df.head(n))\n",
    "    except Exception:\n",
    "        display(df.head(n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5898da4",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Load PDFs\n",
    "\n",
    "We'll extract text with **PyMuPDF** page-by-page. Empty or whitespace-only pages are dropped.  \n",
    "**Why this matters:** We want a clean, normalized text corpus before chunking to ensure consistent chunk lengths and quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da437972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain --quiet\n",
    "# ! pip install langchain-community --quiet\n",
    "# !pip install pypdf --quiet\n",
    "# !pip install --upgrade langchain langchain-community\n",
    "# !pip install --upgrade langchain\n",
    "# !pip uninstall langchain -y\n",
    "# !pip install langchain\n",
    "\n",
    "# !pip install -U langchain-community\n",
    "# !pip install -qU langchain-community pypdf\n",
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c24f7856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2024-08-05T09:56:16-04:00', 'gts_pdfxconformance': 'PDF/X-1a:2001', 'gts_pdfxversion': 'PDF/X-1:2001', 'moddate': '2024-08-09T10:29:40-04:00', 'title': 'Christopher Newport University: Undergraduate Catalog 2024-25', 'trapped': '/False', 'source': './pdfs/2024-25-undergraduate_catalog.pdf', 'total_pages': 323, 'page': 0, 'page_label': '1'}, page_content='Undergraduate Catalog \\nV olume 59, Number 1, July 2024 \\nThe provisions of this catalog do not constitute a contract, expressed or implied, between any applicant or student and the \\nRector and Board of Visitors of Christopher Newport University. The University reserves the right to change any of the provi-\\nsions, schedules, programs, courses, rules, regulations, or fees whenever the University deems it expedient to do so. \\nChristopher Newport University is committed to providing an environment that emphasizes the dignity and worth of every \\nmember of its community and that is free from harassment and discrimination in admissions, employment, and education pro-\\ngrams or activities based on race, color, religion, sex, national origin, age, disability, genetic information, sexual orientation, \\ngender identity, marital status, military/veteran status, political affiliation, pregnancy, or any other status protected by law. Such \\nan environment is necessary to a healthy learning, working, and living atmosphere because discrimination and harassment \\nundermine human dignity and the positive connection among everyone on campus. In pursuit of this goal, any question of \\nimpermissible discrimination and/or harassment on these bases will be addressed with efficiency and energy in accordance with \\nthe Discrimination, Harassment and Sexual Misconduct Policy. Anyone having questions concerning the policy and procedures \\nshould contact the Director of Institutional Compliance/Title IX Coordinator, Christopher Newport University, 1 Avenue of the \\nArts, 100 Christopher Newport Hall, Newport News, V A, 20606, (757) 594-8819, titleixeo@cnu.edu. \\nUnder Title IX of the Education Amendments of 1972, discrimination and harassment on the basis of sex in any education \\nprogram or activity including admission and employment is prohibited. Inquiries about this may be directed to the Director \\nof Title IX and Equal Opportunity or to the U.S. Department of Education, Office of Civil Rights, 400 Maryland Avenue SW, \\nWashington, DC, 20202, (800) 421-3481, OCR@ed.gov. \\n1 Avenue of the Arts \\nNewport News, V A 23606-3072 \\nPhone: (757) 594-7000 / TDD: (757) 594-7938 \\ncnu.edu \\n1')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import glob\n",
    "\n",
    "pages = []\n",
    "pdf_files = glob.glob(os.path.join(PDF_DIR, \"*.pdf\"))\n",
    "for pdf_path in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages.extend(loader.load())\n",
    "print(len(pages))\n",
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce36dbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 338 non-empty pages from ./pdfs.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def normalize_ws(text: str) -> str:\n",
    "    '''Normalize whitespace and strip control chars.'''\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\\n\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\n\\s+\", \"\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_pdfs(pdf_dir: str, max_pages: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    '''\n",
    "    Extract text from all PDFs in a directory.\n",
    "    Returns a list of page records: {\"doc_id\",\"source\",\"page\",\"text\"}.\n",
    "    '''\n",
    "    records = []\n",
    "    paths = sorted(glob.glob(os.path.join(pdf_dir, \"*.pdf\")))\n",
    "    if not paths:\n",
    "        print(f\"[WARN] No PDFs found in {pdf_dir}. Place some PDFs there and re-run.\")\n",
    "        return records\n",
    "\n",
    "    for path in paths:\n",
    "        try:\n",
    "            doc = fitz.open(path)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not open {path}: {e}\")\n",
    "            continue\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        page_count = len(doc)\n",
    "        limit = min(page_count, max_pages) if isinstance(max_pages, int) else page_count\n",
    "        for i in range(limit):\n",
    "            try:\n",
    "                text = doc[i].get_text()\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not read page {i} of {path}: {e}\")\n",
    "                continue\n",
    "            text = normalize_ws(text)\n",
    "            if text:\n",
    "                records.append({\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"source\": os.path.basename(path),\n",
    "                    \"page\": i + 1,  # 1-based for humans\n",
    "                    \"text\": text\n",
    "                })\n",
    "        doc.close()\n",
    "    return records\n",
    "\n",
    "pages = load_pdfs(PDF_DIR, MAX_PAGES_PER_PDF)\n",
    "print(f\"Loaded {len(pages)} non-empty pages from {PDF_DIR}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54500eb2",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Chunking (Fixed-Size with Overlap)\n",
    "\n",
    "We'll implement a tokenizer-free word splitter and then slide a window with overlap.  \n",
    "**Why overlap?** It preserves context that may straddle chunk boundaries, improving recall during retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f004a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import langchain_community\n",
    "# !pip uninstall langchain-community -y\n",
    "# !pip install langchain-community\n",
    "# import langchain_text_splitters\n",
    "# dir(langchain_text_splitters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73bcfeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    # chunk_overlap=20,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b909b06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4850"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many chunks\n",
    "docs=r_splitter.split_documents(pages)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bfc363d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Document' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     32\u001b[39m             out.append({\n\u001b[32m     33\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mchunk_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(uuid.uuid4()),\n\u001b[32m     34\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mdoc_id\u001b[39m\u001b[33m\"\u001b[39m: rec[\u001b[33m\"\u001b[39m\u001b[33mdoc_id\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: ch\n\u001b[32m     39\u001b[39m             })\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m chunk_records_fixed = \u001b[43mchunk_pages_fixed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpages\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m pages \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFixed-size chunking produced \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunk_records_fixed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunk_records_fixed:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mchunk_pages_fixed\u001b[39m\u001b[34m(page_records)\u001b[39m\n\u001b[32m     29\u001b[39m out = []\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rec \u001b[38;5;129;01min\u001b[39;00m page_records:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m chunk_fixed(\u001b[43mrec\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, CHUNK_WORDS, CHUNK_OVERLAP_WORDS):\n\u001b[32m     32\u001b[39m         out.append({\n\u001b[32m     33\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mchunk_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(uuid.uuid4()),\n\u001b[32m     34\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdoc_id\u001b[39m\u001b[33m\"\u001b[39m: rec[\u001b[33m\"\u001b[39m\u001b[33mdoc_id\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: ch\n\u001b[32m     39\u001b[39m         })\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[31mTypeError\u001b[39m: 'Document' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "WORD_RE = re.compile(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", flags=re.UNICODE)\n",
    "\n",
    "def words(text: str):\n",
    "    'A lightweight word+punctuation splitter.'\n",
    "    return WORD_RE.findall(text)\n",
    "\n",
    "def chunk_fixed(text: str, size_words: int = CHUNK_WORDS, overlap_words: int = CHUNK_OVERLAP_WORDS):\n",
    "    '''\n",
    "    Split text into overlapping chunks measured in approx words.\n",
    "    Returns a list of chunk strings.\n",
    "    '''\n",
    "    w = words(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(w):\n",
    "        end = min(start + size_words, len(w))\n",
    "        chunk_text = \" \".join(w[start:end])\n",
    "        if chunk_text.strip():\n",
    "            chunks.append(chunk_text)\n",
    "        if end == len(w):\n",
    "            break\n",
    "        start = max(end - overlap_words, 0)\n",
    "    return chunks\n",
    "\n",
    "def chunk_pages_fixed(page_records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    '''\n",
    "    Map pages -> fixed-size chunks with metadata for each chunk.\n",
    "    '''\n",
    "    out = []\n",
    "    for rec in page_records:\n",
    "        for ch in chunk_fixed(rec[\"text\"], CHUNK_WORDS, CHUNK_OVERLAP_WORDS):\n",
    "            out.append({\n",
    "                \"chunk_id\": str(uuid.uuid4()),\n",
    "                \"doc_id\": rec[\"doc_id\"],\n",
    "                \"source\": rec[\"source\"],\n",
    "                \"page\": rec[\"page\"],\n",
    "                \"method\": \"fixed\",\n",
    "                \"text\": ch\n",
    "            })\n",
    "    return out\n",
    "\n",
    "chunk_records_fixed = chunk_pages_fixed(pages) if pages else []\n",
    "print(f\"Fixed-size chunking produced {len(chunk_records_fixed)} chunks.\")\n",
    "if chunk_records_fixed:\n",
    "    import pandas as _pd\n",
    "    _df_chunks_fixed = _pd.DataFrame(chunk_records_fixed)\n",
    "    head(_df_chunks_fixed[ ['chunk_id','source','page','method','text'] ], 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25a1f687",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_df_chunks_fixed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43m_df_chunks_fixed\u001b[49m[\u001b[32m0\u001b[39m].text\n",
      "\u001b[31mNameError\u001b[39m: name '_df_chunks_fixed' is not defined"
     ]
    }
   ],
   "source": [
    "_df_chunks_fixed[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faea1664",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Optional: LLM-Assisted Chunking (Semantic Segmentation)\n",
    "\n",
    "**Idea:** Ask an LLM to segment a page into coherent sections (headings, paragraphs, lists). Then, for any very long segments, re-chunk using the fixed method above so that chunks stay small.\n",
    "\n",
    "> This path is **optional** and requires an API key. If you set `USE_LLM_CHUNKING = True`, make sure `OPENAI_API_KEY` is in your environment. Keep your key private.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d261a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM-assisted path produced 0 chunks (0 if not enabled).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def llm_segment_page(text: str, model: str = \"gpt-4o-mini\") -> List[str]:\n",
    "    '''\n",
    "    Use an LLM to segment the page text into coherent sections.\n",
    "    Returns a list of segments. On any error or short text, returns [text].\n",
    "    '''\n",
    "    text = text.strip()\n",
    "    if len(text.split()) < 80:\n",
    "        return [text]\n",
    "    try:\n",
    "        import os\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            # No key -> no network call; just fall back gracefully\n",
    "            return [text]\n",
    "\n",
    "        # Install only if needed\n",
    "        try:\n",
    "            from openai import OpenAI  # modern SDK\n",
    "        except Exception:\n",
    "            import sys\n",
    "            print(\"[INFO] Installing openai client...\")\n",
    "            !{sys.executable} -m pip -q install --upgrade openai\n",
    "            from openai import OpenAI\n",
    "\n",
    "        client = OpenAI(api_key=api_key)\n",
    "\n",
    "        prompt = (\n",
    "            \"You segment text into coherent sections such as headings and paragraphs. \"\n",
    "            \"Return a JSON list of strings where each string is a segment. \"\n",
    "            \"Keep segments between 150-400 words when possible.\\n\\n\"\n",
    "            f\"TEXT:\\n{text[:8000]}\"\n",
    "        )\n",
    "        resp = client.responses.create(\n",
    "            model=model,\n",
    "            input=[{\"role\":\"user\",\"content\":prompt}],\n",
    "            response_format={\"type\":\"json_object\"}\n",
    "        )\n",
    "        # Extract JSON safely\n",
    "        content = resp.output[0].content[0].text  # SDK shapes can vary; adjust if needed\n",
    "        import json as _json\n",
    "        parsed = _json.loads(content)\n",
    "        segments = parsed.get(\"segments\") or parsed.get(\"data\") or parsed.get(\"list\")\n",
    "        if isinstance(segments, list) and all(isinstance(s, str) for s in segments):\n",
    "            # light cleanup\n",
    "            segments = [s.strip() for s in segments if s.strip()]\n",
    "            return segments or [text]\n",
    "        return [text]\n",
    "    except Exception as e:\n",
    "        # Silent, robust fallback\n",
    "        return [text]\n",
    "\n",
    "def rechunk_if_long(segments: List[str], max_words: int = 600) -> List[str]:\n",
    "    '''If a segment is too long, re-chunk with the fixed method to keep sizes bounded.'''\n",
    "    out = []\n",
    "    for s in segments:\n",
    "        if len(words(s)) > max_words:\n",
    "            out.extend(chunk_fixed(s, CHUNK_WORDS, CHUNK_OVERLAP_WORDS))\n",
    "        else:\n",
    "            out.append(s)\n",
    "    return out\n",
    "\n",
    "def chunk_pages_llm(page_records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    '''\n",
    "    Segment each page via LLM, then bound segment length. Tag method='llm'.\n",
    "    Requires OPENAI_API_KEY for actual LLM calls; otherwise falls back implicitly.\n",
    "    '''\n",
    "    out = []\n",
    "    for rec in tqdm(page_records, desc=\"LLM segmenting pages\"):\n",
    "        segs = llm_segment_page(rec[\"text\"])\n",
    "        segs = rechunk_if_long(segs, max_words=600)\n",
    "        for s in segs:\n",
    "            out.append({\n",
    "                \"chunk_id\": str(uuid.uuid4()),\n",
    "                \"doc_id\": rec[\"doc_id\"],\n",
    "                \"source\": rec[\"source\"],\n",
    "                \"page\": rec[\"page\"],\n",
    "                \"method\": \"llm\",\n",
    "                \"text\": s\n",
    "            })\n",
    "    return out\n",
    "\n",
    "chunk_records_llm = chunk_pages_llm(pages) if (pages and USE_LLM_CHUNKING) else []\n",
    "print(f\"LLM-assisted path produced {len(chunk_records_llm)} chunks (0 if not enabled).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c7fdc",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Embeddings\n",
    "\n",
    "We'll use a compact, high-quality model: `sentence-transformers/all-MiniLM-L6-v2` (384-dim).  \n",
    "**Why this matters:** Consistent embeddings for documents **and** queries are necessary for good retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1c049f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 924\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>source</th>\n",
       "      <th>page</th>\n",
       "      <th>method</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2d285ae4-71fd-4d66-b1c1-93132a996007</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>fixed</td>\n",
       "      <td>Undergraduate Catalog Volume 59 , Number 1 , J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c0feb838-f099-4814-abf2-b58fcbfe9450</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>fixed</td>\n",
       "      <td>20202 , ( 800 ) 421 - 3481 , OCR @ ed . gov . ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9f33e6c1-a159-4a58-9d76-6da6a23bc408</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>fixed</td>\n",
       "      <td>2 Christopher Newport University 2024 - 2025 W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f85ed341-61c6-4360-ba65-c3bf81d48285</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>fixed</td>\n",
       "      <td>we have produced more than 725 All - Americans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a3e7d43e-1064-40c1-affa-eaf726b04003</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>fixed</td>\n",
       "      <td>is also a center of high - tech development an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               chunk_id                             source  \\\n",
       "0  2d285ae4-71fd-4d66-b1c1-93132a996007  2024-25-undergraduate_catalog.pdf   \n",
       "1  c0feb838-f099-4814-abf2-b58fcbfe9450  2024-25-undergraduate_catalog.pdf   \n",
       "2  9f33e6c1-a159-4a58-9d76-6da6a23bc408  2024-25-undergraduate_catalog.pdf   \n",
       "3  f85ed341-61c6-4360-ba65-c3bf81d48285  2024-25-undergraduate_catalog.pdf   \n",
       "4  a3e7d43e-1064-40c1-affa-eaf726b04003  2024-25-undergraduate_catalog.pdf   \n",
       "\n",
       "   page method                                               text  \n",
       "0     1  fixed  Undergraduate Catalog Volume 59 , Number 1 , J...  \n",
       "1     1  fixed  20202 , ( 800 ) 421 - 3481 , OCR @ ed . gov . ...  \n",
       "2     2  fixed  2 Christopher Newport University 2024 - 2025 W...  \n",
       "3     2  fixed  we have produced more than 725 All - Americans...  \n",
       "4     2  fixed  is also a center of high - tech development an...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded: sentence-transformers/msmarco-distilbert-cos-v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks: 100%|██████████| 15/15 [00:02<00:00,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (924, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Choose which chunks to use based on the flag\n",
    "chunks = chunk_records_llm if (USE_LLM_CHUNKING and chunk_records_llm) else chunk_records_fixed\n",
    "\n",
    "if not chunks:\n",
    "    raise RuntimeError(\"No chunks available. Add PDFs to PDF_DIR and re-run the earlier cells.\")\n",
    "\n",
    "# Build a DataFrame for convenience\n",
    "df = pd.DataFrame(chunks)\n",
    "print(\"Chunks:\", len(df))\n",
    "head(df[['chunk_id','source','page','method','text']], 5)\n",
    "\n",
    "# Load the embedder and encode in batches\n",
    "embedder = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "print(\"Embedding model loaded:\", EMBED_MODEL_NAME)\n",
    "\n",
    "batch_size = 64\n",
    "embeddings = []\n",
    "for i in tqdm(range(0, len(df), batch_size), desc=\"Embedding chunks\"):\n",
    "    batch_texts = df['text'].iloc[i:i+batch_size].tolist()\n",
    "    batch_vecs = embedder.encode(batch_texts, batch_size=min(32, len(batch_texts)), show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    embeddings.append(batch_vecs)\n",
    "\n",
    "embeddings = np.vstack(embeddings).astype(\"float32\")\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "df[\"embedding\"] = list(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f39f3",
   "metadata": {},
   "source": [
    "\n",
    "## 5) ChromaDB: Create / Persist Collection\n",
    "\n",
    "We'll use a **PersistentClient** so your vectors survive across sessions in `PERSIST_DIR`.  \n",
    "**Why this matters:** Persistence lets you build once and query many times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58672a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'cnu_rag_lab' deleted successfully.\n",
      "Collection 'cnu_rag_lab' now has 924 vectors. Persisted at: ./rag_chroma\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "\n",
    "try:\n",
    "    # Delete the collection if it exists\n",
    "    client.delete_collection(name=COLLECTION_NAME)\n",
    "    print(f\"Collection '{COLLECTION_NAME}' deleted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting collection '{COLLECTION_NAME}': {e}\")\n",
    "\n",
    "# Create or get the collection. Using upsert allows repeatable runs.\n",
    "try:\n",
    "    collection = client.get_collection(COLLECTION_NAME)\n",
    "except Exception:\n",
    "    collection = client.create_collection(COLLECTION_NAME)\n",
    "\n",
    "# Upsert (safe to re-run)\n",
    "collection.upsert(\n",
    "    ids=df[\"chunk_id\"].tolist(),\n",
    "    embeddings=df[\"embedding\"].tolist(),\n",
    "    metadatas=df[[\"doc_id\", \"source\", \"page\", \"method\"]].to_dict(orient=\"records\"),\n",
    "    documents=df[\"text\"].tolist()\n",
    ")\n",
    "print(f\"Collection '{COLLECTION_NAME}' now has {collection.count()} vectors. Persisted at: {PERSIST_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c3f276",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Retrieval: Top‑k Nearest Chunks\n",
    "\n",
    "We'll embed the user query with the **same model** and query ChromaDB for the nearest chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "493b3ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try: What are the pre reqs for CPSC 475\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>distance</th>\n",
       "      <th>source</th>\n",
       "      <th>page</th>\n",
       "      <th>method</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.1982</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>252</td>\n",
       "      <td>fixed</td>\n",
       "      <td>1eecb548-41f1-4c70-b5e3-cb8269682cd5</td>\n",
       "      <td>and I / O subsystems ; special purpose archite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.2041</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>253</td>\n",
       "      <td>fixed</td>\n",
       "      <td>af88a296-cdef-4629-831d-cf94008dd333</td>\n",
       "      <td>pages ; interactive web pages ; publishing too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.2056</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>254</td>\n",
       "      <td>fixed</td>\n",
       "      <td>648b3efa-91e2-4636-925e-5d813e27a99f</td>\n",
       "      <td>( 3 - 3 - 0 ) Prerequisite : Grade of C - or h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.2073</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>93</td>\n",
       "      <td>fixed</td>\n",
       "      <td>2ce4669b-1230-483e-a948-9e206717832e</td>\n",
       "      <td>of C . This course introduces students to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.2730</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>255</td>\n",
       "      <td>fixed</td>\n",
       "      <td>a32d1f70-8db0-49bf-8743-e033af20808e</td>\n",
       "      <td>255 2024 - 2025 Physics , Computer Science and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1.2739</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>253</td>\n",
       "      <td>fixed</td>\n",
       "      <td>eed162a1-736d-4924-8359-40b656b33ef2</td>\n",
       "      <td>3 - 0 ) LLFR This course is an introduction to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1.2817</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>256</td>\n",
       "      <td>fixed</td>\n",
       "      <td>f032e498-0610-4090-adf4-3020153296a3</td>\n",
       "      <td>256 Physics , Computer Science and Engineering...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1.2903</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>286</td>\n",
       "      <td>fixed</td>\n",
       "      <td>39bff804-738b-42d6-9bf6-0e390a21efff</td>\n",
       "      <td>286 Psychology PSYC 431L . Psychology of Archi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1.2912</td>\n",
       "      <td>Information Science.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>fixed</td>\n",
       "      <td>c3e015aa-7f83-438f-92af-1edb24413c9f</td>\n",
       "      <td>Information Science 4 - Year Plan Information ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1.3387</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>254</td>\n",
       "      <td>fixed</td>\n",
       "      <td>f3b55bc5-1b0f-4ac9-91f3-51c61d274184</td>\n",
       "      <td>included in network configuration . Once netwo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1.3515</td>\n",
       "      <td>Computer Science.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>fixed</td>\n",
       "      <td>a5f98efa-336a-40f5-9893-c1ea3cbaa3b2</td>\n",
       "      <td>Computer Science ( Precalculus ) Course Credit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1.3528</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>275</td>\n",
       "      <td>fixed</td>\n",
       "      <td>688cd573-d29c-4640-b318-564a7b52b91b</td>\n",
       "      <td>202 / 202L ( one year of College or University...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1.3648</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>254</td>\n",
       "      <td>fixed</td>\n",
       "      <td>c4e6ed72-f04a-422a-ba2b-902ef6b9280c</td>\n",
       "      <td>254 Physics , Computer Science and Engineering...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1.3771</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>92</td>\n",
       "      <td>fixed</td>\n",
       "      <td>1a101998-596f-4d27-9100-c9f04ec72bef</td>\n",
       "      <td>and the use of financial data to make decision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1.3789</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>30</td>\n",
       "      <td>fixed</td>\n",
       "      <td>e59f6744-637f-4811-bda6-e19c5dbb1f47</td>\n",
       "      <td>depending on the existence of a com ­ parable ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1.3808</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>249</td>\n",
       "      <td>fixed</td>\n",
       "      <td>514dba6f-a04c-446a-891f-648440dd9cc1</td>\n",
       "      <td>249 2024 - 2025 Physics , Computer Science and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1.3958</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>286</td>\n",
       "      <td>fixed</td>\n",
       "      <td>abae1a30-fa62-411f-ab0b-f876c49e804e</td>\n",
       "      <td>- 3 - 0 ) Prerequisites : Senior standing ; EN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1.3975</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>252</td>\n",
       "      <td>fixed</td>\n",
       "      <td>25254f08-29d9-4e6d-b421-ef5792cce5b7</td>\n",
       "      <td>, or 504 ( these courses can replace PHYS 401 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1.4077</td>\n",
       "      <td>2024-25-undergraduate_catalog.pdf</td>\n",
       "      <td>93</td>\n",
       "      <td>fixed</td>\n",
       "      <td>6aa394d5-61ec-4e68-bd1a-2bb3cb8cd01e</td>\n",
       "      <td>3 - 3 - 0 ) [ Formerly BUSN 370 ] Prerequisite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1.4081</td>\n",
       "      <td>Cybersecurity.pdf</td>\n",
       "      <td>4</td>\n",
       "      <td>fixed</td>\n",
       "      <td>e65db0b8-c119-4750-8a1a-cbab3bd1a6ff</td>\n",
       "      <td>Cybersecurity 4 - Year Plan Cybersecurity ( Ho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank  distance                             source  page method  \\\n",
       "0      1    1.1982  2024-25-undergraduate_catalog.pdf   252  fixed   \n",
       "1      2    1.2041  2024-25-undergraduate_catalog.pdf   253  fixed   \n",
       "2      3    1.2056  2024-25-undergraduate_catalog.pdf   254  fixed   \n",
       "3      4    1.2073  2024-25-undergraduate_catalog.pdf    93  fixed   \n",
       "4      5    1.2730  2024-25-undergraduate_catalog.pdf   255  fixed   \n",
       "5      6    1.2739  2024-25-undergraduate_catalog.pdf   253  fixed   \n",
       "6      7    1.2817  2024-25-undergraduate_catalog.pdf   256  fixed   \n",
       "7      8    1.2903  2024-25-undergraduate_catalog.pdf   286  fixed   \n",
       "8      9    1.2912            Information Science.pdf     1  fixed   \n",
       "9     10    1.3387  2024-25-undergraduate_catalog.pdf   254  fixed   \n",
       "10    11    1.3515               Computer Science.pdf     1  fixed   \n",
       "11    12    1.3528  2024-25-undergraduate_catalog.pdf   275  fixed   \n",
       "12    13    1.3648  2024-25-undergraduate_catalog.pdf   254  fixed   \n",
       "13    14    1.3771  2024-25-undergraduate_catalog.pdf    92  fixed   \n",
       "14    15    1.3789  2024-25-undergraduate_catalog.pdf    30  fixed   \n",
       "15    16    1.3808  2024-25-undergraduate_catalog.pdf   249  fixed   \n",
       "16    17    1.3958  2024-25-undergraduate_catalog.pdf   286  fixed   \n",
       "17    18    1.3975  2024-25-undergraduate_catalog.pdf   252  fixed   \n",
       "18    19    1.4077  2024-25-undergraduate_catalog.pdf    93  fixed   \n",
       "19    20    1.4081                  Cybersecurity.pdf     4  fixed   \n",
       "\n",
       "                                chunk_id  \\\n",
       "0   1eecb548-41f1-4c70-b5e3-cb8269682cd5   \n",
       "1   af88a296-cdef-4629-831d-cf94008dd333   \n",
       "2   648b3efa-91e2-4636-925e-5d813e27a99f   \n",
       "3   2ce4669b-1230-483e-a948-9e206717832e   \n",
       "4   a32d1f70-8db0-49bf-8743-e033af20808e   \n",
       "5   eed162a1-736d-4924-8359-40b656b33ef2   \n",
       "6   f032e498-0610-4090-adf4-3020153296a3   \n",
       "7   39bff804-738b-42d6-9bf6-0e390a21efff   \n",
       "8   c3e015aa-7f83-438f-92af-1edb24413c9f   \n",
       "9   f3b55bc5-1b0f-4ac9-91f3-51c61d274184   \n",
       "10  a5f98efa-336a-40f5-9893-c1ea3cbaa3b2   \n",
       "11  688cd573-d29c-4640-b318-564a7b52b91b   \n",
       "12  c4e6ed72-f04a-422a-ba2b-902ef6b9280c   \n",
       "13  1a101998-596f-4d27-9100-c9f04ec72bef   \n",
       "14  e59f6744-637f-4811-bda6-e19c5dbb1f47   \n",
       "15  514dba6f-a04c-446a-891f-648440dd9cc1   \n",
       "16  abae1a30-fa62-411f-ab0b-f876c49e804e   \n",
       "17  25254f08-29d9-4e6d-b421-ef5792cce5b7   \n",
       "18  6aa394d5-61ec-4e68-bd1a-2bb3cb8cd01e   \n",
       "19  e65db0b8-c119-4750-8a1a-cbab3bd1a6ff   \n",
       "\n",
       "                                              snippet  \n",
       "0   and I / O subsystems ; special purpose archite...  \n",
       "1   pages ; interactive web pages ; publishing too...  \n",
       "2   ( 3 - 3 - 0 ) Prerequisite : Grade of C - or h...  \n",
       "3   of C . This course introduces students to the ...  \n",
       "4   255 2024 - 2025 Physics , Computer Science and...  \n",
       "5   3 - 0 ) LLFR This course is an introduction to...  \n",
       "6   256 Physics , Computer Science and Engineering...  \n",
       "7   286 Psychology PSYC 431L . Psychology of Archi...  \n",
       "8   Information Science 4 - Year Plan Information ...  \n",
       "9   included in network configuration . Once netwo...  \n",
       "10  Computer Science ( Precalculus ) Course Credit...  \n",
       "11  202 / 202L ( one year of College or University...  \n",
       "12  254 Physics , Computer Science and Engineering...  \n",
       "13  and the use of financial data to make decision...  \n",
       "14  depending on the existence of a com ­ parable ...  \n",
       "15  249 2024 - 2025 Physics , Computer Science and...  \n",
       "16  - 3 - 0 ) Prerequisites : Senior standing ; EN...  \n",
       "17  , or 504 ( these courses can replace PHYS 401 ...  \n",
       "18  3 - 3 - 0 ) [ Formerly BUSN 370 ] Prerequisite...  \n",
       "19  Cybersecurity 4 - Year Plan Cybersecurity ( Ho...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def embed_query(q: str) -> np.ndarray:\n",
    "    '''Embed a query string using the same SentenceTransformer model (normalized).'''\n",
    "    v = embedder.encode([q], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return v[0].astype(\"float32\")\n",
    "\n",
    "def query_topk(question: str, k: int = 5) -> pd.DataFrame:\n",
    "    '''Return a pretty DataFrame with rank, distance (smaller is closer), and metadata.'''\n",
    "    q_emb = embed_query(question)\n",
    "    res = collection.query(\n",
    "        query_embeddings=[q_emb],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    # Flatten results\n",
    "    rows = []\n",
    "    for rank, (cid, dist, meta, doc) in enumerate(zip(res[\"ids\"][0], res[\"distances\"][0], res[\"metadatas\"][0], res[\"documents\"][0]), start=1):\n",
    "        snippet = (doc[:240] + \"…\") if len(doc) > 240 else doc\n",
    "        rows.append({\n",
    "            \"rank\": rank,\n",
    "            \"distance\": round(float(dist), 4),\n",
    "            \"source\": meta.get(\"source\"),\n",
    "            \"page\": meta.get(\"page\"),\n",
    "            \"method\": meta.get(\"method\"),\n",
    "            \"chunk_id\": cid,\n",
    "            \"snippet\": snippet\n",
    "        })\n",
    "    out = pd.DataFrame(rows)\n",
    "    return out\n",
    "\n",
    "# Try a query (edit this to your corpus):\n",
    "example_queries = [\n",
    "    \"What are the pre reqs for CPSC 475\",\n",
    "    \"What are the key definitions introduced?\",\n",
    "    \"List the steps or procedures mentioned in the text.\"\n",
    "]\n",
    "print(\"Try:\", example_queries[0])\n",
    "res_df = query_topk(example_queries[0], k=20)\n",
    "head(res_df, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26969159",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Mini‑RAG: Concise Answer Synthesis (Optional)\n",
    "\n",
    "If you have an API key in your environment (`OPENAI_API_KEY`), we can ask a model for a **short, grounded** answer. Otherwise, we'll just show the retrieved context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "214e9744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_context(df_hits: pd.DataFrame) -> str:\n",
    "    ctx_lines = []\n",
    "    for _, row in df_hits.iterrows():\n",
    "        ctx_lines.append(f\"[{row['source']} p.{row['page']} | {row['method']}] {row['snippet']}\")\n",
    "    return \"\\n\".join(ctx_lines)\n",
    "\n",
    "def synthesize_answer(question: str, topk: int = 5, model: str = \"gpt-4o-mini\") -> None:\n",
    "    hits = query_topk(question, k=topk)\n",
    "    ctx = build_context(hits)\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"No OPENAI_API_KEY found. Showing retrieved context instead:\\n\")\n",
    "        print(ctx)\n",
    "        return\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "    except Exception:\n",
    "        import sys\n",
    "        print(\"[INFO] Installing openai client...\")\n",
    "        !{sys.executable} -m pip -q install --upgrade openai\n",
    "        from openai import OpenAI\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    sys_prompt = \"You are a concise teaching assistant. Answer only using the provided evidence. Cite filename and page.\"\n",
    "    user_prompt = f\"Question: {question}\\n\\nEvidence:\\n{ctx}\\n\\nIf insufficient, say you don't know.\"\n",
    "    try:\n",
    "        resp = client.responses.create(model=model, input=[\n",
    "            {\"role\":\"system\",\"content\":sys_prompt},\n",
    "            {\"role\":\"user\",\"content\":user_prompt}\n",
    "        ])\n",
    "        answer = resp.output_text if hasattr(resp, \"output_text\") else str(resp)\n",
    "        print(answer.strip())\n",
    "    except Exception as e:\n",
    "        print(\"LLM call failed; printing context instead.\\nReason:\", e)\n",
    "        print(ctx)\n",
    "\n",
    "# Example (uncomment to try):\n",
    "# synthesize_answer(\"What problem does this document address?\", topk=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9954510d",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Sanity Checks & Simple Stats\n",
    "\n",
    "Quick checks to ensure counts and shapes look reasonable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dfac5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk length (words) stats:\n",
      " count    924.000000\n",
      "mean     334.988095\n",
      "std      100.712311\n",
      "min        1.000000\n",
      "25%      269.250000\n",
      "50%      400.000000\n",
      "75%      400.000000\n",
      "max      400.000000\n",
      "\n",
      "Vectors in collection: 924\n",
      "Unique docs: 4\n",
      "Chunking method(s): {'fixed': 924}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def chunk_word_count(s: str) -> int:\n",
    "    return len(words(s))\n",
    "\n",
    "stats = df[\"text\"].apply(chunk_word_count).describe()\n",
    "print(\"Chunk length (words) stats:\\n\", stats.to_string())\n",
    "\n",
    "print(\"\\nVectors in collection:\", collection.count())\n",
    "print(\"Unique docs:\", df[\"doc_id\"].nunique())\n",
    "print(\"Chunking method(s):\", df[\"method\"].value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e7ce03",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Wrap‑Up & Next Steps\n",
    "\n",
    "**Key takeaways**\n",
    "- Fixed-size chunks with overlap are simple and reliable; LLM-assisted segmentation can yield cleaner semantic boundaries but requires an API key and careful prompting.\n",
    "- Use the **same** embedding model for both documents and queries.\n",
    "- Persisting to ChromaDB allows fast iteration without re-embedding every run.\n",
    "\n",
    "**Try next**\n",
    "- Tune `CHUNK_WORDS`/`CHUNK_OVERLAP_WORDS`.\n",
    "- Swap the embedder: e.g., `\"BAAI/bge-small-en-v1.5\"` (remember to re-embed queries too).\n",
    "- Add a reranker (e.g., cross-encoder) to re-order top-k hits before synthesis.\n",
    "- Track sources when synthesizing answers (we included filename + page).\n",
    "\n",
    "Happy building! 🎓\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

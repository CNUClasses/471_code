{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540331d5",
   "metadata": {},
   "source": [
    "\n",
    "# RAG Lab 2: Query → Retrieve (Chroma) → Rerank → HF LLM Answer\n",
    "\n",
    "**Updated:** 2025-11-08\n",
    "\n",
    "In this short lab you will:\n",
    "1. Enter a natural-language **query** via a widget\n",
    "2. **Embed** the query using the **same model** as your document embeddings\n",
    "3. Retrieve **top k×2** candidates from an **existing ChromaDB** collection\n",
    "4. **Rerank** those candidates with a **cross-encoder** reranker\n",
    "5. Keep the **top k** passages and build a grounded prompt\n",
    "6. Run **Hugging Face LLM** inference and display the **answer**\n",
    "\n",
    "> Goal: illustrate a lean RAG pipeline with reranking. Designed for CPU-only machines and minimal code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced5dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Minimal dependencies (CPU friendly). If already installed, this is a no-op.\n",
    "%pip -q install \"chromadb==0.4.24\" \"sentence-transformers==2.5.1\" \"transformers>=4.41.0\" \"ipywidgets>=8.1.0\" \"tqdm>=4.66.0\"\n",
    "\n",
    "import sys, platform, importlib\n",
    "print(\"Python:\", sys.version.split()[0], \"| Platform:\", platform.platform())\n",
    "\n",
    "def _ver(m):\n",
    "    try:\n",
    "        return importlib.import_module(m).__version__\n",
    "    except Exception as e:\n",
    "        return f\"not found ({e})\"\n",
    "\n",
    "print(\"chromadb:\", _ver(\"chromadb\"))\n",
    "print(\"sentence_transformers:\", _ver(\"sentence_transformers\"))\n",
    "print(\"transformers:\", _ver(\"transformers\"))\n",
    "print(\"ipywidgets:\", _ver(\"ipywidgets\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e53a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Imports & configuration ----\n",
    "import os, numpy as np, pandas as pd\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as w\n",
    "\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import pipeline\n",
    "\n",
    "# === Paths & model names (match prior lab defaults) ===\n",
    "PERSIST_DIR    = \"./rag_chroma\"                      # where the prior lab persisted vectors\n",
    "COLLECTION_NAME= \"cnu_rag_lab\"                       # collection name used previously\n",
    "EMBED_MODEL    = \"sentence-transformers/all-MiniLM-L6-v2\"  # same embedder for query\n",
    "RERANK_MODEL   = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"    # compact cross-encoder\n",
    "LLM_MODEL      = \"google/flan-t5-base\"                     # small HF seq2seq model (CPU-ok)\n",
    "K_DEFAULT      = 5\n",
    "\n",
    "# Connect to existing Chroma collection\n",
    "client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "try:\n",
    "    collection = client.get_collection(COLLECTION_NAME)\n",
    "    print(f\"Connected to collection '{COLLECTION_NAME}' with {collection.count()} vectors at {PERSIST_DIR}\")\n",
    "except Exception as e:\n",
    "    raise SystemExit(\n",
    "        f\"[Error] Could not open Chroma collection '{COLLECTION_NAME}' at {PERSIST_DIR}.\\n\"\n",
    "        \"Run the previous RAG lab to build it, then re-run this notebook.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa052b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the SAME embedder used for documents, so query embeddings live in the same space.\n",
    "embedder = SentenceTransformer(EMBED_MODEL)\n",
    "print(\"Embedder:\", EMBED_MODEL)\n",
    "\n",
    "# Cross-encoder reranker: computes relevance for (query, passage) pairs. Higher = better.\n",
    "reranker = CrossEncoder(RERANK_MODEL)\n",
    "print(\"Reranker:\", RERANK_MODEL)\n",
    "\n",
    "# Lightweight HF text2text model for grounded answering (fast on CPU relative to larger LLMs).\n",
    "# You can swap to a chat model later if you have more compute.\n",
    "gen = pipeline(\"text2text-generation\", model=LLM_MODEL)\n",
    "print(\"HF LLM:\", LLM_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2699069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_query(query: str) -> np.ndarray:\n",
    "    '''Return a normalized embedding for the user query (same model as documents).'''\n",
    "    v = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return v[0].astype(\"float32\")\n",
    "\n",
    "def chroma_retrieve(query: str, k2: int) -> List[Dict]:\n",
    "    '''Retrieve top k*2 candidates from Chroma using vector similarity.'''\n",
    "    q_emb = embed_query(query)\n",
    "    res = collection.query(\n",
    "        query_embeddings=[q_emb],\n",
    "        n_results=k2,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\", \"ids\"]\n",
    "    )\n",
    "    # Flatten results into a list of dicts for easy handling\n",
    "    items = []\n",
    "    for cid, doc, meta, dist in zip(res[\"ids\"][0], res[\"documents\"][0], res[\"metadatas\"][0], res[\"distances\"][0]):\n",
    "        items.append({\n",
    "            \"id\": cid,\n",
    "            \"text\": doc,\n",
    "            \"source\": meta.get(\"source\"),\n",
    "            \"page\": meta.get(\"page\"),\n",
    "            \"method\": meta.get(\"method\"),\n",
    "            \"distance\": float(dist)\n",
    "        })\n",
    "    return items\n",
    "\n",
    "def rerank(query: str, candidates: List[Dict], k: int) -> List[Dict]:\n",
    "    '''Use a cross-encoder to rerank and return the top-k passages for the final prompt.'''\n",
    "    pairs = [(query, c[\"text\"]) for c in candidates]\n",
    "    scores = reranker.predict(pairs)  # vector of relevance scores\n",
    "    for c, s in zip(candidates, scores):\n",
    "        c[\"score\"] = float(s)\n",
    "    # Sort by score descending and keep top-k\n",
    "    return sorted(candidates, key=lambda x: x[\"score\"], reverse=True)[:k]\n",
    "\n",
    "def build_context(passages: List[Dict]) -> str:\n",
    "    '''Build a compact context block with source/page for grounding.'''\n",
    "    lines = []\n",
    "    for p in passages:\n",
    "        tag = f\"[{p.get('source')} p.{p.get('page')} | {p.get('method')}] \"\n",
    "        text = p[\"text\"].strip().replace(\"\\n\", \" \")\n",
    "        lines.append(tag + text)\n",
    "    return \"\\n- \" + \"\\n- \".join(lines) if lines else \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f5780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SYS = \"You are a concise TA. Answer ONLY using the provided context. If insufficient, say you don't know. Cite filename and page when possible.\"\n",
    "\n",
    "def answer_with_llm(query: str, passages: List[Dict], max_new_tokens: int = 256) -> str:\n",
    "    '''Create a grounded prompt and generate an answer with a small HF model (FLAN-T5).'''\n",
    "    context = build_context(passages)\n",
    "    # FLAN-T5 is seq2seq; a plain instruction-style prompt works well\n",
    "    prompt = (\n",
    "        f\"{SYS}\\n\\n\"\n",
    "        f\"Question: {query}\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "    out = gen(prompt, max_new_tokens=max_new_tokens, num_beams=4, do_sample=False)\n",
    "    return out[0][\"generated_text\"].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fbf98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple UI: text area for the query and slider for k\n",
    "q_box   = w.Textarea(value=\"\", placeholder=\"Type your question here…\", description=\"Query:\", layout=w.Layout(width=\"100%\", height=\"100px\"))\n",
    "k_slider= w.IntSlider(value=K_DEFAULT, min=2, max=10, step=1, description=\"k\")\n",
    "run_btn = w.Button(description=\"Retrieve → Rerank → Answer\", button_style=\"primary\")\n",
    "out     = w.Output()\n",
    "\n",
    "def on_click(_):\n",
    "    out.clear_output(wait=True)\n",
    "    query = q_box.value.strip()\n",
    "    k = int(k_slider.value)\n",
    "    if not query:\n",
    "        with out: print(\"Please type a query.\")\n",
    "        return\n",
    "    with out:\n",
    "        print(\"Retrieving from Chroma…\")\n",
    "        cands = chroma_retrieve(query, k2=k*2)\n",
    "        if not cands:\n",
    "            print(\"No results. Make sure your Chroma collection is populated (run the first lab).\")\n",
    "            return\n",
    "        print(f\"Retrieved {len(cands)} candidates. Reranking…\")\n",
    "        topk = rerank(query, cands, k=k)\n",
    "        # Tiny preview table of the top-k reranked passages\n",
    "        df = pd.DataFrame([{\n",
    "            \"rank\": i+1,\n",
    "            \"score\": round(p[\"score\"], 4),\n",
    "            \"source\": p[\"source\"],\n",
    "            \"page\": p[\"page\"],\n",
    "            \"snippet\": (p[\"text\"][:220] + \"…\") if len(p[\"text\"]) > 220 else p[\"text\"]\n",
    "        } for i, p in enumerate(topk)])\n",
    "        display(df)\n",
    "        print(\"\\nGenerating grounded answer…\")\n",
    "        ans = answer_with_llm(query, topk, max_new_tokens=256)\n",
    "        print(\"\\n=== Answer ===\\n\", ans)\n",
    "\n",
    "run_btn.on_click(on_click)\n",
    "w.VBox([q_box, k_slider, run_btn, out])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab471b58",
   "metadata": {},
   "source": [
    "\n",
    "### Tips & next steps\n",
    "- **Why rerankers help:** bi-encoders (embeddings) are fast but approximate; cross-encoders read the *pair* (query, passage) and usually re-order the top candidates more accurately.\n",
    "- Try other rerankers: `cross-encoder/ms-marco-MiniLM-L-12-v2`, `BAAI/bge-reranker-base` (bigger = slower but better).\n",
    "- You can swap the LLM (e.g., `TinyLlama/TinyLlama-1.1B-Chat-v1.0`) via `pipeline(\"text-generation\", ...)` if you prefer chat-style models.\n",
    "- Keep the **same embedder** for documents and queries. Only the reranker changes.\n",
    "- Experiment: different `k`, prompt templates, or add citation formatting to your final answer.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

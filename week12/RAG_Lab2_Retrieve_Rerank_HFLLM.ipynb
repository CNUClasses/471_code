{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540331d5",
   "metadata": {},
   "source": [
    "\n",
    "# RAG Lab 2: Query → Retrieve (Chroma) → Rerank → HF LLM Answer\n",
    "\n",
    "**Updated:** 2025-11-08\n",
    "\n",
    "In this short lab you will:\n",
    "1. Enter a natural-language **query** via a widget\n",
    "2. **Embed** the query using the **same model** as your document embeddings\n",
    "3. Retrieve **top k×2** candidates from an **existing ChromaDB** collection\n",
    "4. **Rerank** those candidates with a **cross-encoder** reranker\n",
    "5. Keep the **top k** passages and build a grounded prompt\n",
    "6. Run **Hugging Face LLM** inference and display the **answer**\n",
    "\n",
    "> Goal: illustrate a lean RAG pipeline with reranking. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced5dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.13 | Platform: Linux-5.14.0-570.55.1.el9_6.x86_64-x86_64-with-glibc2.34\n",
      "chromadb: 1.3.2\n",
      "sentence_transformers: 5.1.2\n",
      "transformers: 4.57.1\n",
      "ipywidgets: 8.1.8\n"
     ]
    }
   ],
   "source": [
    "# ✅ Install minimal dependencies. If these are already installed, this cell is a no-op.\n",
    "# !pip install langchain langchain-community pypdf chromadb sentence-transformers transformers tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e53a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Imports & configuration ----\n",
    "import os, numpy as np, pandas as pd\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as w\n",
    "\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import pipeline\n",
    "\n",
    "from pprint import PrettyPrinter\n",
    "# Create a PrettyPrinter with custom indentation\n",
    "pp = PrettyPrinter(indent=4)\n",
    "\n",
    "# === Paths & model names (match prior lab defaults) ===\n",
    "PERSIST_DIR    = \"./rag_chroma\"                      # where the prior lab persisted vectors\n",
    "COLLECTION_NAME= \"cnu_rag_lab\"                       # collection name used previously\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/msmarco-distilbert-cos-v5\"\n",
    "RERANK_MODEL   = \"cross-encoder/ms-marco-electra-base\"    # compact cross-encoder\n",
    "LLM_MODEL     = \"google/flan-t5-small\"               # compact text-generation model\n",
    "K_DEFAULT      = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f6d8f3",
   "metadata": {},
   "source": [
    "## 1) Load embedder, reranker and a text generation model to give answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aa052b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedder: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "Reranker: cross-encoder/ms-marco-electra-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2ed2690b8c4c03899d085655ecf116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9ea6b5c2b04f02a10dd9b11c18b621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05de81d73a0e4549a46ff8c77074627a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64fe1a8d3974f17b0a6edd7c094478e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7a045f81cf45f5989eb1a62731967d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8128c1a90f433e9c876a16c889e371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5277bdeaeb5a44b7bfa33444ca87f5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF LLM: google/flan-t5-small\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the SAME embedder used for documents, so query embeddings live in the same space.\n",
    "embedder = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "print(\"Embedder:\", EMBED_MODEL_NAME)\n",
    "\n",
    "# Cross-encoder reranker: computes relevance for (query, passage) pairs. Higher = better.\n",
    "reranker = CrossEncoder(RERANK_MODEL)\n",
    "print(\"Reranker:\", RERANK_MODEL)\n",
    "\n",
    "# Lightweight HF text2text model for grounded answering (fast on CPU relative to larger LLMs).\n",
    "# You can swap to a chat model later if you have more compute.\n",
    "gen = pipeline(\"text2text-generation\", model=LLM_MODEL)\n",
    "print(\"HF LLM:\", LLM_MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3e2195",
   "metadata": {},
   "source": [
    "## 2) Load the chroma database generated in Lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa7a5567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to collection 'cnu_rag_lab' with 1239 vectors at ./rag_chroma\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# Connect to existing Chroma collection\n",
    "client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "try: \n",
    "    collection = client.get_collection(name=COLLECTION_NAME)\n",
    "    print(f\"Connected to collection '{COLLECTION_NAME}' with {collection.count()} vectors at {PERSIST_DIR}\")\n",
    "except Exception as e:\n",
    "    raise SystemExit(\n",
    "        f\"[Error] Could not open Chroma collection '{COLLECTION_NAME}' at {PERSIST_DIR}.\\n\"\n",
    "        \"Run the previous RAG lab to build it, then re-run this notebook.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d76b1f",
   "metadata": {},
   "source": [
    "\n",
    "## Typical RAG Retrieval Flow (two‑stage)\n",
    "\n",
    "```\n",
    "Query  ──► Bi‑encoder vector ──► ANN index (top‑k docs)\n",
    "                               └─► k candidates\n",
    "Query + each candidate doc ──► Cross‑encoder (re‑ranker) ──► final ordered list\n",
    "Top‑m chunks ──► Prompt context for LLM\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5eb63",
   "metadata": {},
   "source": [
    "\n",
    "## Why is a re‑ranker more accurate than a bi‑encoder? (Core intuition)\n",
    "\n",
    "| Model | Input/Scoring | Strengths | Weaknesses |\n",
    "|---|---|---|---|\n",
    "| **Bi‑encoder** | Encodes **query** and **doc** *independently* into vectors; score = cosine/dot | Very **fast**, **scalable** (precompute doc vectors; index with FAISS/Chroma/Pinecone) | **No token‑level interaction** between query & doc; can miss subtle meaning (negation, entities, context) |\n",
    "| **Cross‑encoder** (Re‑ranker) | Reads **query + doc together** (e.g., `[CLS] query [SEP] doc`); predicts **relevance** | **Deep token‑level attention**; **context‑sensitive** scoring; higher **accuracy** | **Slower** (must score each pair), no doc precomputation |\n",
    "\n",
    "**Example:**\n",
    "\n",
    "**Query:** “Documents not about CNNs”\n",
    "\n",
    "**Doc:** “This paper discusses convolutional networks”\n",
    "\n",
    "- Bi-encoder: high similarity (misses “not”)\n",
    "\n",
    "- Re-ranker: low relevance (understands negation)\n",
    "\n",
    "\n",
    "**Reason the re‑ranker wins:** It attends across tokens of query **and** document jointly, so it can model negation, long‑distance dependencies, and nuanced phrasing that a single fixed vector (bi‑encoder output) cannot fully capture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d8c724",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Retrieval: Top‑k Nearest Chunks\n",
    "\n",
    "We'll embed the user query with the **same model** used to create the chroma database in the last lab, then query ChromaDB for the nearest chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2699069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_query(q: str) -> np.ndarray:\n",
    "    '''Embed a query string using the same SentenceTransformer model (normalized).'''\n",
    "    # For asymmetric semantic search, you are recommended to use SentenceTransformer.encode_query to encode your queries\n",
    "    v = embedder.encode_query([q], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return v[0].astype(\"float32\")\n",
    "\n",
    "def chroma_retrieve(query: str, k2: int) -> List[Dict]:\n",
    "    '''Retrieve top k*2 candidates from Chroma using vector similarity.'''\n",
    "    # q_emb = embed_query(query).tolist()\n",
    "    q_emb = embed_query(query)\n",
    "\n",
    "    res = collection.query(\n",
    "        query_embeddings=[q_emb],\n",
    "        n_results=k2,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]  #returns string chunks, plus info about each page in the pdf and chunk number, and finaly the cosign distance    )\n",
    "    )\n",
    "    \n",
    "    # Flatten results into a list of dicts for easy handling\n",
    "    items = []\n",
    "    for cid, doc, meta, dist in zip(res[\"ids\"][0], res[\"documents\"][0], res[\"metadatas\"][0], res[\"distances\"][0]):\n",
    "        items.append({\n",
    "            \"id\": cid,\n",
    "            \"text\": doc,\n",
    "            \"source\": meta.get(\"source\"),\n",
    "            \"page\": meta.get(\"page\"),\n",
    "            \"method\": meta.get(\"method\"),\n",
    "            \"distance\": float(dist)\n",
    "        })\n",
    "    return items\n",
    "\n",
    "def rerank(query: str, candidates: List[Dict], k: int) -> List[Dict]:\n",
    "    '''Use a cross-encoder to rerank and return the top-k passages for the final prompt.'''\n",
    "    pairs = [(query, c[\"text\"]) for c in candidates]\n",
    "    scores = reranker.predict(pairs)  # vector of relevance scores (higher is better)\n",
    "\n",
    "    # Attach scores to candidates\n",
    "    for c, s in zip(candidates, scores):\n",
    "        c[\"score\"] = float(s)\n",
    "        \n",
    "    # Sort by score descending and keep top-k\n",
    "    return sorted(candidates, key=lambda x: x[\"score\"], reverse=True)[:k]\n",
    "\n",
    "def build_context(passages: List[Dict]) -> str:\n",
    "    '''Build a compact context block with source/page for grounding.'''\n",
    "    lines = []\n",
    "    for p in passages:\n",
    "        tag = f\"[{p.get('source')} p.{p.get('page')} | {p.get('method')}] \"\n",
    "        text = p[\"text\"].strip().replace(\"\\n\", \" \")\n",
    "        lines.append(tag + text)\n",
    "    return \"\\n- \" + \"\\n- \".join(lines) if lines else \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7557cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same function from week11/embeddings_cosine_similarity_mini_lab.ipynb\n",
    "def build_prompt(passages: List[Dict], question: str) -> dict:\n",
    "\n",
    "    # context = \" \".join(context)  #a list of docs is provided\n",
    "    context=build_context(passages)\n",
    "\n",
    "    system_msg = (\"\"\"You are a helpful assistant. Answer the user's question **using only** the provided Data.\n",
    "    If the answer isn't in the context, say you don't know.\n",
    "    Instructions:\n",
    "    - Ground your answer in the context.\n",
    "    - If the answer is not in the context, say \"I don't know based on the provided context.\"\n",
    "    \"\"\")\n",
    "\n",
    "    user_msg = f\"Context:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": user_msg}\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a314e5",
   "metadata": {},
   "source": [
    "## Pass prompt to LLM for answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69e8c23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82bc82f0247f45768ca0066a83a13b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# model_name=\"text-generation\", model=\"distilgpt2\"   # not a chat model\n",
    "# model_name = \"deepset/bert-large-uncased-whole-word-masking-squad2\"\n",
    "# model_name=\"knowledgator/Qwen-encoder-1.5B\"\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "llm = pipeline(\"text-generation\", model=model_name,tokenizer=model_name) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "92f5780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SYS = \"You are a concise TA. Answer ONLY using the provided context. If insufficient, say you don't know. Cite filename and page when possible.\"\n",
    "\n",
    "# def answer_with_llm(query: str, passages: List[Dict], max_new_tokens: int = 256, verbose: bool = True) -> str:\n",
    "#     '''Create a grounded prompt and generate an answer with a small HF model (FLAN-T5).'''\n",
    "#     context = build_context(passages)\n",
    "#     # FLAN-T5 is seq2seq; a plain instruction-style prompt works well\n",
    "#     prompt = (\n",
    "#         f\"{SYS}\\n\\n\"\n",
    "#         f\"Question: {query}\\n\"\n",
    "#         f\"Context:\\n{context}\\n\\n\"\n",
    "#         f\"Answer:\"\n",
    "#     )\n",
    "\n",
    "#     if(verbose):print(prompt)\n",
    "#     out = gen(prompt, max_new_tokens=max_new_tokens, num_beams=4, do_sample=False)\n",
    "#     return out[0][\"generated_text\"].strip()\n",
    "def answer_with_llm(query: str, passages: List[Dict], max_new_tokens: int = 256, verbose: bool = True) -> str:\n",
    "    prompt=build_prompt(passages, query)\n",
    "    if(verbose):\n",
    "        pp.pprint(prompt)\n",
    "    return llm(prompt, max_new_tokens=1000)[0]['generated_text'][2]['content']\n",
    "\n",
    "\n",
    "\n",
    "#     if(verbose):print(prompt)\n",
    "#     out = gen(prompt, max_new_tokens=max_new_tokens, num_beams=4, do_sample=False)\n",
    "#     return out[0][\"generated_text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4ff0c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query=\"what are the pre reqs for CPSC 475\"\n",
    "# chroma_retrieve(query, k2=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fbf98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bfa55466f24710a0ea1febd398c7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='', description='Query:', layout=Layout(height='100px', width='100%'), placehold…"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Simple UI: text area for the query and slider for k\n",
    "q_box   = w.Textarea(value=\"\", placeholder=\"Type Query Here\", description=\"Query:\", layout=w.Layout(width=\"100%\", height=\"100px\"))\n",
    "k_slider= w.IntSlider(value=K_DEFAULT, min=2, max=10, step=1, description=\"k\")\n",
    "run_btn = w.Button(description=\"Retrieve → Rerank → Answer\", button_style=\"primary\")\n",
    "out     = w.Output()\n",
    "\n",
    "def on_click(_):\n",
    "    out.clear_output(wait=True)\n",
    "    query = q_box.value.strip()\n",
    "    k = int(k_slider.value)\n",
    "    if not query:\n",
    "        with out: print(\"Please type a query.\")\n",
    "        return\n",
    "    with out:\n",
    "        print(f'Query length={len(query)}')\n",
    "        print(\"Retrieving from Chroma…\")\n",
    "        cands = chroma_retrieve(query, k2=k*10)\n",
    "        if not cands:\n",
    "            print(\"No results. Make sure your Chroma collection is populated (run the first lab).\")\n",
    "            return\n",
    "        print(f\"Retrieved {len(cands)} candidates. Reranking…\")\n",
    "        topk = rerank(query, cands, k=k)\n",
    "        # Tiny preview table of the top-k reranked passages\n",
    "        df = pd.DataFrame([{\n",
    "            \"rank\": i+1,\n",
    "            \"score\": round(p[\"score\"], 4),\n",
    "            \"source\": p[\"source\"],\n",
    "            \"page\": p[\"page\"],\n",
    "            \"snippet\": (p[\"text\"][:220] + \"…\") if len(p[\"text\"]) > 220 else p[\"text\"]\n",
    "        } for i, p in enumerate(topk)])\n",
    "        display(df)\n",
    "        print(\"---\")\n",
    "        ans = answer_with_llm(query, topk, max_new_tokens=1000, verbose=False)\n",
    "        print(\"---\")\n",
    "        pp.pprint( ans)\n",
    "\n",
    "run_btn.on_click(on_click)\n",
    "w.VBox([q_box, k_slider, run_btn, out])\n",
    "\n",
    "#what are the pre reqs for CPSC 475"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab471b58",
   "metadata": {},
   "source": [
    "\n",
    "### Tips & next steps\n",
    "- **Why rerankers help:** bi-encoders (embeddings) are fast but approximate; cross-encoders read the *pair* (query, passage) and usually re-order the top candidates more accurately.\n",
    "- Try other rerankers: `cross-encoder/ms-marco-MiniLM-L-12-v2`, `BAAI/bge-reranker-base` (bigger = slower but better).\n",
    "- You can swap the LLM (e.g., `TinyLlama/TinyLlama-1.1B-Chat-v1.0`) via `pipeline(\"text-generation\", ...)` if you prefer chat-style models.\n",
    "- Keep the **same embedder** for documents and queries. Only the reranker changes.\n",
    "- Experiment: different `k`, prompt templates, or add citation formatting to your final answer.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

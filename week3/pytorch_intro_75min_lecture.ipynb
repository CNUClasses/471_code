{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "826121d3",
   "metadata": {},
   "source": [
    "\n",
    "# PyTorch Introduction \n",
    "\n",
    "**Pre Reqs:** basic Python/NumPy  \n",
    "**Goal:** Leave ready to read PyTorch code, build/train simple models, and debug common issues.\n",
    "\n",
    "**What you'll learn**\n",
    "1. Tensors & vectorization (device, dtype, shapes, broadcasting)\n",
    "2. Autograd: building and differentiating computation graphs\n",
    "3. `nn.Module`, losses, and optimizers\n",
    "4. Input pipelines with `Dataset` / `DataLoader`\n",
    "5. Canonical training & evaluation loops (+ checkpoints)\n",
    "6. Mini project (MNIST)\n",
    "7. Performance tips: `torch.compile`, mixed precision\n",
    "8. Common gotchas and debugging patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca3875",
   "metadata": {},
   "source": [
    "## 1) Framing & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7465e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.5\n",
      "PyTorch: 2.2.2+cu121\n",
      "Device in use: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Environment & reproducibility\n",
    "import sys, math, time, random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "\n",
    "#the following determines if we use the GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device in use:\", device)\n",
    "\n",
    "#for reproducibility, set all random seeds to a fixed value\n",
    "torch.manual_seed(123)\n",
    "random.seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c56369",
   "metadata": {},
   "source": [
    "### <mark>Why Pytorch instead of Numpy?\n",
    "\n",
    "<mark>It can use a GPU as well as a CPU<br>\n",
    "It can compute gradients automatically (autograd)<br>\n",
    "It has a lot of useful functions for deep learning (e.g. layers, loss fns, optimizers, etc.)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd06386a",
   "metadata": {},
   "source": [
    "## 2) Pytorch â€” Where are things?\n",
    "\n",
    "![](./pytorch_whereisit.png)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55761dc7",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Tensors, Vectorization, Broadcasting\n",
    "\n",
    "![](./tensors.png)<br><br>\n",
    "\n",
    "Key ideas: **device**, **dtype**, **shapes**, **broadcasting**, and avoiding Python loops.<br>\n",
    "<mark>See pytorch_broadcasting.ipynb for further broadcasting details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfa6bb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "shapes: x.shape=torch.Size([3, 4]), w.shape=torch.Size([4, 2]), y.shape=torch.Size([3, 2]), b.shape=torch.Size([2])\n",
      "tensor([[ 3.1669, -2.8220],\n",
      "        [ 8.9699, -3.8728]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create tensors directly on the chosen device\n",
    "x = torch.arange(12, dtype=torch.float32, device=device).view(3, 4)  #could also use .reshape(3,4)\n",
    "w = torch.randn(4, 2, device=device)\n",
    "b = torch.zeros(2, device=device)\n",
    "print(b.shape)\n",
    "# stopped here 9/8\n",
    "\n",
    "y = x @ w + b  # broadcast bias\n",
    "print(f\"shapes: x.shape={x.shape}, w.shape={w.shape}, y.shape={y.shape}, b.shape={b.shape}\")\n",
    "print(y[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc850768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([10, 20, 30])\n",
      "tensor([[11, 22, 33],\n",
      "        [14, 25, 36]])\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting Demo \n",
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Shape: (2, 3)\n",
    "b = torch.tensor([10, 20, 30])           # Shape: (3,)\n",
    "print(a)\n",
    "print(b)\n",
    "# Compare shapes, starting from the last dimension\n",
    "#  - Dimension 1: 3 and 3 are equal. Compatible.\n",
    "#  - Dimension 0: 2 and (implicitly) 1. Compatible.\n",
    "# Essentially, b is treated as if it were [[10, 20, 30], [10, 20, 30]]\n",
    "# The resulting shape is (max(2,1), max(3,3)) -> (2, 3).\n",
    "c = a + b\n",
    "\n",
    "print(c)\n",
    "# Output:\n",
    "# tensor([[11, 22, 33],\n",
    "#         [14, 25, 36]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e069c7",
   "metadata": {},
   "source": [
    "\n",
    "**Notes**\n",
    "- Prefer constructing on the right device (`device=device`) or use `.to(device)`.\n",
    "- `view` vs `reshape`: `view` requires contiguous memory; `reshape` is safer BUT may copy.\n",
    "- <mark>Use pytorch vectorized operations; avoid explicit Python loops for math on tensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cecd07e",
   "metadata": {},
   "source": [
    "## 4) Autograd (automatic differentiation)\n",
    "\n",
    "<mark>Just to show it works, pytorch handles backpropagation automatically for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd113c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: tensor([7.])\n",
      "y.requires_grad: True\n",
      "x.grad: tensor([0.])\n",
      "x.grad after second backward: tensor([5.])\n",
      "x2.requires_grad: False\n",
      "z.requires_grad: False\n",
      "no_grad result: 15.0\n"
     ]
    }
   ],
   "source": [
    "# A scalar function and its gradient\n",
    "x = torch.tensor([2.0], requires_grad=True, device=device)  #need totrack x gradient\n",
    "y = x**2 + 3*x + 1            # dy/dx = 2x + 3 = 7 when x=2\n",
    "y.backward()\n",
    "print(\"x.grad:\", x.grad)\n",
    "print(\"y.requires_grad:\", y.requires_grad)\n",
    "\n",
    "# Grads accumulate: zero them if reusing tensors (you are responsible for this)\n",
    "x.grad.zero_()\n",
    "print(\"x.grad:\", x.grad)\n",
    "y2 = (x * 5 + 1)\n",
    "y2.backward()\n",
    "print(\"x.grad after second backward:\", x.grad)\n",
    "\n",
    "# Detach and no_grad\n",
    "x2 = (x.detach() * 10)        # breaks graph\n",
    "print(\"x2.requires_grad:\", x2.requires_grad)\n",
    "y2 = (x2 * 5 + 1)\n",
    "# y2.backward()            # throws exception\n",
    "\n",
    "#when doing inference or when validating, we don't need (or want) gradients.(per parameter float the model has to track)\n",
    "with torch.no_grad():\n",
    "    z = x * 7 + 1               # won't track gradients\n",
    "    print(\"z.requires_grad:\", z.requires_grad)\n",
    "    # z.backward()                # throws exception\n",
    "print(\"no_grad result:\", z.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803bf0c0",
   "metadata": {},
   "source": [
    "\n",
    "**Pitfalls**\n",
    "- <mark>Gradients **accumulate**; be sure to zero them between steps.\n",
    "- Wrap evaluation in `torch.no_grad()` to save memory/compute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9aee1",
   "metadata": {},
   "source": [
    "## 5) Input Pipelines: `Dataset` / `DataLoader`\n",
    "\n",
    "Pytorch has 2 classes to manage data.<br><br>\n",
    "**Dataset class:** A wrapper that enables iterating over a dataset. It has 2 methods __len__, to get the total number of items in a dataset, and __getitem__, which gets an item at a particular index<br><br>\n",
    "**DataLoader class:** Wraps an Iterable around a DataSet.  Generates batches of data from the Dataset at a time.  Can be configured to draw batches sequentially or randomly sample from dataset (keeps model from memorizing training order<br><br>\n",
    "\n",
    "Pytorch has many built in DataSets types so often you can use them instead of rolling your own.  We will see some of them throughout the course.<br>\n",
    "For a more in depth look see <a href=\"https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html\">Datasets & DataLoaders</a><br>\n",
    "\n",
    "Below is an example of a custom Dataset and DataLoader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d66e7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_y_equals_x1_times_x2(n,d, x_range=3.0):\n",
    "    x = torch.empty(n, 2).uniform_(-x_range, x_range)    \n",
    "    y = x[:, 0] * x[:, 1]\n",
    "    return x, y \n",
    "\n",
    "# A tiny synthetic dataset\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, n=1024, d=2, x_range=3.0):\n",
    "        super().__init__() # Optional, but good practice\n",
    "        self.x,self.y = make_y_equals_x1_times_x2(n,d,x_range)\n",
    "\n",
    "    def __len__(self): return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx): return self.x[idx], self.y[idx]\n",
    "\n",
    "train_ds = ToyDataset(2048, d=2)\n",
    "test_ds  = ToyDataset(512, d=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc63713",
   "metadata": {},
   "source": [
    "### Or just use the built in TensorDataset and not bother with writing your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62b10d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "x,y = make_y_equals_x1_times_x2(2048,2,3)\n",
    "train_ds = TensorDataset(x,y)                   #can make this a oneliner TensorDataset(*make_y_equals_x1_times_x2(n,d,x_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1d4ed5",
   "metadata": {},
   "source": [
    "### Now wrap the DataSet with a DataLoader and your in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f746bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes train: torch.Size([64, 2]) torch.Size([64]) | dtypes: torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)    #randomly select for each batch\n",
    "test_loader  = DataLoader(test_ds,  batch_size=256, shuffle=False)  #sequentially select for each batch\n",
    "\n",
    "xb, yb = next(iter(train_loader))   # a single batch\n",
    "print(\"Batch shapes train:\", xb.shape, yb.shape, \"| dtypes:\", xb.dtype, yb.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eac5b7",
   "metadata": {},
   "source": [
    "## 6) `nn.Module`, Losses, Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83dc5d7",
   "metadata": {},
   "source": [
    "### see mlp_x1x2_regression.ipynb for simple MLP predicting a number (like our Value class MLP)\n",
    "### see mlp_classification.ipynb for simple classification on MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df28110f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

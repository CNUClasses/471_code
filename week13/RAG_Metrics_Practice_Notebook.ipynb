{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f432bb67",
   "metadata": {},
   "source": [
    "\n",
    "# Practice Notebook: RAG evaluatinn metrics \n",
    "**Audience:** 4th-year CS students  \n",
    "**Focus Metrics:** Precision@K, Recall@K, Mean Average Precision (MAP), Mean Reciprocal Rank (MRR)\n",
    "\n",
    "## What you'll do\n",
    "1. Review what each metric measures and **what it’s used for** in RAG evaluation.  \n",
    "2. Work through **guided practice problems** that use synthetic ranked results and ground-truth relevance labels.  \n",
    "3. Use helper functions to **compute** P@K, R@K, AP, MAP, and MRR — then check your work with a **solutions toggle**.\n",
    "\n",
    "> **Context:** In Retrieval-Augmented Generation (RAG), the retriever's quality strongly determines how well the generator can answer. These metrics help you quantify how good your retriever is at bringing useful context to the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d9259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Toggle solutions here.\n",
    "# Set to True to reveal solution cells, or False to hide them.\n",
    "SHOW_SOLUTIONS = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04130971",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Metrics Overview — What they measure and why we use them\n",
    "\n",
    "**Precision@K**  \n",
    "- **What it measures:** The **fraction of the top-K retrieved documents that are actually relevant**.  \n",
    "- **Why it matters:** In RAG, **top-of-list quality** matters because only a small number of docs go into the LLM context. High Precision@K means you minimize noise presented to the generator.\n",
    "\n",
    "**Recall@K**  \n",
    "- **What it measures:** The **fraction of all relevant documents that appear within the top-K**.  \n",
    "- **Why it matters:** High recall ensures you **don’t miss** critical evidence the generator might need. This is especially important for knowledge-dense queries with multiple supporting docs.\n",
    "\n",
    "**Average Precision (AP)**  \n",
    "- **What it measures:** Aggregates **precision at the ranks where relevant documents appear** for a single query.  \n",
    "- **Why it matters:** Rewards methods that **rank relevant documents early and often** — a smooth measure that considers the whole ranking.\n",
    "\n",
    "**Mean Average Precision (MAP)**  \n",
    "- **What it measures:** The **mean of AP** across many queries.  \n",
    "- **Why it matters:** A **global measure** of ranking quality over a dataset — useful when queries have **multiple relevant** documents.\n",
    "\n",
    "**Reciprocal Rank (RR)** and **Mean Reciprocal Rank (MRR)**  \n",
    "- **What they measure:** **RR = 1 / (rank of the first relevant doc)** for a query; **MRR** is the average of RR across queries.  \n",
    "- **Why it matters:** Ideal when you **only need one good document quickly** (e.g., QA or chat) — it emphasizes how early the **first** relevant doc appears.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf32dc2",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Helper Functions (use these for the problems)\n",
    "These pure-Python functions implement the IR metrics. They accept:\n",
    "- `ranked_doc_ids`: a list of **doc IDs in ranked order** (best to worst) for a query\n",
    "- `relevant_doc_ids`: a **set** of relevant doc IDs for that query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae0f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Set, Dict\n",
    "import numpy as np\n",
    "\n",
    "def precision_at_k(ranked_doc_ids: List[int], relevant_doc_ids: Set[int], k: int) -> float:\n",
    "    \"\"\"Precision@K: fraction of top-K that is relevant.\"\"\"\n",
    "    top_k = ranked_doc_ids[:k]\n",
    "    hits = sum(1 for d in top_k if d in relevant_doc_ids)\n",
    "    return hits / float(k)\n",
    "\n",
    "def recall_at_k(ranked_doc_ids: List[int], relevant_doc_ids: Set[int], k: int) -> float:\n",
    "    \"\"\"Recall@K: fraction of all relevant docs that appear in top-K.\"\"\"\n",
    "    if len(relevant_doc_ids) == 0:\n",
    "        return 0.0\n",
    "    top_k = ranked_doc_ids[:k]\n",
    "    hits = sum(1 for d in top_k if d in relevant_doc_ids)\n",
    "    return hits / float(len(relevant_doc_ids))\n",
    "\n",
    "def average_precision(ranked_doc_ids: List[int], relevant_doc_ids: Set[int]) -> float:\n",
    "    \"\"\"Average Precision (AP) for a single query.\n",
    "    AP = mean of Precision@k over ranks k where the item at rank k is relevant.\n",
    "    If no relevant docs are retrieved, AP = 0.\n",
    "    \"\"\"\n",
    "    if len(relevant_doc_ids) == 0:\n",
    "        return 0.0\n",
    "    precisions = []\n",
    "    hits = 0\n",
    "    for i, doc_id in enumerate(ranked_doc_ids, start=1):  # ranks start at 1\n",
    "        if doc_id in relevant_doc_ids:\n",
    "            hits += 1\n",
    "            precisions.append(hits / i)  # precision at rank i\n",
    "    return float(np.mean(precisions)) if precisions else 0.0\n",
    "\n",
    "def reciprocal_rank(ranked_doc_ids: List[int], relevant_doc_ids: Set[int]) -> float:\n",
    "    \"\"\"Reciprocal Rank (RR): 1 / (rank of the first relevant doc), or 0 if none.\"\"\"\n",
    "    for i, doc_id in enumerate(ranked_doc_ids, start=1):\n",
    "        if doc_id in relevant_doc_ids:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def evaluate_query(ranked: List[int], rel: Set[int], ks=(5,10)) -> Dict[str, float]:\n",
    "    \"\"\"Convenience function to compute P@K, R@K, AP, RR for a single query.\"\"\"\n",
    "    out = {}\n",
    "    for k in ks:\n",
    "        out[f'P@{k}'] = precision_at_k(ranked, rel, k)\n",
    "        out[f'R@{k}'] = recall_at_k(ranked, rel, k)\n",
    "    out['AP'] = average_precision(ranked, rel)\n",
    "    out['RR'] = reciprocal_rank(ranked, rel)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba26e8b",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Practice A — Single Query (Basics)\n",
    "**Given:** a ranked list of doc IDs for a query, and the set of relevant doc IDs.  \n",
    "**Compute:** Precision@5, Precision@10, Recall@5, Recall@10.\n",
    "\n",
    "> Tip: Start by counting how many relevant docs appear in the top-K, then divide by K (for precision) or by the number of relevant docs (for recall).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4724c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Problem 3.1 ---\n",
    "# Ranked results for Query Q1 (best -> worst)\n",
    "ranked_Q1 = [3, 9, 12, 7, 2, 5, 10, 4, 8, 1, 6, 11]\n",
    "# Ground-truth relevant docs for Q1\n",
    "relevant_Q1 = {2, 5, 8, 14}  # note: some IDs may not appear in the top results\n",
    "\n",
    "# TODO: Compute P@5, P@10, R@5, R@10 (by hand or using helper functions)\n",
    "# Replace the '...' with code. Use evaluate_query if you want a shortcut.\n",
    "# Example: precision_at_k(ranked_Q1, relevant_Q1, 5)\n",
    "\n",
    "P5_Q1  = precision_at_k(ranked_Q1, relevant_Q1, 5)   # ...\n",
    "P10_Q1 = precision_at_k(ranked_Q1, relevant_Q1, 10)  # ...\n",
    "R5_Q1  = recall_at_k(ranked_Q1, relevant_Q1, 5)      # ...\n",
    "R10_Q1 = recall_at_k(ranked_Q1, relevant_Q1, 10)     # ...\n",
    "\n",
    "print({ \"P@5\": P5_Q1, \"P@10\": P10_Q1, \"R@5\": R5_Q1, \"R@10\": R10_Q1 })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1cee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Solution 3.1 (revealed if SHOW_SOLUTIONS=True) ---\n",
    "if SHOW_SOLUTIONS:\n",
    "    sol = evaluate_query([3,9,12,7,2,5,10,4,8,1,6,11], {2,5,8,14})\n",
    "    print(\"Q1 Solution:\", sol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc99d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Problem 3.2 ---\n",
    "ranked_Q2 = [1, 13, 6, 4, 8, 3, 10, 2, 5, 12, 9, 7]\n",
    "relevant_Q2 = {4, 6, 9}\n",
    "\n",
    "# TODO: Compute P@5, P@10, R@5, R@10\n",
    "P5_Q2  = precision_at_k(ranked_Q2, relevant_Q2, 5)\n",
    "P10_Q2 = precision_at_k(ranked_Q2, relevant_Q2, 10)\n",
    "R5_Q2  = recall_at_k(ranked_Q2, relevant_Q2, 5)\n",
    "R10_Q2 = recall_at_k(ranked_Q2, relevant_Q2, 10)\n",
    "\n",
    "print({ \"P@5\": P5_Q2, \"P@10\": P10_Q2, \"R@5\": R5_Q2, \"R@10\": R10_Q2 })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105e5477",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Solution 3.2 ---\n",
    "if SHOW_SOLUTIONS:\n",
    "    sol = evaluate_query([1,13,6,4,8,3,10,2,5,12,9,7], {4,6,9})\n",
    "    print(\"Q2 Solution:\", sol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b583d",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Practice B — Average Precision (AP) and Reciprocal Rank (RR)\n",
    "**Given:** ranked list + relevant set.  \n",
    "**Compute:** the **AP** (single query) and **RR** (single query).\n",
    "\n",
    "- **AP**: For each rank *k* where the item is relevant, compute Precision@k. AP is the **mean** of those precisions.  \n",
    "- **RR**: 1 divided by the **rank of the first relevant** item.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6589ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Problem 4.1 (AP & RR) ---\n",
    "ranked_Q3 = [20, 2, 5, 17, 9, 11, 3, 7, 8, 6, 1, 4]\n",
    "relevant_Q3 = {1, 3, 8, 11}\n",
    "\n",
    "# TODO: Compute AP and RR for Q3\n",
    "AP_Q3 = average_precision(ranked_Q3, relevant_Q3)\n",
    "RR_Q3 = reciprocal_rank(ranked_Q3, relevant_Q3)\n",
    "\n",
    "print({ \"AP\": AP_Q3, \"RR\": RR_Q3 })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec70b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Solution 4.1 ---\n",
    "if SHOW_SOLUTIONS:\n",
    "    sol = evaluate_query([20,2,5,17,9,11,3,7,8,6,1,4], {1,3,8,11})\n",
    "    print(\"Q3 Solution:\", sol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc372b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Problem 4.2 (AP & RR) ---\n",
    "ranked_Q4 = [9, 5, 2, 4, 1, 3, 6, 7, 8]\n",
    "relevant_Q4 = {2}\n",
    "\n",
    "# TODO: Compute AP and RR for Q4\n",
    "AP_Q4 = average_precision(ranked_Q4, relevant_Q4)\n",
    "RR_Q4 = reciprocal_rank(ranked_Q4, relevant_Q4)\n",
    "\n",
    "print({ \"AP\": AP_Q4, \"RR\": RR_Q4 })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Solution 4.2 ---\n",
    "if SHOW_SOLUTIONS:\n",
    "    sol = evaluate_query([9,5,2,4,1,3,6,7,8], {2})\n",
    "    print(\"Q4 Solution:\", sol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3121d30",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Practice C — MAP & MRR over Multiple Queries\n",
    "Below is a small **synthetic dataset** of **four queries**. For each query you get:\n",
    "- `ranked`: list of doc IDs in descending relevance order\n",
    "- `relevant`: a set of ground-truth relevant doc IDs\n",
    "\n",
    "**Tasks:**\n",
    "1. Compute P@5, P@10, R@5, R@10, AP, and RR **per query**.  \n",
    "2. Compute **MAP** (mean of AP across all queries) and **MRR** (mean of RR across all queries).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Synthetic dataset: 4 queries ---\n",
    "toy_data = [\n",
    "    {\n",
    "        \"qid\": \"Q5\",\n",
    "        \"ranked\":   [3, 1, 6, 8, 2, 5, 9, 4, 7, 10],\n",
    "        \"relevant\": {1, 2, 5}\n",
    "    },\n",
    "    {\n",
    "        \"qid\": \"Q6\",\n",
    "        \"ranked\":   [11, 4, 9, 2, 1, 7, 3, 6, 5, 8],\n",
    "        \"relevant\": {2, 11}\n",
    "    },\n",
    "    {\n",
    "        \"qid\": \"Q7\",\n",
    "        \"ranked\":   [14, 12, 15, 1, 5, 2, 3, 6, 8, 9],\n",
    "        \"relevant\": {5, 6, 9, 14}\n",
    "    },\n",
    "    {\n",
    "        \"qid\": \"Q8\",\n",
    "        \"ranked\":   [21, 19, 17, 16, 18, 20, 22, 23, 24, 25],\n",
    "        \"relevant\": {30}  # intentionally missing (no relevant retrieved)\n",
    "    },\n",
    "]\n",
    "\n",
    "# --- YOUR WORK: compute per-query metrics and then MAP/MRR ---\n",
    "per_query = []\n",
    "for item in toy_data:\n",
    "    ranked = item[\"ranked\"]\n",
    "    rel = item[\"relevant\"]\n",
    "    result = evaluate_query(ranked, rel, ks=(5,10))\n",
    "    per_query.append({ \"qid\": item[\"qid\"], **result })\n",
    "\n",
    "import pandas as pd\n",
    "df_results = pd.DataFrame(per_query)\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b633c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Aggregate MAP and MRR over the toy set ---\n",
    "MAP = df_results[\"AP\"].mean()\n",
    "MRR = df_results[\"RR\"].mean()\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\"MAP\", \"MRR\", \"Precision@5 (mean)\", \"Precision@10 (mean)\", \"Recall@5 (mean)\", \"Recall@10 (mean)\"],\n",
    "    \"Score\":  [MAP, MRR, df_results[\"P@5\"].mean(), df_results[\"P@10\"].mean(), df_results[\"R@5\"].mean(), df_results[\"R@10\"].mean()]\n",
    "})\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd2e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Solution discussion for Section 5 (shown only if SHOW_SOLUTIONS=True) ---\n",
    "if SHOW_SOLUTIONS:\n",
    "    from pprint import pprint\n",
    "    print(\"Per-query metrics:\")\n",
    "    pprint(per_query)\n",
    "    print(\"\\nMAP, MRR, and means:\")\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b1b5c6",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Concept Checks (Short Answer)\n",
    "\n",
    "1. **When would you prefer Recall@10 over Precision@5 in RAG?**  \n",
    "   *Hint:* Think about tasks where missing evidence is costly.\n",
    "\n",
    "2. **Why might MRR be a better indicator than MAP for a chat assistant that inserts only a single snippet into the prompt?**  \n",
    "   *Hint:* Consider what the generator needs to answer a single-turn question.\n",
    "\n",
    "3. **How can increasing K change your evaluation story even if MAP stays similar?**  \n",
    "   *Hint:* Consider precision vs. recall trade-offs and context-window limits.\n",
    "\n",
    "4. **If a retriever has high Recall@10 but low Precision@5, what concrete change might you try?**  \n",
    "   *Hint:* Consider rerankers or hybrid search to improve ranking at the top.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea42cc",
   "metadata": {},
   "source": [
    "\n",
    "## 7) (Optional) Visual Aid — Where do the first relevant documents appear?\n",
    "This simple plot shows the **1-indexed rank of the first relevant document** for each query in our toy set (ignoring queries with none).  \n",
    "Lower is better — it reflects the behavior MRR cares about.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4776b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "first_ranks = []\n",
    "for item in toy_data:\n",
    "    ranked = item[\"ranked\"]\n",
    "    rel = item[\"relevant\"]\n",
    "    # Find first relevant rank\n",
    "    pos = None\n",
    "    for i, d in enumerate(ranked, start=1):\n",
    "        if d in rel:\n",
    "            pos = i\n",
    "            break\n",
    "    if pos is not None:\n",
    "        first_ranks.append(pos)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(range(len(first_ranks)), first_ranks)\n",
    "plt.title(\"Rank of First Relevant Document (lower is better)\")\n",
    "plt.xlabel(\"Toy Query Index\")\n",
    "plt.ylabel(\"First Relevant Rank\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

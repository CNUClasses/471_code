{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f432bb67",
   "metadata": {},
   "source": [
    "\n",
    "# Practice Notebook: RAG evaluatinn metrics \n",
    "**Focus Metrics:** Precision@K, Recall@K, Mean Average Precision (MAP), Mean Reciprocal Rank (MRR)\n",
    "\n",
    "## What you'll do\n",
    "1. Review what each metric measures and **what it’s used for** in RAG evaluation.  \n",
    "2. Work through **guided practice problems** that use synthetic ranked results and ground-truth relevance labels.  \n",
    "3. Use helper functions to **compute** P@K, R@K, AP, MAP, and MRR — then check your work with a **solutions toggle**.\n",
    "\n",
    "> **Context:** In Retrieval-Augmented Generation (RAG), the retriever's quality strongly determines how well the generator can answer. These metrics help you quantify how good your retriever is at bringing useful context to the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76d9259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Toggle solutions here.\n",
    "# Set to True to reveal solution cells, or False to hide them.\n",
    "SHOW_SOLUTIONS = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04130971",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Metrics Overview — What they measure and why we use them\n",
    "\n",
    "**Precision@K**  \n",
    "- **What it measures:** The **fraction of the top-K retrieved documents that are actually relevant**.  \n",
    "- **Why it matters:** In RAG, **top-of-list quality** matters because only a small number of docs go into the LLM context. High Precision@K means you minimize noise presented to the generator.\n",
    "\n",
    "**Recall@K**  \n",
    "- **What it measures:** The **fraction of all relevant documents that appear within the top-K**.  \n",
    "- **Why it matters:** High recall ensures you **don’t miss** critical evidence the generator might need. This is especially important for knowledge-dense queries with multiple supporting docs.\n",
    "\n",
    "**Average Precision (AP)**  \n",
    "- **What it measures:** Aggregates **precision at the ranks where relevant documents appear** for a single query.  \n",
    "- **Why it matters:** Rewards methods that **rank relevant documents early and often** — a smooth measure that considers the whole ranking.\n",
    "\n",
    "**Mean Average Precision (MAP)**  \n",
    "- **What it measures:** The **mean of AP** across many queries.  \n",
    "- **Why it matters:** A **global measure** of ranking quality over a dataset — useful when queries have **multiple relevant** documents.\n",
    "\n",
    "**Reciprocal Rank (RR)** and **Mean Reciprocal Rank (MRR)**  \n",
    "- **What they measure:** **RR = 1 / (rank of the first relevant doc)** for a query; **MRR** is the average of RR across queries.  \n",
    "- **Why it matters:** Ideal when you **only need one good document quickly** (e.g., QA or chat) — it emphasizes how early the **first** relevant doc appears.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf32dc2",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Helper Functions (use these for the problems)\n",
    "These pure-Python functions implement the IR metrics. They accept:\n",
    "- `ranked_doc_ids`: a list of **doc IDs in ranked order** (best to worst) for a query\n",
    "- `relevant_doc_ids`: a **set** of relevant doc IDs for that query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae0f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Set, Dict\n",
    "import numpy as np\n",
    "\n",
    "def precision_at_k(ranked_doc_ids: List[int], relevant_doc_ids: Set[int], k: int) -> float:\n",
    "    \"\"\"Precision@K: fraction of top-K that is relevant.\"\"\"\n",
    "    top_k = ranked_doc_ids[:k]\n",
    "    hits = sum(1 for d in top_k if d in relevant_doc_ids)\n",
    "    return hits / float(k)\n",
    "\n",
    "def recall_at_k(ranked_doc_ids: List[int], relevant_doc_ids: Set[int], k: int) -> float:\n",
    "    \"\"\"Recall@K: fraction of all relevant docs that appear in top-K.\"\"\"\n",
    "    if len(relevant_doc_ids) == 0:\n",
    "        return 0.0\n",
    "    top_k = ranked_doc_ids[:k]\n",
    "    hits = sum(1 for d in top_k if d in relevant_doc_ids)\n",
    "    return hits / float(len(relevant_doc_ids))\n",
    "\n",
    "def average_precision(ranked_doc_ids: List[int], relevant_doc_ids: Set[int]) -> float:\n",
    "    \"\"\"Average Precision (AP) for a single query.\n",
    "    AP = mean of Precision@k over ranks k where the item at rank k is relevant.\n",
    "    If no relevant docs are retrieved, AP = 0.\n",
    "    \"\"\"\n",
    "    if len(relevant_doc_ids) == 0:\n",
    "        return 0.0\n",
    "    precisions = []\n",
    "    hits = 0\n",
    "    for i, doc_id in enumerate(ranked_doc_ids, start=1):  # ranks start at 1\n",
    "        if doc_id in relevant_doc_ids:\n",
    "            hits += 1\n",
    "            precisions.append(hits / i)  # precision at rank i\n",
    "    return float(np.mean(precisions)) if precisions else 0.0\n",
    "\n",
    "def reciprocal_rank(ranked_doc_ids: List[int], relevant_doc_ids: Set[int]) -> float:\n",
    "    \"\"\"Reciprocal Rank (RR): 1 / (rank of the first relevant doc), or 0 if none.\"\"\"\n",
    "    for i, doc_id in enumerate(ranked_doc_ids, start=1):\n",
    "        if doc_id in relevant_doc_ids:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def evaluate_query(ranked: List[int], rel: Set[int], ks=(5,10)) -> Dict[str, float]:\n",
    "    \"\"\"Convenience function to compute P@K, R@K, AP, RR for a single query.\"\"\"\n",
    "    out = {}\n",
    "    for k in ks:\n",
    "        out[f'P@{k}'] = precision_at_k(ranked, rel, k)\n",
    "        out[f'R@{k}'] = recall_at_k(ranked, rel, k)\n",
    "    out['AP'] = average_precision(ranked, rel)\n",
    "    out['RR'] = reciprocal_rank(ranked, rel)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba26e8b",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Practice A — Single Query (Basics)\n",
    "**Given:** a ranked list of doc IDs for a query, and the set of relevant doc IDs.  \n",
    "**Compute:** Precision@5, Precision@10, Recall@5, Recall@10.\n",
    "\n",
    "> Tip: Start by counting how many relevant docs appear in the top-K, then divide by K (for precision) or by the number of relevant docs (for recall).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4724c45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'P@5': 0.2, 'P@10': 0.3, 'R@5': 0.25, 'R@10': 0.75}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Problem 3.1 ---\n",
    "# Ranked results for Query Q1 (best -> worst)\n",
    "ranked_Q1 = [3, 9, 12, 7, 2, 5, 10, 4, 8, 1, 6, 11]\n",
    "# Ground-truth relevant docs for Q1\n",
    "relevant_Q1 = {2, 5, 8, 14}  # note: some IDs may not appear in the top results\n",
    "\n",
    "# TODO: Compute P@5, P@10, R@5, R@10 (by hand or using helper functions)\n",
    "# Replace the '...' with code. Use evaluate_query if you want a shortcut.\n",
    "# Example: precision_at_k(ranked_Q1, relevant_Q1, 5)\n",
    "\n",
    "P5_Q1  = precision_at_k(ranked_Q1, relevant_Q1, 5)   # ...\n",
    "P10_Q1 = precision_at_k(ranked_Q1, relevant_Q1, 10)  # ...\n",
    "R5_Q1  = recall_at_k(ranked_Q1, relevant_Q1, 5)      # ...\n",
    "R10_Q1 = recall_at_k(ranked_Q1, relevant_Q1, 10)     # ...\n",
    "\n",
    "print({ \"P@5\": P5_Q1, \"P@10\": P10_Q1, \"R@5\": R5_Q1, \"R@10\": R10_Q1 })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc1cee87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 Solution: {'P@5': 0.2, 'R@5': 0.25, 'P@10': 0.3, 'R@10': 0.75, 'AP': 0.2888888888888889, 'RR': 0.2}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Solution 3.1 (revealed if SHOW_SOLUTIONS=True) ---\n",
    "if SHOW_SOLUTIONS:\n",
    "    sol = evaluate_query([3,9,12,7,2,5,10,4,8,1,6,11], {2,5,8,14})\n",
    "    print(\"Q1 Solution:\", sol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b583d",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Practice B — Average Precision (AP) and Reciprocal Rank (RR)\n",
    "**Given:** ranked list + relevant set.  \n",
    "**Compute:** the **AP** (single query) and **RR** (single query).\n",
    "\n",
    "- **AP**: For each rank *k* where the item is relevant, compute Precision@k. AP is the **mean** of those precisions.  \n",
    "- **RR**: 1 divided by the **rank of the first relevant** item.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff6589ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AP': 0.28733766233766234, 'RR': 0.16666666666666666}\n"
     ]
    }
   ],
   "source": [
    "# --- Problem 4.1 (AP & RR) ---\n",
    "ranked_Q3 = [20, 2, 5, 17, 9, 11, 3, 7, 8, 6, 1, 4]\n",
    "relevant_Q3 = {1, 3, 8, 11}\n",
    "\n",
    "# TODO: Compute AP and RR for Q3\n",
    "AP_Q3 = average_precision(ranked_Q3, relevant_Q3)\n",
    "RR_Q3 = reciprocal_rank(ranked_Q3, relevant_Q3)\n",
    "\n",
    "print({ \"AP\": AP_Q3, \"RR\": RR_Q3 })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ec70b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 Solution: {'P@5': 0.0, 'R@5': 0.0, 'P@10': 0.3, 'R@10': 0.75, 'AP': 0.28733766233766234, 'RR': 0.16666666666666666}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Solution 4.1 ---\n",
    "if SHOW_SOLUTIONS:\n",
    "    sol = evaluate_query([20,2,5,17,9,11,3,7,8,6,1,4], {1,3,8,11})\n",
    "    print(\"Q3 Solution:\", sol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fc372b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AP': 0.3333333333333333, 'RR': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "# --- Problem 4.2 (AP & RR) ---\n",
    "ranked_Q4 = [9, 5, 2, 4, 1, 3, 6, 7, 8]\n",
    "relevant_Q4 = {2}\n",
    "\n",
    "# TODO: Compute AP and RR for Q4\n",
    "AP_Q4 = average_precision(ranked_Q4, relevant_Q4)\n",
    "RR_Q4 = reciprocal_rank(ranked_Q4, relevant_Q4)\n",
    "\n",
    "print({ \"AP\": AP_Q4, \"RR\": RR_Q4 })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Solution 4.2 ---\n",
    "if SHOW_SOLUTIONS:\n",
    "    sol = evaluate_query([9,5,2,4,1,3,6,7,8], {2})\n",
    "    print(\"Q4 Solution:\", sol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3121d30",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Practice C — MAP & MRR over Multiple Queries\n",
    "Below is a small **synthetic dataset** of **four queries**. For each query you get:\n",
    "- `ranked`: list of doc IDs in descending relevance order\n",
    "- `relevant`: a set of ground-truth relevant doc IDs\n",
    "\n",
    "**Tasks:**\n",
    "1. Compute P@5, P@10, R@5, R@10, AP, and RR **per query**.  \n",
    "2. Compute **MAP** (mean of AP across all queries) and **MRR** (mean of RR across all queries).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e231c877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>P@5</th>\n",
       "      <th>R@5</th>\n",
       "      <th>P@10</th>\n",
       "      <th>R@10</th>\n",
       "      <th>AP</th>\n",
       "      <th>RR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.543750</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid  P@5       R@5  P@10  R@10        AP   RR\n",
       "0  Q5  0.4  0.666667   0.3   1.0  0.466667  0.5\n",
       "1  Q6  0.4  1.000000   0.2   1.0  0.750000  1.0\n",
       "2  Q7  0.4  0.500000   0.4   1.0  0.543750  1.0\n",
       "3  Q8  0.0  0.000000   0.0   0.0  0.000000  0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Synthetic dataset: 4 queries ---\n",
    "toy_data = [\n",
    "    {\n",
    "        \"qid\": \"Q5\",\n",
    "        \"ranked\":   [3, 1, 6, 8, 2, 5, 9, 4, 7, 10],\n",
    "        \"relevant\": {1, 2, 5}\n",
    "    },\n",
    "    {\n",
    "        \"qid\": \"Q6\",\n",
    "        \"ranked\":   [11, 4, 9, 2, 1, 7, 3, 6, 5, 8],\n",
    "        \"relevant\": {2, 11}\n",
    "    },\n",
    "    {\n",
    "        \"qid\": \"Q7\",\n",
    "        \"ranked\":   [14, 12, 15, 1, 5, 2, 3, 6, 8, 9],\n",
    "        \"relevant\": {5, 6, 9, 14}\n",
    "    },\n",
    "    {\n",
    "        \"qid\": \"Q8\",\n",
    "        \"ranked\":   [21, 19, 17, 16, 18, 20, 22, 23, 24, 25],\n",
    "        \"relevant\": {30}  # intentionally missing (no relevant retrieved)\n",
    "    },\n",
    "]\n",
    "\n",
    "# --- YOUR WORK: compute per-query metrics and then MAP/MRR ---\n",
    "per_query = []\n",
    "for item in toy_data:\n",
    "    ranked = item[\"ranked\"]\n",
    "    rel = item[\"relevant\"]\n",
    "    result = evaluate_query(ranked, rel, ks=(5,10))\n",
    "    per_query.append({ \"qid\": item[\"qid\"], **result })\n",
    "\n",
    "import pandas as pd\n",
    "df_results = pd.DataFrame(per_query)\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42b633c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAP</td>\n",
       "      <td>0.440104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MRR</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision@5 (mean)</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Precision@10 (mean)</td>\n",
       "      <td>0.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Recall@5 (mean)</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Recall@10 (mean)</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Metric     Score\n",
       "0                  MAP  0.440104\n",
       "1                  MRR  0.625000\n",
       "2   Precision@5 (mean)  0.300000\n",
       "3  Precision@10 (mean)  0.225000\n",
       "4      Recall@5 (mean)  0.541667\n",
       "5     Recall@10 (mean)  0.750000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Aggregate MAP and MRR over the toy set ---\n",
    "MAP = df_results[\"AP\"].mean()\n",
    "MRR = df_results[\"RR\"].mean()\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\"MAP\", \"MRR\", \"Precision@5 (mean)\", \"Precision@10 (mean)\", \"Recall@5 (mean)\", \"Recall@10 (mean)\"],\n",
    "    \"Score\":  [MAP, MRR, df_results[\"P@5\"].mean(), df_results[\"P@10\"].mean(), df_results[\"R@5\"].mean(), df_results[\"R@10\"].mean()]\n",
    "})\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdd2e39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-query metrics:\n",
      "[{'AP': 0.4666666666666666,\n",
      "  'P@10': 0.3,\n",
      "  'P@5': 0.4,\n",
      "  'R@10': 1.0,\n",
      "  'R@5': 0.6666666666666666,\n",
      "  'RR': 0.5,\n",
      "  'qid': 'Q5'},\n",
      " {'AP': 0.75,\n",
      "  'P@10': 0.2,\n",
      "  'P@5': 0.4,\n",
      "  'R@10': 1.0,\n",
      "  'R@5': 1.0,\n",
      "  'RR': 1.0,\n",
      "  'qid': 'Q6'},\n",
      " {'AP': 0.54375,\n",
      "  'P@10': 0.4,\n",
      "  'P@5': 0.4,\n",
      "  'R@10': 1.0,\n",
      "  'R@5': 0.5,\n",
      "  'RR': 1.0,\n",
      "  'qid': 'Q7'},\n",
      " {'AP': 0.0,\n",
      "  'P@10': 0.0,\n",
      "  'P@5': 0.0,\n",
      "  'R@10': 0.0,\n",
      "  'R@5': 0.0,\n",
      "  'RR': 0.0,\n",
      "  'qid': 'Q8'}]\n",
      "\n",
      "MAP, MRR, and means:\n",
      "                Metric     Score\n",
      "0                  MAP  0.440104\n",
      "1                  MRR  0.625000\n",
      "2   Precision@5 (mean)  0.300000\n",
      "3  Precision@10 (mean)  0.225000\n",
      "4      Recall@5 (mean)  0.541667\n",
      "5     Recall@10 (mean)  0.750000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Solution discussion for Section 5 (shown only if SHOW_SOLUTIONS=True) ---\n",
    "if SHOW_SOLUTIONS:\n",
    "    from pprint import pprint\n",
    "    print(\"Per-query metrics:\")\n",
    "    pprint(per_query)\n",
    "    print(\"\\nMAP, MRR, and means:\")\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b1b5c6",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Concept Checks (Short Answer)\n",
    "\n",
    "1. **When would you prefer Recall@10 over Precision@5 in RAG?**  \n",
    "   *Hint:* Think about tasks where missing evidence is costly.\n",
    "\n",
    "3. **How can increasing K change your evaluation story even if MAP stays similar?**  \n",
    "   *Hint:* Consider precision vs. recall trade-offs and context-window limits.\n",
    "\n",
    "4. **If a retriever has high Recall@10 but low Precision@5, what concrete change might you try?**  \n",
    "   *Hint:* Consider rerankers or hybrid search to improve ranking at the top.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3aec40",
   "metadata": {},
   "source": [
    "### 1. When would you prefer Recall@10 over Precision@5 in RAG?\n",
    "You would prefer **Recall@10** when **missing important evidence is more harmful than retrieving some irrelevant chunks**—for example, long-form QA, research tasks, legal/medical reasoning, or any workflow where the system must capture *all* supporting information.\n",
    "\n",
    "\n",
    "### 3. How can increasing K change your evaluation story even if MAP stays similar?\n",
    "Increasing **K** increases **recall** (you locate the relevant chunk more often) but decreases **precision** and may overload the model’s context window. MAP might remain steady, but practical performance can worsen because the LLM sees more noise, even though the metric looks unchanged.\n",
    "\n",
    "\n",
    "\n",
    "### 4. If a retriever has high Recall@10 but low Precision@5, what concrete change might you try?\n",
    "Add a **reranker** (e.g., a cross-encoder) to improve the **ordering at the top** of the list. This keeps recall high while boosting Precision@5 by moving the relevant chunk higher in the ranking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a0f2fb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

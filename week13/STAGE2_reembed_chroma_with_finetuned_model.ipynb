{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44bb3ac8",
   "metadata": {},
   "source": [
    "# Re-Embedding Chunks in ChromaDB with a Fine-Tuned SentenceTransformer\n",
    "<img src=\"https://raw.githubusercontent.com/CNUClasses/CPSC471/master/content/lectures/week13/stage2.png\" alt=\"standard\" style=\"max-height:300px;  margin:10px 0; vertical-align:middle;\">\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "\n",
    "1. **Load a fine-tuned embedding model** from a local directory:\n",
    "   - `/models/sentence-transformers/msmarco-distilbert-cos-v5`\n",
    "2. **Connect to an existing ChromaDB persistent database**:\n",
    "   - `PERSIST_DIR = \"./rag_chroma\"`\n",
    "   - Existing collection: `cnu_rag_lab`\n",
    "3. **Re-embed all documents (chunks) from the existing collection** using the fine-tuned model.\n",
    "4. **Create a new ChromaDB collection** with updated embeddings:\n",
    "   - New collection name: `cnu_rag_mnrl_lab`\n",
    "\n",
    "This is useful when you have improved your embedding model (e.g., via fine-tuning with MNRL)\n",
    "and want your retrieval index (Chroma) to use the new embeddings without changing the\n",
    "underlying text or metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eab59ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# (Optional) Install Dependencies\n",
    "# ============================================================================\n",
    "# Uncomment and run this cell if you do NOT already have these libraries\n",
    "# installed in your Python environment.\n",
    "#\n",
    "# - chromadb: for the vector database\n",
    "# - sentence-transformers: for the embedding model\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# !pip install -U chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3e32481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Imports and Configuration\n",
    "# ============================================================================\n",
    "# This section imports all required libraries and defines configuration\n",
    "# variables such as model path, Chroma persistence directory, and collection\n",
    "# names.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# import os\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'  # Specify which GPUs to use\n",
    "from typing import List, Dict\n",
    "\n",
    "import utils as ut\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Path to the fine-tuned SentenceTransformer model.\n",
    "# This should be a directory containing the saved model files.\n",
    "FINETUNED_MODEL_PATH = \"./models/sentence-transformers/msmarco-distilbert-cos-v5\"\n",
    "\n",
    "# ChromaDB persistence directory (where the DB is stored on disk)\n",
    "PERSIST_DIR = \"../week12/rag_chroma\"\n",
    "\n",
    "# Name of the existing collection with old embeddings\n",
    "SOURCE_COLLECTION_NAME = \"cnu_rag_lab\"\n",
    "\n",
    "# Name of the new collection where we will store re-embedded chunks\n",
    "TARGET_COLLECTION_NAME = \"cnu_rag_mnrl_lab\"\n",
    "\n",
    "# Device configuration: use GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20f8d9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from: ./models/sentence-transformers/msmarco-distilbert-cos-v5\n",
      "Max sequence length: 384\n",
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Load the Fine-Tuned Embedding Model\n",
    "# ============================================================================\n",
    "# We now load the fine-tuned SentenceTransformer model from the specified\n",
    "# local directory. This model will be used to compute new embeddings for\n",
    "# all documents in the ChromaDB collection.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Load the model from the local path\n",
    "model = SentenceTransformer(FINETUNED_MODEL_PATH)\n",
    "model.to(device)\n",
    "\n",
    "# Quick sanity check: print out the model's max sequence length and embedding dimension\n",
    "print(\"Model loaded from:\", FINETUNED_MODEL_PATH)\n",
    "print(\"Max sequence length:\", model.get_max_seq_length())\n",
    "print(\"Embedding dimension:\", model.get_sentence_embedding_dimension())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aed0e000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to ChromaDB with persist_directory = ../week12/rag_chroma\n",
      "Loaded source collection: cnu_rag_lab\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Connect to ChromaDB and Access the Source Collection\n",
    "# ============================================================================\n",
    "# We connect to the persistent ChromaDB instance stored in PERSIST_DIR.\n",
    "# Then we open the existing collection that contains the original embeddings\n",
    "# and chunks.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Create a ChromaDB client that uses the persistent directory\n",
    "client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "collection = client.get_collection(SOURCE_COLLECTION_NAME)\n",
    "\n",
    "print(\"Connected to ChromaDB with persist_directory =\", PERSIST_DIR)\n",
    "\n",
    "# Get the source collection which already exists and contains original embeddings\n",
    "source_collection = client.get_collection(SOURCE_COLLECTION_NAME)\n",
    "print(\"Loaded source collection:\", source_collection.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c0e21c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of items in source collection: 1256\n",
      "Fetched 1000 items (offset now 1000)\n",
      "Fetched 256 items (offset now 1256)\n",
      "Total fetched IDs: 1256\n",
      "Total fetched documents: 1256\n",
      "Total fetched metadatas: 1256\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Fetch All Documents (Chunks) from the Source Collection\n",
    "# ============================================================================\n",
    "# ChromaDB returns results in batches via `.get()`, so we may need to paginate\n",
    "# through the collection if it is large. Here, we:\n",
    "#\n",
    "#   1. Determine how many items are in the collection.\n",
    "#   2. Fetch them in chunks (batches) using limit/offset.\n",
    "#   3. Aggregate all 'ids', 'documents', and 'metadatas' in memory.\n",
    "#\n",
    "# We purposely ignore the existing embeddings, as we plan to recompute them.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# First, get a small batch to estimate total number of items in the collection\n",
    "sample = source_collection.get(limit=1)\n",
    "total_count = sample.get(\"count\", None)\n",
    "\n",
    "# Some versions of Chroma may not return 'count'; if so, we can approximate by\n",
    "# asking for a large batch and checking how many we get.\n",
    "if total_count is None:\n",
    "    sample_large = source_collection.get(limit=1000000)\n",
    "    total_count = len(sample_large[\"ids\"])\n",
    "\n",
    "print(\"Total number of items in source collection:\", total_count)\n",
    "\n",
    "# Define batch size for pagination\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "all_ids = []\n",
    "all_documents = []\n",
    "all_metadatas = []\n",
    "\n",
    "offset = 0\n",
    "\n",
    "while offset < total_count:\n",
    "    # Fetch a batch of documents from the collection\n",
    "    batch = source_collection.get(\n",
    "        limit=BATCH_SIZE,\n",
    "        offset=offset,\n",
    "        include=[\"documents\", \"metadatas\"],\n",
    "    )\n",
    "    \n",
    "    ids = batch[\"ids\"]\n",
    "    docs = batch[\"documents\"]\n",
    "    metas = batch[\"metadatas\"]\n",
    "    \n",
    "    all_ids.extend(ids)\n",
    "    all_documents.extend(docs)\n",
    "    all_metadatas.extend(metas)\n",
    "    \n",
    "    offset += len(ids)\n",
    "    print(f\"Fetched {len(ids)} items (offset now {offset})\")\n",
    "\n",
    "print(\"Total fetched IDs:\", len(all_ids))\n",
    "print(\"Total fetched documents:\", len(all_documents))\n",
    "print(\"Total fetched metadatas:\", len(all_metadatas))\n",
    "\n",
    "# Quick sanity check\n",
    "if len(all_ids) != total_count:\n",
    "    print(\"Warning: fetched item count does not match expected total_count.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b0dc59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================================\n",
    "# # Helper Function: Encode Documents in Batches\n",
    "# # ============================================================================\n",
    "# # We define a small helper function that takes a list of texts (documents) and\n",
    "# # uses the SentenceTransformer model to compute embeddings in batches. This\n",
    "# # avoids running out of memory when there are many documents.\n",
    "# # ----------------------------------------------------------------------------\n",
    "\n",
    "# from math import ceil\n",
    "\n",
    "# def encode_documents(\n",
    "#     texts: List[str],\n",
    "#     model: SentenceTransformer,\n",
    "#     batch_size: int = 64,\n",
    "# ) -> List[List[float]]:\n",
    "#     \"\"\"Encode a list of documents into embeddings using the given model.\n",
    "\n",
    "#     This function:\n",
    "#       - Splits the input texts into batches.\n",
    "#       - Uses `model.encode` with `convert_to_numpy=True` and `normalize_embeddings=True`\n",
    "#         for cosine similarity–friendly vectors.\n",
    "#       - Converts the resulting numpy arrays into Python lists (for ChromaDB).\n",
    "\n",
    "#     Args:\n",
    "#         texts: List of string documents to encode.\n",
    "#         model: A SentenceTransformer model.\n",
    "#         batch_size: Number of documents to encode per batch.\n",
    "\n",
    "#     Returns:\n",
    "#         A list of embeddings, where each embedding is a list[float].\n",
    "#     \"\"\"\n",
    "#     all_embeddings: List[List[float]] = []\n",
    "#     num_texts = len(texts)\n",
    "#     num_batches = ceil(num_texts / batch_size)\n",
    "\n",
    "#     for i in range(num_batches):\n",
    "#         start = i * batch_size\n",
    "#         end = min(start + batch_size, num_texts)\n",
    "#         batch_texts = texts[start:end]\n",
    "\n",
    "#         # Compute embeddings for this batch\n",
    "#         # - convert_to_numpy=True returns a numpy array\n",
    "#         # - normalize_embeddings=True L2-normalizes the embeddings\n",
    "#         batch_embeddings = model.encode(\n",
    "#             batch_texts,\n",
    "#             batch_size=batch_size,\n",
    "#             convert_to_numpy=True,\n",
    "#             normalize_embeddings=True,\n",
    "#             show_progress_bar=False,\n",
    "#         )\n",
    "\n",
    "#         # Convert each embedding vector to a plain Python list\n",
    "#         for emb in batch_embeddings:\n",
    "#             all_embeddings.append(emb.tolist())\n",
    "\n",
    "#         # Optional logging for long runs\n",
    "#         if (i + 1) % 10 == 0 or (i + 1) == num_batches:\n",
    "#             print(f\"Encoded batch {i + 1}/{num_batches} (docs {start}–{end})\")\n",
    "\n",
    "#     return all_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18a30232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting re-embedding of all documents...\n",
      "Encoded batch 10/20 (docs 576–640)\n",
      "Encoded batch 20/20 (docs 1216–1256)\n",
      "Re-embedding complete.\n",
      "Number of embeddings: 1256\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Encode All Documents with the Fine-Tuned Model\n",
    "# ============================================================================\n",
    "# We now use the helper function `encode_documents` to compute new embeddings\n",
    "# for every document in the source collection using the fine-tuned model.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"Starting re-embedding of all documents...\")\n",
    "\n",
    "new_embeddings = ut.encode_documents(\n",
    "    texts=all_documents,\n",
    "    model=model,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "print(\"Re-embedding complete.\")\n",
    "print(\"Number of embeddings:\", len(new_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "335f0b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target collection 'cnu_rag_mnrl_lab' already exists. Deleting it first...\n",
      "Created target collection: cnu_rag_mnrl_lab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1256 items into target collection: cnu_rag_mnrl_lab\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Create the Target Collection and Insert Re-Embedded Documents\n",
    "# ============================================================================\n",
    "# We create a new Chroma collection named `cnu_rag_mnrl_lab` and insert:\n",
    "#   - the original IDs\n",
    "#   - the original documents (chunks)\n",
    "#   - the original metadatas\n",
    "#   - the **new** embeddings computed by the fine-tuned model\n",
    "#\n",
    "# If a collection with this name already exists, we delete it first to avoid\n",
    "# mixing old data.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# If the target collection already exists, delete it for a clean re-index\n",
    "existing_collections = [c.name for c in client.list_collections()]\n",
    "if TARGET_COLLECTION_NAME in existing_collections:\n",
    "    print(f\"Target collection '{TARGET_COLLECTION_NAME}' already exists. Deleting it first...\")\n",
    "    client.delete_collection(TARGET_COLLECTION_NAME)\n",
    "\n",
    "# Create the new target collection\n",
    "target_collection = client.create_collection(name=TARGET_COLLECTION_NAME)\n",
    "print(\"Created target collection:\", target_collection.name)\n",
    "\n",
    "# Insert the documents, metadatas, and new embeddings into the target collection\n",
    "target_collection.add(\n",
    "    ids=all_ids,\n",
    "    documents=all_documents,\n",
    "    metadatas=all_metadatas,\n",
    "    embeddings=new_embeddings,\n",
    ")\n",
    "\n",
    "print(\"Inserted\", len(all_ids), \"items into target collection:\", TARGET_COLLECTION_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85b38696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Persist Changes to Disk\n",
    "# ============================================================================\n",
    "# For persistent ChromaDB setups, we call `client.persist()` to ensure\n",
    "# that all changes (including the new collection and its embeddings)\n",
    "# are written to disk under PERSIST_DIR.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Persist the current state of the database to disk\n",
    "# client.persist()\n",
    "# print(\"ChromaDB changes persisted to disk at:\", PERSIST_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed30ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

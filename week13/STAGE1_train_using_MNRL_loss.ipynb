{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f65c879a",
   "metadata": {},
   "source": [
    "# RAG2: Fine-Tuning `msmarco-distilbert-cos-v5` with MultipleNegativesRankingLoss\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/CNUClasses/CPSC471/master/content/lectures/week13/stage1.png\" alt=\"standard\" style=\"max-height:400px;  margin:10px 0; vertical-align:middle;\">\n",
    "\n",
    "This notebook demonstrates how to **fine-tune a Hugging Face SentenceTransformer embedding model**\n",
    "for an **asymmetric search / RAG retrieval** task using the **SentenceTransformers Trainer API**.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Load a CSV file `rag_train_dataset.csv` containing training pairs:\n",
    "   - `question` — a user query\n",
    "   - `chunk` — the correct / relevant text passage for that question\n",
    "2. Wrap the CSV as a Hugging Face `datasets.Dataset` object.\n",
    "3. Initialize the base model: `sentence-transformers/msmarco-distilbert-cos-v5`\n",
    "4. Define a **MultipleNegativesRankingLoss (MNRL)** objective, which uses **in-batch negatives**.\n",
    "5. Configure **SentenceTransformerTrainingArguments**.\n",
    "6. Train the model using **SentenceTransformerTrainer**.\n",
    "7. Save the fine-tuned model locally for later use in a RAG pipeline.\n",
    "\n",
    "> Note: This notebook assumes you have `rag_train_dataset.csv` in the working directory\n",
    "> with columns `question` and `chunk`. Adjust column names if your data differs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "326f5dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# (Optional) Install dependencies\n",
    "# ============================================================================\n",
    "# Uncomment and run this cell if you do NOT already have these libraries\n",
    "# installed in your environment.\n",
    "#\n",
    "# - sentence-transformers >= 3.x for SentenceTransformerTrainer API\n",
    "# - datasets for easy data handling\n",
    "# - transformers / torch are pulled in as dependencies\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# !pip install -U sentence-transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a1130b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU(s): 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils_gpu as ug\n",
    "ug.get_free_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ef3bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Imports and Basic Configuration\n",
    "# ============================================================================\n",
    "# This section imports all the Python packages we need and sets up some\n",
    "# basic configuration such as the model name, CSV path, and random seeds.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]='2'\n",
    "\n",
    "import torch\n",
    "# torch.cuda.device_count() \n",
    "# torch.cuda.is_available()\n",
    "# torch.set_default_device('cuda:2')\n",
    "\n",
    "# Force PyTorch to use only CUDA device 2\n",
    "# CUDA_DEVICE = 2\n",
    "# torch.cuda.set_device(CUDA_DEVICE)\n",
    "# assert torch.cuda.current_device() == CUDA_DEVICE\n",
    "# print(f\"Using CUDA device {CUDA_DEVICE}:\", torch.cuda.get_device_name(CUDA_DEVICE))\n",
    "# # Force SentenceTransformers to use the specified CUDA device\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{CUDA_DEVICE}\"\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    "    SentenceTransformerModelCardData,\n",
    ")\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "\n",
    "# Set a random seed for reproducibility (affects sampling / shuffling)\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Path to the local CSV file containing training data\n",
    "# The CSV should contain at least these columns:\n",
    "#   - 'question'\n",
    "#   - 'chunk'\n",
    "CSV_PATH = \"rag_train_dataset.csv\"\n",
    "\n",
    "# Base embedding model for asymmetric search\n",
    "BASE_MODEL_NAME = \"sentence-transformers/msmarco-distilbert-cos-v5\"\n",
    "\n",
    "# Directory where the fine-tuned model will be saved\n",
    "OUTPUT_DIR = f\"models/{BASE_MODEL_NAME}\"\n",
    "\n",
    "# Basic training hyperparameters (you can adjust these)\n",
    "NUM_EPOCHS = 5\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "EVAL_BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "WARMUP_RATIO = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3a79c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26a604f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['chunk_id', 'question', 'answer', 'source', 'metadata', 'chunk'],\n",
      "    num_rows: 607\n",
      "})\n",
      "Column names: ['chunk_id', 'question', 'answer', 'source', 'metadata', 'chunk']\n",
      "Example 0:\n",
      "  question: What is Christopher Newport University known for?\n",
      "  chunk   : 2 \n",
      "      \n",
      "  \n",
      "   \n",
      " \n",
      "  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "    \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "   \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "      \n",
      " \n",
      "    \n",
      "   \n",
      " \n",
      " \n",
      "  \n",
      "   \n",
      "   \n",
      "Christopher Newport University 2025-2026 \n",
      "Welcome to Christopher Newport University \n",
      "At Christopher Newport, your college journey will be \n",
      "anything but ordinary. This is a place where you will be \n",
      "challenged, celebrated and supported from day one. With \n",
      "small classes, caring professors, and a stunning, modern, \n",
      "safe campus that feels like home, you can dream big and \n",
      "make friends for life. \n",
      "Be Part of Something Bold \n",
      "Our students come from across Virginia, the country, \n",
      "and the world, but they all share one thing: a drive to make a \n",
      "difference. CNU students form close bonds with professors, \n",
      "dive into groundbreaking research and make their mark long \n",
      "before graduation. \n",
      "You could: \n",
      "• Create cutting-edge technology alongside NASA \n",
      "engineers \n",
      "• Conduct particle research at the Thomas Jefferson \n",
      "National Accelerator Facility (Jefferson Lab) \n",
      "• Perform with Broadway stars during special work-\n",
      "shops or grace the stages of international events \n",
      "• Make a name for yourself by participating in under -\n",
      "graduate research \n",
      "• Study abroad or join faculty-led trips across the globe \n",
      "If you are looking for opportunities to strengthen your \n",
      "leadership skills, check out our President’s Leadership \n",
      "Program and Honors Program. Both are nationally recog-\n",
      "nized and full of exclusive benefits that help you stand out \n",
      "and maximize your time here. \n",
      "Learn from Professors Who Know Your Name\n",
      "------\n",
      "Example 1:\n",
      "  question: What are some opportunities for students at Christopher Newport University?\n",
      "  chunk   : 2 \n",
      "      \n",
      "  \n",
      "   \n",
      " \n",
      "  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "    \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "   \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "      \n",
      " \n",
      "    \n",
      "   \n",
      " \n",
      " \n",
      "  \n",
      "   \n",
      "   \n",
      "Christopher Newport University 2025-2026 \n",
      "Welcome to Christopher Newport University \n",
      "At Christopher Newport, your college journey will be \n",
      "anything but ordinary. This is a place where you will be \n",
      "challenged, celebrated and supported from day one. With \n",
      "small classes, caring professors, and a stunning, modern, \n",
      "safe campus that feels like home, you can dream big and \n",
      "make friends for life. \n",
      "Be Part of Something Bold \n",
      "Our students come from across Virginia, the country, \n",
      "and the world, but they all share one thing: a drive to make a \n",
      "difference. CNU students form close bonds with professors, \n",
      "dive into groundbreaking research and make their mark long \n",
      "before graduation. \n",
      "You could: \n",
      "• Create cutting-edge technology alongside NASA \n",
      "engineers \n",
      "• Conduct particle research at the Thomas Jefferson \n",
      "National Accelerator Facility (Jefferson Lab) \n",
      "• Perform with Broadway stars during special work-\n",
      "shops or grace the stages of international events \n",
      "• Make a name for yourself by participating in under -\n",
      "graduate research \n",
      "• Study abroad or join faculty-led trips across the globe \n",
      "If you are looking for opportunities to strengthen your \n",
      "leadership skills, check out our President’s Leadership \n",
      "Program and Honors Program. Both are nationally recog-\n",
      "nized and full of exclusive benefits that help you stand out \n",
      "and maximize your time here. \n",
      "Learn from Professors Who Know Your Name\n",
      "------\n",
      "Example 2:\n",
      "  question: What programs does Christopher Newport University offer to help students develop leadership skills?\n",
      "  chunk   : 2 \n",
      "      \n",
      "  \n",
      "   \n",
      " \n",
      "  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "    \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "   \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "      \n",
      " \n",
      "    \n",
      "   \n",
      " \n",
      " \n",
      "  \n",
      "   \n",
      "   \n",
      "Christopher Newport University 2025-2026 \n",
      "Welcome to Christopher Newport University \n",
      "At Christopher Newport, your college journey will be \n",
      "anything but ordinary. This is a place where you will be \n",
      "challenged, celebrated and supported from day one. With \n",
      "small classes, caring professors, and a stunning, modern, \n",
      "safe campus that feels like home, you can dream big and \n",
      "make friends for life. \n",
      "Be Part of Something Bold \n",
      "Our students come from across Virginia, the country, \n",
      "and the world, but they all share one thing: a drive to make a \n",
      "difference. CNU students form close bonds with professors, \n",
      "dive into groundbreaking research and make their mark long \n",
      "before graduation. \n",
      "You could: \n",
      "• Create cutting-edge technology alongside NASA \n",
      "engineers \n",
      "• Conduct particle research at the Thomas Jefferson \n",
      "National Accelerator Facility (Jefferson Lab) \n",
      "• Perform with Broadway stars during special work-\n",
      "shops or grace the stages of international events \n",
      "• Make a name for yourself by participating in under -\n",
      "graduate research \n",
      "• Study abroad or join faculty-led trips across the globe \n",
      "If you are looking for opportunities to strengthen your \n",
      "leadership skills, check out our President’s Leadership \n",
      "Program and Honors Program. Both are nationally recog-\n",
      "nized and full of exclusive benefits that help you stand out \n",
      "and maximize your time here. \n",
      "Learn from Professors Who Know Your Name\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Load the Training Data from CSV\n",
    "# ============================================================================\n",
    "# We now load the CSV file `rag_train_dataset.csv` using Hugging Face Datasets.\n",
    "# This makes it easy to work with the data in the SentenceTransformers Trainer.\n",
    "#\n",
    "# Expected columns:\n",
    "#   - 'question' : the user query text\n",
    "#   - 'chunk'    : the corresponding relevant passage\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Load dataset from CSV into a DatasetDict with a 'train' split\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\"train\": CSV_PATH},\n",
    ")\n",
    "\n",
    "# Extract the 'train' split\n",
    "pairs_dataset = dataset[\"train\"]\n",
    "\n",
    "# Inspect the dataset structure\n",
    "print(pairs_dataset)\n",
    "print(\"Column names:\", pairs_dataset.column_names)\n",
    "\n",
    "# Show a few examples to ensure the data is loaded correctly\n",
    "for i in range(3):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"  question:\", pairs_dataset[i][\"question\"])\n",
    "    print(\"  chunk   :\", pairs_dataset[i][\"chunk\"])\n",
    "    print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1dc0fc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 546\n",
      "Eval size : 61\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Prepare Train / Eval Splits and Select Required Columns\n",
    "# ============================================================================\n",
    "# MultipleNegativesRankingLoss expects **pairs** of texts per example.\n",
    "# Here we treat:\n",
    "#   - 'question' as the anchor\n",
    "#   - 'chunk'   as the positive\n",
    "#\n",
    "# We select only these two columns and create a train/validation split\n",
    "# (e.g., 90% train, 10% validation).\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Keep only the 'question' and 'chunk' columns in that order.\n",
    "# The SentenceTransformers Trainer will interpret this as (anchor, positive).\n",
    "pairs_dataset = pairs_dataset.select_columns([\"question\", \"chunk\"])\n",
    "\n",
    "# Create a train/validation split from the single 'train' split\n",
    "split = pairs_dataset.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "train_dataset = split[\"train\"]\n",
    "eval_dataset = split[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Eval size :\", len(eval_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e20553c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (unfined-tuned) model performance will be computed after model initialization.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Build a Simple TripletEvaluator for Validation\n",
    "# ============================================================================\n",
    "# For evaluation, we use a TripletEvaluator from SentenceTransformers.\n",
    "# It expects three parallel lists:\n",
    "#   - anchors   (e.g., questions)\n",
    "#   - positives (e.g., their matching chunks)\n",
    "#   - negatives (e.g., randomly sampled non-matching chunks)\n",
    "#\n",
    "# Here we construct **synthetic triplets** from the eval split by:\n",
    "#   - using each (question, chunk) pair as (anchor, positive)\n",
    "#   - sampling a random different chunk as the negative\n",
    "#\n",
    "# This gives us a rough \"does anchor-positive score higher than anchor-negative\"\n",
    "# style retrieval accuracy metric for monitoring training.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def make_fake_triplets_from_pairs(ds, max_samples: int = 500) -> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"Create synthetic triplets (anchor, positive, negative) from a pairs dataset.\n",
    "\n",
    "    Args:\n",
    "        ds: A Hugging Face Dataset with columns ['question', 'chunk'].\n",
    "        max_samples: Maximum number of triplets to create (for efficiency).\n",
    "\n",
    "    Returns:\n",
    "        anchors: List of anchor strings (questions).\n",
    "        positives: List of positive strings (true chunks).\n",
    "        negatives: List of negative strings (random other chunks).\n",
    "    \"\"\"\n",
    "    # Ensure we do not sample more than the dataset size\n",
    "    max_samples = min(max_samples, len(ds))\n",
    "\n",
    "    anchors: List[str] = []\n",
    "    positives: List[str] = []\n",
    "    negatives: List[str] = []\n",
    "\n",
    "    # Pre-collect all chunks for negative sampling\n",
    "    all_chunks: List[str] = ds[\"chunk\"]\n",
    "\n",
    "    for i in range(max_samples):\n",
    "        anchor = ds[i][\"question\"]\n",
    "        positive = ds[i][\"chunk\"]\n",
    "\n",
    "        # Sample a random negative chunk that is different from the positive\n",
    "        negative = positive\n",
    "        while negative == positive:\n",
    "            negative = random.choice(all_chunks)\n",
    "\n",
    "        anchors.append(anchor)\n",
    "        positives.append(positive)\n",
    "        negatives.append(negative)\n",
    "\n",
    "    return anchors, positives, negatives\n",
    "\n",
    "# Build triplets from the eval split for use in the TripletEvaluator\n",
    "eval_anchors, eval_positives, eval_negatives = make_fake_triplets_from_pairs(\n",
    "    eval_dataset,\n",
    "    max_samples=500\n",
    ")\n",
    "\n",
    "# Create the TripletEvaluator\n",
    "dev_evaluator = TripletEvaluator(\n",
    "    anchors=eval_anchors,\n",
    "    positives=eval_positives,\n",
    "    negatives=eval_negatives,\n",
    "    name=\"rag2-mnrl-dev\",\n",
    ")\n",
    "\n",
    "# Quick baseline evaluation before fine-tuning\n",
    "print(\"Baseline (unfined-tuned) model performance will be computed after model initialization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01b85f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "982d135a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating base model before fine-tuning...\n",
      "{'rag2-mnrl-dev_cosine_accuracy': 0.9508196711540222}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Initialize the SentenceTransformer Model and MNRL Loss\n",
    "# ============================================================================\n",
    "# We now load the base SentenceTransformer model and wrap it with optional\n",
    "# model card metadata. Then we define the MultipleNegativesRankingLoss, which\n",
    "# will be used by the trainer.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Initialize the SentenceTransformer model from the base checkpoint\n",
    "model = SentenceTransformer(\n",
    "    BASE_MODEL_NAME,\n",
    "    model_card_data=SentenceTransformerModelCardData(\n",
    "        language=\"en\",\n",
    "        license=\"apache-2.0\",\n",
    "        model_name=\"msmarco-distilbert-cos-v5 fine-tuned on RAG2 question-chunk pairs\",\n",
    "    ))\n",
    "\n",
    "# Define the MultipleNegativesRankingLoss.\n",
    "# This expects each training example to consist of two texts:\n",
    "#   (anchor, positive)\n",
    "# and uses other positives in the same batch as implicit negatives.\n",
    "loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Evaluate the base model before training using the TripletEvaluator\n",
    "print(\"Evaluating base model before fine-tuning...\")\n",
    "base_score = dev_evaluator(model)\n",
    "print(base_score)\n",
    "# print(f\"Baseline accuracy (TripletEvaluator): {base_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ddb51b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Define SentenceTransformer Training Arguments\n",
    "# ============================================================================\n",
    "# SentenceTransformerTrainingArguments configures the training process:\n",
    "#   - number of epochs\n",
    "#   - batch sizes\n",
    "#   - learning rate, warmup\n",
    "#   - evaluation and saving strategy\n",
    "#\n",
    "# We also specify NO_DUPLICATES batch sampler, which is beneficial for\n",
    "# MultipleNegativesRankingLoss (we want each text to appear at most once\n",
    "# per batch so it's a valid negative for others).\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required: where to save training outputs and checkpoints\n",
    "    output_dir=OUTPUT_DIR,\n",
    "\n",
    "    # Core training hyperparameters\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "\n",
    "    # Mixed precision settings (set fp16=False if your GPU does not support it)\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "\n",
    "    # Batch sampler: NO_DUPLICATES is recommended for MNRL\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
    "\n",
    "    # Evaluation and checkpointing\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,       # keep only the 2 most recent checkpoints\n",
    "    logging_steps=100,\n",
    "\n",
    "    # Optional: a descriptive run name (helpful if using W&B or similar tools)\n",
    "    run_name=\"msmarco-distilbert-rag2-mnrl\",\n",
    "  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acce44e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8521ae018c1498e9863fefa804e6806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rag2-mnrl-dev Cosine Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.343058</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.256014</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.242982</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.243339</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.240404</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=45, training_loss=0.3148843341403537, metrics={'train_runtime': 11.9883, 'train_samples_per_second': 227.721, 'train_steps_per_second': 3.754, 'total_flos': 0.0, 'train_loss': 0.3148843341403537, 'epoch': 5.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Create the SentenceTransformerTrainer and Start Training\n",
    "# ============================================================================\n",
    "# The SentenceTransformerTrainer encapsulates the training loop.\n",
    "# We pass:\n",
    "#   - model: SentenceTransformer instance\n",
    "#   - args:  SentenceTransformerTrainingArguments\n",
    "#   - train_dataset: HF Dataset with (question, chunk) pairs\n",
    "#   - eval_dataset:  HF Dataset for evaluation\n",
    "#   - loss: MultipleNegativesRankingLoss\n",
    "#   - evaluator: TripletEvaluator for dev set performance\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss=loss,\n",
    "    evaluator=dev_evaluator,\n",
    ")\n",
    "\n",
    "# Start the training process\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49b1a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model saved to: models/sentence-transformers/msmarco-distilbert-cos-v5\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Save the Fine-Tuned Model Locally\n",
    "# ============================================================================\n",
    "# After training completes, we save the final model to disk.\n",
    "# This folder can later be loaded with SentenceTransformer(...)\n",
    "# or used as a Hugging Face model checkpoint path.\n",
    "# ----------------------------------------------------------------------------\n",
    "import os\n",
    "# final_model_path = os.path.join(OUTPUT_DIR, \"final\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Fine-tuned model saved to: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457f9144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aa5cdb1",
   "metadata": {},
   "source": [
    "\n",
    "# Generate a QA Evaluation Dataset from ChromaDB (10% sample) for RAG Testing\n",
    "<img src=\"https://raw.githubusercontent.com/CNUClasses/CPSC471/master/content/lectures/week13/generate_synthetic_dataset_no_heading.png\" alt=\"standard\" style=\"max-height:300px;  margin:10px 0; vertical-align:middle;\">\n",
    "\n",
    "**Goal:** Build an **evaluation dataset** for RAG by sampling **10%** of your **pre‑chunked ChromaDB** collection and generating up to **5 question–answer pairs per chunk** using **OpenAI** *or* a **Hugging Face** model. Save the result to **CSV**.\n",
    "\n",
    "### Why a relatively powerful LLM for question generation?\n",
    "- **Answerability & grounding:** It must read a chunk and ask **non‑trivial but answerable** questions grounded **only** in that chunk (no outside knowledge).  \n",
    "- **Paraphrase variety:** Better models generate diverse phrasings, reducing evaluation overfitting.  \n",
    "- **Domain nuance:** Strong models handle **technical jargon** and produce **faithful** answers rather than hallucinations.  \n",
    "- **Format adherence:** Higher‑end models more reliably output **valid JSON** you can parse automatically.\n",
    "\n",
    "> This notebook keeps costs in check by sampling **10%** of chunks and capping to **≤5 Q/A per chunk**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da314073",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup & Installation\n",
    "\n",
    "Uncomment the cell below if you need to install dependencies.  \n",
    "We support **either** OpenAI **or** Hugging Face for question generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a638abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install -U chromadb openai transformers accelerate torch pandas numpy tqdm python-dotenv\n",
    "# If you're on Apple Silicon:\n",
    "# !pip install 'torch>=2.2.0' --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9e2bd0",
   "metadata": {},
   "source": [
    "\n",
    "### Configuration\n",
    "- **ChromaDB:** point to your persisted DB folder and collection name.  \n",
    "- **Sampling:** set `SAMPLE_FRACTION=0.10` (10%).  \n",
    "- **Generation choice:** set `USE_OPENAI=True` (needs `OPENAI_API_KEY`) or `USE_HF=True` (downloads HF model).  \n",
    "- **Safety knobs:** `MAX_Q_PER_CHUNK` (≤5), `MAX_CHARS_PER_CHUNK` to truncate very long chunks.\n",
    "\n",
    "> If both `USE_OPENAI` and `USE_HF` are True, OpenAI is used by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9894b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# ---- User-editable parameters ----\n",
    "PERSIST_DIR = \"../week12/rag_chroma\"        # ChromaDB persistence path (folder will be created)\n",
    "# PERSIST_DIR = os.getenv(\"../week12\", \"./rag_chroma\")\n",
    "COLLECTION_NAME= \"cnu_rag_lab\"                       # collection name used previously\n",
    "# COLLECTION_NAME   = os.getenv(\"cnu_rag_lab\", \"my_chunks\")\n",
    "\n",
    "SAMPLE_FRACTION   = float(os.getenv(\"SAMPLE_FRACTION\", \"0.1\"))  # 10% of chunks\n",
    "MAX_Q_PER_CHUNK   = int(os.getenv(\"MAX_Q_PER_CHUNK\", \"5\"))       # cap at 5\n",
    "MAX_CHARS_PER_CHUNK = int(os.getenv(\"MAX_CHARS_PER_CHUNK\", \"3000\"))\n",
    "RANDOM_SEED       = int(os.getenv(\"RANDOM_SEED\", \"471\"))\n",
    "\n",
    "# Generation backends\n",
    "USE_OPENAI = os.getenv(\"USE_OPENAI\", \"False\").lower() == \"true\"\n",
    "USE_HF     = os.getenv(\"USE_HF\", \"True\").lower() == \"true\"  # default to HF to avoid API keys\n",
    "\n",
    "# OpenAI config\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  # set in env or .env\n",
    "\n",
    "# Hugging Face config (choose a small instruct model if running on CPU)\n",
    "HF_TASK  = \"text2text-generation\"  # for instruction-style prompting\n",
    "HF_MODEL = os.getenv(\"HF_MODEL\", \"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "# HF_MODEL = os.getenv(\"HF_MODEL\", \"google/flan-t5-base\")  # lightweight; upgrade if you have GPU\n",
    "\n",
    "HF_MAX_NEW_TOKENS = int(os.getenv(\"HF_MAX_NEW_TOKENS\", \"512\"))\n",
    "HF_DO_SAMPLE = os.getenv(\"HF_DO_SAMPLE\", \"False\").lower() == \"true\"\n",
    "\n",
    "# Output\n",
    "OUTPUT_CSV = os.getenv(\"OUTPUT_CSV\", \"rag_eval_dataset.csv\")\n",
    "\n",
    "random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f1ac11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_OPENAI\n",
    "USE_HF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ed370",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Connect to ChromaDB and load chunks\n",
    "\n",
    "We assume your data is **already chunked** and stored in a Chroma collection.  \n",
    "We’ll read IDs, documents, and metadatas in **pages** to avoid loading everything at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6add652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks in collection 'cnu_rag_lab': 1256\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "collection = client.get_collection(COLLECTION_NAME)\n",
    "\n",
    "total = collection.count()\n",
    "print(f\"Total chunks in collection '{COLLECTION_NAME}': {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fe9bd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1256 items from Chroma.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- Retrieve all ids, docs, and metadatas with paging ----\n",
    "ALL_IDS, ALL_DOCS, ALL_META = [], [], []\n",
    "\n",
    "PAGE = 1000\n",
    "offset = 0\n",
    "while True:\n",
    "    batch = collection.get(\n",
    "        include=[\"documents\", \"metadatas\"],\n",
    "        limit=PAGE,\n",
    "        offset=offset\n",
    "    )\n",
    "    ids = batch.get(\"ids\", [])\n",
    "    docs = batch.get(\"documents\", [])\n",
    "    metas = batch.get(\"metadatas\", [])\n",
    "    if not ids:\n",
    "        break\n",
    "    ALL_IDS.extend(ids)\n",
    "    ALL_DOCS.extend(docs)\n",
    "    ALL_META.extend(metas)\n",
    "    offset += len(ids)\n",
    "    if offset >= total:\n",
    "        break\n",
    "\n",
    "print(f\"Loaded {len(ALL_IDS)} items from Chroma.\")\n",
    "assert len(ALL_IDS) == len(ALL_DOCS) == len(ALL_META)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cb2366",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Sample ~10% of chunks (reproducible)\n",
    "\n",
    "We take a 10% sample to keep cost/time manageable. You can adjust `SAMPLE_FRACTION`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dfa34e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 125 / 1256 chunks (~10.0%).\n",
      "Prepared 125 chunks for question generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_total = len(ALL_IDS)\n",
    "n_sample = max(1, int(n_total * SAMPLE_FRACTION))\n",
    "indices = list(range(n_total))\n",
    "random.shuffle(indices)\n",
    "sample_idx = sorted(indices[:n_sample])\n",
    "\n",
    "print(f\"Sampling {n_sample} / {n_total} chunks (~{SAMPLE_FRACTION*100:.1f}%).\")\n",
    "\n",
    "SAMPLED = [\n",
    "    {\n",
    "        \"chunk_id\": ALL_IDS[i],\n",
    "        \"text\": (ALL_DOCS[i] or \"\")[:MAX_CHARS_PER_CHUNK],  # truncate long chunks\n",
    "        \"metadata\": ALL_META[i] or {}\n",
    "    }\n",
    "    for i in sample_idx\n",
    "]\n",
    "print(f\"Prepared {len(SAMPLED)} chunks for question generation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721120f0",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Prompt design for grounded Q/A\n",
    "\n",
    "We ask the model to produce **up to 5 Q/A pairs** that are **fully answerable from the chunk** and return **valid JSON**:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\"question\": \"...\", \"answer\": \"...\"},\n",
    "  ...\n",
    "]\n",
    "```\n",
    "If the chunk lacks enough information, the model should return **[]**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0991a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "JSON_INSTRUCTIONS = (\n",
    "    \"Return ONLY a JSON array of objects, each with keys 'question' and 'answer'. \"\n",
    "    \"Do not include any extra commentary. If the chunk lacks enough info, return [].\"\n",
    ")\n",
    "\n",
    "def build_prompt(chunk_text: str, max_q: int) -> str:\n",
    "    return (\n",
    "        \"You are a careful question writer for Retrieval-Augmented Generation (RAG).\\n\"\n",
    "        \"Read the CHUNK below and create up to {max_q} question–answer pairs that are:\\n\"\n",
    "        \" - Non-trivial but fully answerable using ONLY the CHUNK\\n\"\n",
    "        \" - If you think there may be other chunks with relevant information, only create questions that can be answered using this CHUNK alone.\\n\"\n",
    "        \" - Concise, precise, and faithful to the CHUNK (no outside knowledge)\\n\"\n",
    "        \" - Useful for evaluating a retriever's ability to find this CHUNK\\n\\n\"\n",
    "        f\"{JSON_INSTRUCTIONS}\\n\\n\"\n",
    "        \"CHUNK:\\n\"\n",
    "        f\"{chunk_text}\\n\\n\"\n",
    "        \"JSON:\"\n",
    "    ).format(max_q=max_q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c10a8",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Choose a generation backend\n",
    "\n",
    "Set **`USE_OPENAI=True`** to call OpenAI Chat Completions; otherwise we default to a **Hugging Face** model (`text2text-generation`).  \n",
    "For CPU‑only environments, start with `google/flan-t5-base` (lower quality, but free). On GPU, try a stronger instruct model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9446030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- OpenAI backend (chat completions) ----\n",
    "def openai_generate_json(prompt: str) -> str:\n",
    "    \"\"\"Return raw JSON string from OpenAI Chat Completions.\"\"\"\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        resp = client.chat.completions.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            messages=[\n",
    "                {\"role\":\"system\",\"content\":\"You write grounded Q/A pairs in strict JSON.\"},\n",
    "                {\"role\":\"user\",\"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"OpenAI generation failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66c1c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Hugging Face backend (text2text-generation) ----\n",
    "from transformers import pipeline\n",
    "\n",
    "_hf_pipe = None\n",
    "def hf_generate_json(prompt: str) -> str:\n",
    "    global _hf_pipe\n",
    "    if _hf_pipe is None:\n",
    "        _hf_pipe = pipeline(HF_TASK, model=HF_MODEL)\n",
    "    out = _hf_pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=HF_MAX_NEW_TOKENS,\n",
    "        do_sample=HF_DO_SAMPLE\n",
    "    )\n",
    "    # HF pipelines return a list of dicts; try the first item\n",
    "    text = out[0].get(\"generated_text\", \"\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1b2df3",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Robust JSON parsing of model output\n",
    "\n",
    "Models sometimes wrap JSON in prose or code fences. We defensively extract the first JSON array.\n",
    "If parsing fails, we fall back to a very simple regex that looks for `\"question\"` and `\"answer\"` pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b265f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def extract_json_array(text: str) -> List[Dict[str, Any]]:\n",
    "    # Try to find a JSON array block\n",
    "    # 1) Inline code fence\n",
    "    fence = re.findall(r\"```(?:json)?\\s*(\\[.*?\\])\\s*```\", text, flags=re.S)\n",
    "    candidates = fence if fence else re.findall(r\"(\\[\\s*{.*?}\\s*\\])\", text, flags=re.S)\n",
    "    if candidates:\n",
    "        raw = candidates[0]\n",
    "    else:\n",
    "        raw = text.strip()\n",
    "    try:\n",
    "        arr = json.loads(raw)\n",
    "        if isinstance(arr, list):\n",
    "            # sanitize items\n",
    "            clean = []\n",
    "            for it in arr:\n",
    "                if isinstance(it, dict) and 'question' in it and 'answer' in it:\n",
    "                    q = str(it['question']).strip()\n",
    "                    a = str(it['answer']).strip()\n",
    "                    if q and a:\n",
    "                        clean.append({'question': q, 'answer': a})\n",
    "            return clean\n",
    "        return []\n",
    "    except Exception:\n",
    "        # Fallback: scrape \"question\":\"...\", \"answer\":\"...\" naive pairs\n",
    "        pairs = []\n",
    "        pattern = re.findall(r'\"question\"\\s*:\\s*\"(.*?)\"\\s*,\\s*\"answer\"\\s*:\\s*\"(.*?)\"', text, flags=re.S)\n",
    "        for q,a in pattern:\n",
    "            q = q.strip().replace('\\n',' ')\n",
    "            a = a.strip().replace('\\n',' ')\n",
    "            if q and a:\n",
    "                pairs.append({'question': q, 'answer': a})\n",
    "        return pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68aa8ac",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Generate Q/A pairs for the sampled chunks\n",
    "\n",
    "We iterate over the sample, build a grounded prompt, call the selected backend, parse JSON,\n",
    "and collect **up to 5 Q/A** per chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16846e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using backend: hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Q/A:   0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bada90ea72314020916eef09156793f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'MistralForCausalLM' is not supported for text2text-generation. Supported models are ['PeftModelForSeq2SeqLM', 'BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'T5GemmaForConditionalGeneration', 'UMT5ForConditionalGeneration', 'VoxtralForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:   1%|          | 1/125 [00:13<28:23, 13.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:   2%|▏         | 2/125 [00:21<21:30, 10.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:   2%|▏         | 3/125 [00:34<23:32, 11.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:   3%|▎         | 4/125 [00:51<27:19, 13.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:   4%|▍         | 5/125 [01:04<26:36, 13.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:   5%|▍         | 6/125 [01:13<23:32, 11.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:   6%|▌         | 7/125 [01:23<22:26, 11.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:   6%|▋         | 8/125 [01:35<22:34, 11.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:   7%|▋         | 9/125 [01:46<21:56, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:   8%|▊         | 10/125 [02:02<24:32, 12.80s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:   9%|▉         | 11/125 [02:13<22:54, 12.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  10%|▉         | 12/125 [02:22<21:12, 11.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  10%|█         | 13/125 [02:34<21:27, 11.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  11%|█         | 14/125 [02:48<22:31, 12.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  12%|█▏        | 15/125 [02:59<21:34, 11.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  13%|█▎        | 16/125 [03:11<21:42, 11.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  14%|█▎        | 17/125 [03:28<24:01, 13.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  14%|█▍        | 18/125 [03:38<21:59, 12.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  15%|█▌        | 19/125 [03:49<21:34, 12.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  16%|█▌        | 20/125 [04:01<20:56, 11.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  17%|█▋        | 21/125 [04:16<22:13, 12.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  18%|█▊        | 22/125 [04:27<21:00, 12.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  18%|█▊        | 23/125 [04:35<19:07, 11.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  19%|█▉        | 24/125 [04:46<18:19, 10.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  20%|██        | 25/125 [04:58<19:00, 11.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  21%|██        | 26/125 [05:11<19:22, 11.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  22%|██▏       | 27/125 [05:23<19:14, 11.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  22%|██▏       | 28/125 [05:39<21:27, 13.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  23%|██▎       | 29/125 [05:54<21:42, 13.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  24%|██▍       | 30/125 [06:07<21:39, 13.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  25%|██▍       | 31/125 [06:24<22:58, 14.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  26%|██▌       | 32/125 [06:37<21:43, 14.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  26%|██▋       | 33/125 [06:50<21:09, 13.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  27%|██▋       | 34/125 [06:59<18:25, 12.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  28%|██▊       | 35/125 [07:13<19:04, 12.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  29%|██▉       | 36/125 [07:21<17:02, 11.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  30%|██▉       | 37/125 [07:37<18:46, 12.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  30%|███       | 38/125 [07:50<18:40, 12.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  31%|███       | 39/125 [08:02<18:04, 12.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  32%|███▏      | 40/125 [08:15<18:09, 12.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  33%|███▎      | 41/125 [08:29<18:05, 12.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  34%|███▎      | 42/125 [08:37<16:02, 11.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  34%|███▍      | 43/125 [08:46<14:36, 10.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  35%|███▌      | 44/125 [08:56<14:08, 10.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  36%|███▌      | 45/125 [09:08<14:39, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  37%|███▋      | 46/125 [09:25<16:42, 12.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  38%|███▊      | 47/125 [09:34<15:06, 11.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  38%|███▊      | 48/125 [09:50<16:53, 13.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  39%|███▉      | 49/125 [10:07<18:01, 14.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  40%|████      | 50/125 [10:21<17:34, 14.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  41%|████      | 51/125 [10:35<17:13, 13.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  42%|████▏     | 52/125 [10:46<16:14, 13.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  42%|████▏     | 53/125 [11:03<17:12, 14.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  43%|████▎     | 54/125 [11:15<16:14, 13.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  44%|████▍     | 55/125 [11:32<17:02, 14.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  45%|████▍     | 56/125 [11:45<16:11, 14.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  46%|████▌     | 57/125 [11:59<16:02, 14.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  46%|████▋     | 58/125 [12:16<16:46, 15.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  47%|████▋     | 59/125 [12:26<14:49, 13.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  48%|████▊     | 60/125 [12:37<13:54, 12.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  49%|████▉     | 61/125 [12:54<14:45, 13.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  50%|████▉     | 62/125 [13:05<13:44, 13.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  50%|█████     | 63/125 [13:17<13:12, 12.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  51%|█████     | 64/125 [13:31<13:22, 13.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  52%|█████▏    | 65/125 [13:48<14:13, 14.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  53%|█████▎    | 66/125 [14:05<14:48, 15.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  54%|█████▎    | 67/125 [14:14<12:53, 13.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  54%|█████▍    | 68/125 [14:29<13:03, 13.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  55%|█████▌    | 69/125 [14:37<11:22, 12.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  56%|█████▌    | 70/125 [14:51<11:39, 12.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  57%|█████▋    | 71/125 [15:02<10:49, 12.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  58%|█████▊    | 72/125 [15:14<10:41, 12.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  58%|█████▊    | 73/125 [15:31<11:40, 13.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  59%|█████▉    | 74/125 [15:41<10:40, 12.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  60%|██████    | 75/125 [15:51<09:51, 11.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  61%|██████    | 76/125 [16:02<09:26, 11.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  62%|██████▏   | 77/125 [16:14<09:20, 11.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  62%|██████▏   | 78/125 [16:25<09:01, 11.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  63%|██████▎   | 79/125 [16:36<08:39, 11.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  64%|██████▍   | 80/125 [16:43<07:36, 10.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  65%|██████▍   | 81/125 [16:55<07:38, 10.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  66%|██████▌   | 82/125 [17:11<08:50, 12.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  66%|██████▋   | 83/125 [17:27<09:19, 13.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  67%|██████▋   | 84/125 [17:39<08:47, 12.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  68%|██████▊   | 85/125 [17:50<08:10, 12.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  69%|██████▉   | 86/125 [18:03<08:12, 12.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  70%|██████▉   | 87/125 [18:19<08:35, 13.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  70%|███████   | 88/125 [18:30<07:51, 12.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  71%|███████   | 89/125 [18:46<08:22, 13.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  72%|███████▏  | 90/125 [18:58<07:39, 13.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  73%|███████▎  | 91/125 [19:10<07:15, 12.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  74%|███████▎  | 92/125 [19:26<07:40, 13.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  74%|███████▍  | 93/125 [19:37<06:57, 13.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  75%|███████▌  | 94/125 [19:49<06:32, 12.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  76%|███████▌  | 95/125 [20:01<06:11, 12.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  77%|███████▋  | 96/125 [20:18<06:39, 13.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  78%|███████▊  | 97/125 [20:26<05:40, 12.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  78%|███████▊  | 98/125 [20:43<06:03, 13.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  79%|███████▉  | 99/125 [20:56<05:48, 13.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  80%|████████  | 100/125 [21:08<05:25, 13.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  81%|████████  | 101/125 [21:16<04:32, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  82%|████████▏ | 102/125 [21:28<04:26, 11.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  82%|████████▏ | 103/125 [21:35<03:49, 10.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  83%|████████▎ | 104/125 [21:48<03:55, 11.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  84%|████████▍ | 105/125 [21:58<03:36, 10.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  85%|████████▍ | 106/125 [22:09<03:24, 10.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  86%|████████▌ | 107/125 [22:19<03:12, 10.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  86%|████████▋ | 108/125 [22:33<03:16, 11.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  87%|████████▋ | 109/125 [22:47<03:18, 12.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  88%|████████▊ | 110/125 [22:56<02:49, 11.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  89%|████████▉ | 111/125 [23:10<02:49, 12.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  90%|████████▉ | 112/125 [23:21<02:30, 11.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  90%|█████████ | 113/125 [23:33<02:20, 11.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  91%|█████████ | 114/125 [23:43<02:06, 11.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  92%|█████████▏| 115/125 [23:53<01:48, 10.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  93%|█████████▎| 116/125 [24:05<01:42, 11.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  94%|█████████▎| 117/125 [24:19<01:37, 12.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  94%|█████████▍| 118/125 [24:30<01:22, 11.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  95%|█████████▌| 119/125 [24:39<01:05, 10.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  96%|█████████▌| 120/125 [24:47<00:50, 10.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  97%|█████████▋| 121/125 [24:54<00:36,  9.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  98%|█████████▊| 122/125 [25:03<00:26,  8.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  98%|█████████▊| 123/125 [25:10<00:16,  8.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A:  99%|█████████▉| 124/125 [25:18<00:08,  8.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Q/A: 100%|██████████| 125/125 [25:30<00:00, 12.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 607 Q/A rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "backend = \"openai\" if USE_OPENAI else \"hf\"\n",
    "print(f\"Using backend: {backend}\")\n",
    "\n",
    "rows = []\n",
    "for item in tqdm(SAMPLED, desc=\"Generating Q/A\"):\n",
    "    chunk_id = item[\"chunk_id\"]\n",
    "    text = item[\"text\"] or \"\"\n",
    "    meta = item[\"metadata\"] or {}\n",
    "\n",
    "    if not text.strip():\n",
    "        continue\n",
    "\n",
    "    prompt = build_prompt(text, MAX_Q_PER_CHUNK)\n",
    "    try:\n",
    "        if USE_OPENAI:  # prefer OpenAI if explicitly enabled\n",
    "            raw = openai_generate_json(prompt)\n",
    "        else:\n",
    "            raw = hf_generate_json(prompt)\n",
    "        qa_list = extract_json_array(raw)[:MAX_Q_PER_CHUNK]\n",
    "    except Exception as e:\n",
    "        qa_list = []\n",
    "        print(f\"[WARN] Generation failed for chunk {chunk_id}: {e}\")\n",
    "\n",
    "    preview = (text[:220] + \"…\") if len(text) > 220 else text\n",
    "    source = meta.get(\"source\") or meta.get(\"file_path\") or meta.get(\"url\") or \"\"\n",
    "\n",
    "    for qa in qa_list:\n",
    "        rows.append({\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"question\": qa[\"question\"],\n",
    "            \"answer\": qa[\"answer\"],\n",
    "            \"source\": source,\n",
    "            \"metadata\": json.dumps(meta, ensure_ascii=False),\n",
    "            \"chunk_preview\": preview\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"chunk_id\",\"question\",\"answer\",\"source\",\"metadata\",\"chunk_preview\"])\n",
    "print(f\"Generated {len(df)} Q/A rows.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee6ed29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source</th>\n",
       "      <th>metadata</th>\n",
       "      <th>chunk_preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>What days of the week are campus tours offered...</td>\n",
       "      <td>Campus tours are offered Monday through Saturday.</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>Come See for Yourself \\nReady to picture your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>How can one schedule a visit to Christopher Ne...</td>\n",
       "      <td>One can schedule a visit online at admission.c...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>Come See for Yourself \\nReady to picture your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>What is the phone number to call for schedulin...</td>\n",
       "      <td>(757) 594-7015/(800) 333-4268</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>Come See for Yourself \\nReady to picture your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>Is it possible to schedule a campus tour on ma...</td>\n",
       "      <td>No, campus tours are not offered on major holi...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>Come See for Yourself \\nReady to picture your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>What is the website address to schedule a visi...</td>\n",
       "      <td>admission.cnu.edu</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>Come See for Yourself \\nReady to picture your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>Which minor fields of study are offered in the...</td>\n",
       "      <td>The Philosophy of Law and Professional Ethics ...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>Philosophy of Law, minor ........................</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>How many minor fields of study are available i...</td>\n",
       "      <td>The Political Science department offers 2 mino...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>Philosophy of Law, minor ........................</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>Which departments offer minors in Human Rights...</td>\n",
       "      <td>The Human Rights and Conflict Resolution and U...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>Philosophy of Law, minor ........................</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>What department offers a minor in Psychology?</td>\n",
       "      <td>The Psychology minor is offered by the Psychol...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>Philosophy of Law, minor ........................</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>How many minor fields of study are available i...</td>\n",
       "      <td>There are 7 minor fields of study available in...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>Philosophy of Law, minor ........................</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>31</td>\n",
       "      <td>Which building is the Office of Admission loca...</td>\n",
       "      <td>Christopher Newport Hall 200</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>10 \\n           \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>31</td>\n",
       "      <td>Who is the Dean of Admission at Christopher Ne...</td>\n",
       "      <td>Dr. Robert Lange</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>10 \\n           \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>31</td>\n",
       "      <td>What services does the Office of Admission pro...</td>\n",
       "      <td>The Office of Admission provides services such...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>10 \\n           \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>31</td>\n",
       "      <td>How can prospective students contact the Offic...</td>\n",
       "      <td>(757) 594-7334</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>10 \\n           \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>31</td>\n",
       "      <td>What is the email address for the Office of Ad...</td>\n",
       "      <td>admit@cnu.edu</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>10 \\n           \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>36</td>\n",
       "      <td>Which English language proficiency tests are a...</td>\n",
       "      <td>Christopher Newport University accepts the fol...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>translations are required. \\n3. Submit officia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>36</td>\n",
       "      <td>What are the SAT and ACT codes for Christopher...</td>\n",
       "      <td>The SAT code for Christopher Newport Universit...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>translations are required. \\n3. Submit officia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>36</td>\n",
       "      <td>What documents must be submitted to demonstrat...</td>\n",
       "      <td>If the native language is not English, applica...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>translations are required. \\n3. Submit officia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>36</td>\n",
       "      <td>What must be submitted to demonstrate financia...</td>\n",
       "      <td>Applicants must submit a Financial Resources S...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>translations are required. \\n3. Submit officia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>36</td>\n",
       "      <td>Who is eligible for the Enrichment Program for...</td>\n",
       "      <td>The Enrichment Program for High School Student...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>translations are required. \\n3. Submit officia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_id                                           question  \\\n",
       "0         7  What days of the week are campus tours offered...   \n",
       "1         7  How can one schedule a visit to Christopher Ne...   \n",
       "2         7  What is the phone number to call for schedulin...   \n",
       "3         7  Is it possible to schedule a campus tour on ma...   \n",
       "4         7  What is the website address to schedule a visi...   \n",
       "5        20  Which minor fields of study are offered in the...   \n",
       "6        20  How many minor fields of study are available i...   \n",
       "7        20  Which departments offer minors in Human Rights...   \n",
       "8        20      What department offers a minor in Psychology?   \n",
       "9        20  How many minor fields of study are available i...   \n",
       "10       31  Which building is the Office of Admission loca...   \n",
       "11       31  Who is the Dean of Admission at Christopher Ne...   \n",
       "12       31  What services does the Office of Admission pro...   \n",
       "13       31  How can prospective students contact the Offic...   \n",
       "14       31  What is the email address for the Office of Ad...   \n",
       "15       36  Which English language proficiency tests are a...   \n",
       "16       36  What are the SAT and ACT codes for Christopher...   \n",
       "17       36  What documents must be submitted to demonstrat...   \n",
       "18       36  What must be submitted to demonstrate financia...   \n",
       "19       36  Who is eligible for the Enrichment Program for...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   Campus tours are offered Monday through Saturday.   \n",
       "1   One can schedule a visit online at admission.c...   \n",
       "2                       (757) 594-7015/(800) 333-4268   \n",
       "3   No, campus tours are not offered on major holi...   \n",
       "4                                   admission.cnu.edu   \n",
       "5   The Philosophy of Law and Professional Ethics ...   \n",
       "6   The Political Science department offers 2 mino...   \n",
       "7   The Human Rights and Conflict Resolution and U...   \n",
       "8   The Psychology minor is offered by the Psychol...   \n",
       "9   There are 7 minor fields of study available in...   \n",
       "10                       Christopher Newport Hall 200   \n",
       "11                                   Dr. Robert Lange   \n",
       "12  The Office of Admission provides services such...   \n",
       "13                                     (757) 594-7334   \n",
       "14                                      admit@cnu.edu   \n",
       "15  Christopher Newport University accepts the fol...   \n",
       "16  The SAT code for Christopher Newport Universit...   \n",
       "17  If the native language is not English, applica...   \n",
       "18  Applicants must submit a Financial Resources S...   \n",
       "19  The Enrichment Program for High School Student...   \n",
       "\n",
       "                                      source  \\\n",
       "0   ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "1   ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "2   ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "3   ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "4   ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "5   ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "6   ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "7   ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "8   ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "9   ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "10  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "11  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "12  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "13  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "14  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "15  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "16  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "17  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "18  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "19  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "\n",
       "                                             metadata  \\\n",
       "0   {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "1   {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "2   {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "3   {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "4   {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "5   {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "6   {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "7   {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "8   {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "9   {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "10  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "11  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "12  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "13  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "14  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "15  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "16  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "17  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "18  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "19  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "\n",
       "                                        chunk_preview  \n",
       "0   Come See for Yourself \\nReady to picture your ...  \n",
       "1   Come See for Yourself \\nReady to picture your ...  \n",
       "2   Come See for Yourself \\nReady to picture your ...  \n",
       "3   Come See for Yourself \\nReady to picture your ...  \n",
       "4   Come See for Yourself \\nReady to picture your ...  \n",
       "5   Philosophy of Law, minor ........................  \n",
       "6   Philosophy of Law, minor ........................  \n",
       "7   Philosophy of Law, minor ........................  \n",
       "8   Philosophy of Law, minor ........................  \n",
       "9   Philosophy of Law, minor ........................  \n",
       "10  10 \\n           \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n...  \n",
       "11  10 \\n           \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n...  \n",
       "12  10 \\n           \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n...  \n",
       "13  10 \\n           \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n...  \n",
       "14  10 \\n           \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n...  \n",
       "15  translations are required. \\n3. Submit officia...  \n",
       "16  translations are required. \\n3. Submit officia...  \n",
       "17  translations are required. \\n3. Submit officia...  \n",
       "18  translations are required. \\n3. Submit officia...  \n",
       "19  translations are required. \\n3. Submit officia...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edc7d749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('What is the website address to schedule a visit to Christopher Newport '\n",
      " 'University?')\n",
      "'admission.cnu.edu'\n"
     ]
    }
   ],
   "source": [
    "from pprint import PrettyPrinter\n",
    "# Create a PrettyPrinter with custom indentation\n",
    "pp = PrettyPrinter(indent=4)\n",
    "\n",
    "df.head(5).iloc[:5,:3]\n",
    "pp.pprint(df.iloc[4,1])\n",
    "pp.pprint(df.iloc[4,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2facacce",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Save evaluation dataset to CSV\n",
    "\n",
    "This CSV can be loaded by your **RAG evaluation** notebook to compute **Recall@K, Precision@K, MAP, and MRR**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46f90319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation dataset to: rag_eval_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Saved evaluation dataset to: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673f5048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 452\n",
      "drwxr-xr-x.  2 kperkins411 kperkins411    157 Nov 14 14:13 .\n",
      "drwxr-xr-x. 16 kperkins411 kperkins411   4096 Nov 14 13:22 ..\n",
      "-rw-r--r--.  1 kperkins411 kperkins411  59163 Nov 14 14:59 Chroma_QA_Eval_Generator.ipynb\n",
      "-rw-r--r--.  1 kperkins411 kperkins411 348168 Nov 14 14:42 rag_eval_dataset.csv\n",
      "-rw-r--r--.  1 kperkins411 kperkins411  16648 Nov 14 13:22 RAG_Metrics_Practice_Notebook.ipynb\n",
      "-rw-r--r--.  1 kperkins411 kperkins411  22276 Nov 14 13:22 RAG_Retriever_Eval_with_BM25.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90277448",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Tips & Next Steps\n",
    "\n",
    "- **Quality control:** Spot‑check a few rows to ensure questions are **answerable from the chunk** and answers are **faithful**.  \n",
    "- **Stronger models ⇒ better Q/A:** If using HF on CPU (e.g., `flan‑t5‑base`), consider upgrading to a stronger instruct model on GPU for higher‑quality questions.  \n",
    "- **De‑duplication:** Remove near‑duplicate questions across chunks to reduce evaluation bias.  \n",
    "- **Balance sampling:** You can stratify sampling by source/file/topic to ensure coverage.  \n",
    "- **Costs:** OpenAI usage scales with tokens; keep chunks truncated and sample fraction small to control spend.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

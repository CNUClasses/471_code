{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aa5cdb1",
   "metadata": {},
   "source": [
    "\n",
    "# Generate a QA Evaluation Dataset from ChromaDB (10% sample) for RAG Testing\n",
    "\n",
    "**Audience:** 4th‑year CS students  \n",
    "**Goal:** Build an **evaluation dataset** for RAG by sampling **10%** of your **pre‑chunked ChromaDB** collection and generating up to **5 question–answer pairs per chunk** using **OpenAI** *or* a **Hugging Face** model. Save the result to **CSV**.\n",
    "\n",
    "### Why a relatively powerful LLM for question generation?\n",
    "- **Answerability & grounding:** It must read a chunk and ask **non‑trivial but answerable** questions grounded **only** in that chunk (no outside knowledge).  \n",
    "- **Paraphrase variety:** Better models generate diverse phrasings, reducing evaluation overfitting.  \n",
    "- **Domain nuance:** Strong models handle **technical jargon** and produce **faithful** answers rather than hallucinations.  \n",
    "- **Format adherence:** Higher‑end models more reliably output **valid JSON** you can parse automatically.\n",
    "\n",
    "> This notebook keeps costs in check by sampling **10%** of chunks and capping to **≤5 Q/A per chunk**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da314073",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup & Installation\n",
    "\n",
    "Uncomment the cell below if you need to install dependencies.  \n",
    "We support **either** OpenAI **or** Hugging Face for question generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a638abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install -U chromadb openai transformers accelerate torch pandas numpy tqdm python-dotenv\n",
    "# If you're on Apple Silicon:\n",
    "# !pip install 'torch>=2.2.0' --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9e2bd0",
   "metadata": {},
   "source": [
    "\n",
    "### Configuration\n",
    "- **ChromaDB:** point to your persisted DB folder and collection name.  \n",
    "- **Sampling:** set `SAMPLE_FRACTION=0.10` (10%).  \n",
    "- **Generation choice:** set `USE_OPENAI=True` (needs `OPENAI_API_KEY`) or `USE_HF=True` (downloads HF model).  \n",
    "- **Safety knobs:** `MAX_Q_PER_CHUNK` (≤5), `MAX_CHARS_PER_CHUNK` to truncate very long chunks.\n",
    "\n",
    "> If both `USE_OPENAI` and `USE_HF` are True, OpenAI is used by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9894b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# ---- User-editable parameters ----\n",
    "PERSIST_DIRECTORY = os.getenv(\"CHROMA_PERSIST_DIR\", \"./chroma\")\n",
    "COLLECTION_NAME   = os.getenv(\"CHROMA_COLLECTION\", \"my_chunks\")\n",
    "\n",
    "SAMPLE_FRACTION   = float(os.getenv(\"SAMPLE_FRACTION\", \"0.10\"))  # 10% of chunks\n",
    "MAX_Q_PER_CHUNK   = int(os.getenv(\"MAX_Q_PER_CHUNK\", \"5\"))       # cap at 5\n",
    "MAX_CHARS_PER_CHUNK = int(os.getenv(\"MAX_CHARS_PER_CHUNK\", \"3000\"))\n",
    "RANDOM_SEED       = int(os.getenv(\"RANDOM_SEED\", \"471\"))\n",
    "\n",
    "# Generation backends\n",
    "USE_OPENAI = os.getenv(\"USE_OPENAI\", \"False\").lower() == \"true\"\n",
    "USE_HF     = os.getenv(\"USE_HF\", \"True\").lower() == \"true\"  # default to HF to avoid API keys\n",
    "\n",
    "# OpenAI config\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  # set in env or .env\n",
    "\n",
    "# Hugging Face config (choose a small instruct model if running on CPU)\n",
    "HF_TASK  = \"text2text-generation\"  # for instruction-style prompting\n",
    "HF_MODEL = os.getenv(\"HF_MODEL\", \"google/flan-t5-base\")  # lightweight; upgrade if you have GPU\n",
    "HF_MAX_NEW_TOKENS = int(os.getenv(\"HF_MAX_NEW_TOKENS\", \"512\"))\n",
    "HF_DO_SAMPLE = os.getenv(\"HF_DO_SAMPLE\", \"False\").lower() == \"true\"\n",
    "\n",
    "# Output\n",
    "OUTPUT_CSV = os.getenv(\"OUTPUT_CSV\", \"rag_eval_dataset.csv\")\n",
    "\n",
    "random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ed370",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Connect to ChromaDB and load chunks\n",
    "\n",
    "We assume your data is **already chunked** and stored in a Chroma collection.  \n",
    "We’ll read IDs, documents, and metadatas in **pages** to avoid loading everything at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6add652",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=PERSIST_DIRECTORY)\n",
    "collection = client.get_collection(COLLECTION_NAME)\n",
    "\n",
    "total = collection.count()\n",
    "print(f\"Total chunks in collection '{COLLECTION_NAME}': {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Retrieve all ids, docs, and metadatas with paging ----\n",
    "ALL_IDS, ALL_DOCS, ALL_META = [], [], []\n",
    "\n",
    "PAGE = 1000\n",
    "offset = 0\n",
    "while True:\n",
    "    batch = collection.get(\n",
    "        include=[\"documents\", \"metadatas\"],\n",
    "        limit=PAGE,\n",
    "        offset=offset\n",
    "    )\n",
    "    ids = batch.get(\"ids\", [])\n",
    "    docs = batch.get(\"documents\", [])\n",
    "    metas = batch.get(\"metadatas\", [])\n",
    "    if not ids:\n",
    "        break\n",
    "    ALL_IDS.extend(ids)\n",
    "    ALL_DOCS.extend(docs)\n",
    "    ALL_META.extend(metas)\n",
    "    offset += len(ids)\n",
    "    if offset >= total:\n",
    "        break\n",
    "\n",
    "print(f\"Loaded {len(ALL_IDS)} items from Chroma.\")\n",
    "assert len(ALL_IDS) == len(ALL_DOCS) == len(ALL_META)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cb2366",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Sample ~10% of chunks (reproducible)\n",
    "\n",
    "We take a 10% sample to keep cost/time manageable. You can adjust `SAMPLE_FRACTION`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfa34e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_total = len(ALL_IDS)\n",
    "n_sample = max(1, int(n_total * SAMPLE_FRACTION))\n",
    "indices = list(range(n_total))\n",
    "random.shuffle(indices)\n",
    "sample_idx = sorted(indices[:n_sample])\n",
    "\n",
    "print(f\"Sampling {n_sample} / {n_total} chunks (~{SAMPLE_FRACTION*100:.1f}%).\")\n",
    "\n",
    "SAMPLED = [\n",
    "    {\n",
    "        \"chunk_id\": ALL_IDS[i],\n",
    "        \"text\": (ALL_DOCS[i] or \"\")[:MAX_CHARS_PER_CHUNK],  # truncate long chunks\n",
    "        \"metadata\": ALL_META[i] or {}\n",
    "    }\n",
    "    for i in sample_idx\n",
    "]\n",
    "print(f\"Prepared {len(SAMPLED)} chunks for question generation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721120f0",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Prompt design for grounded Q/A\n",
    "\n",
    "We ask the model to produce **up to 5 Q/A pairs** that are **fully answerable from the chunk** and return **valid JSON**:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\"question\": \"...\", \"answer\": \"...\"},\n",
    "  ...\n",
    "]\n",
    "```\n",
    "If the chunk lacks enough information, the model should return **[]**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0991a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "JSON_INSTRUCTIONS = (\n",
    "    \"Return ONLY a JSON array of objects, each with keys 'question' and 'answer'. \"\n",
    "    \"Do not include any extra commentary. If the chunk lacks enough info, return [].\"\n",
    ")\n",
    "\n",
    "def build_prompt(chunk_text: str, max_q: int) -> str:\n",
    "    return (\n",
    "        \"You are a careful question writer for Retrieval-Augmented Generation (RAG).\\n\"\n",
    "        \"Read the CHUNK below and create up to {max_q} question–answer pairs that are:\\n\"\n",
    "        \" - Non-trivial but fully answerable using ONLY the CHUNK\\n\"\n",
    "        \" - Concise, precise, and faithful to the CHUNK (no outside knowledge)\\n\"\n",
    "        \" - Useful for evaluating a retriever's ability to find this CHUNK\\n\\n\"\n",
    "        f\"{JSON_INSTRUCTIONS}\\n\\n\"\n",
    "        \"CHUNK:\\n\"\n",
    "        f\"{chunk_text}\\n\\n\"\n",
    "        \"JSON:\"\n",
    "    ).format(max_q=max_q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c10a8",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Choose a generation backend\n",
    "\n",
    "Set **`USE_OPENAI=True`** to call OpenAI Chat Completions; otherwise we default to a **Hugging Face** model (`text2text-generation`).  \n",
    "For CPU‑only environments, start with `google/flan-t5-base` (lower quality, but free). On GPU, try a stronger instruct model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9446030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- OpenAI backend (chat completions) ----\n",
    "def openai_generate_json(prompt: str) -> str:\n",
    "    \"\"\"Return raw JSON string from OpenAI Chat Completions.\"\"\"\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        resp = client.chat.completions.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            messages=[\n",
    "                {\"role\":\"system\",\"content\":\"You write grounded Q/A pairs in strict JSON.\"},\n",
    "                {\"role\":\"user\",\"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"OpenAI generation failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c1c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Hugging Face backend (text2text-generation) ----\n",
    "from transformers import pipeline\n",
    "\n",
    "_hf_pipe = None\n",
    "def hf_generate_json(prompt: str) -> str:\n",
    "    global _hf_pipe\n",
    "    if _hf_pipe is None:\n",
    "        _hf_pipe = pipeline(HF_TASK, model=HF_MODEL)\n",
    "    out = _hf_pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=HF_MAX_NEW_TOKENS,\n",
    "        do_sample=HF_DO_SAMPLE\n",
    "    )\n",
    "    # HF pipelines return a list of dicts; try the first item\n",
    "    text = out[0].get(\"generated_text\", \"\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1b2df3",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Robust JSON parsing of model output\n",
    "\n",
    "Models sometimes wrap JSON in prose or code fences. We defensively extract the first JSON array.\n",
    "If parsing fails, we fall back to a very simple regex that looks for `\"question\"` and `\"answer\"` pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b265f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def extract_json_array(text: str) -> List[Dict[str, Any]]:\n",
    "    # Try to find a JSON array block\n",
    "    # 1) Inline code fence\n",
    "    fence = re.findall(r\"```(?:json)?\\s*(\\[.*?\\])\\s*```\", text, flags=re.S)\n",
    "    candidates = fence if fence else re.findall(r\"(\\[\\s*{.*?}\\s*\\])\", text, flags=re.S)\n",
    "    if candidates:\n",
    "        raw = candidates[0]\n",
    "    else:\n",
    "        raw = text.strip()\n",
    "    try:\n",
    "        arr = json.loads(raw)\n",
    "        if isinstance(arr, list):\n",
    "            # sanitize items\n",
    "            clean = []\n",
    "            for it in arr:\n",
    "                if isinstance(it, dict) and 'question' in it and 'answer' in it:\n",
    "                    q = str(it['question']).strip()\n",
    "                    a = str(it['answer']).strip()\n",
    "                    if q and a:\n",
    "                        clean.append({'question': q, 'answer': a})\n",
    "            return clean\n",
    "        return []\n",
    "    except Exception:\n",
    "        # Fallback: scrape \"question\":\"...\", \"answer\":\"...\" naive pairs\n",
    "        pairs = []\n",
    "        pattern = re.findall(r'\"question\"\\s*:\\s*\"(.*?)\"\\s*,\\s*\"answer\"\\s*:\\s*\"(.*?)\"', text, flags=re.S)\n",
    "        for q,a in pattern:\n",
    "            q = q.strip().replace('\\n',' ')\n",
    "            a = a.strip().replace('\\n',' ')\n",
    "            if q and a:\n",
    "                pairs.append({'question': q, 'answer': a})\n",
    "        return pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68aa8ac",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Generate Q/A pairs for the sampled chunks\n",
    "\n",
    "We iterate over the sample, build a grounded prompt, call the selected backend, parse JSON,\n",
    "and collect **up to 5 Q/A** per chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16846e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "backend = \"openai\" if USE_OPENAI else \"hf\"\n",
    "print(f\"Using backend: {backend}\")\n",
    "\n",
    "rows = []\n",
    "for item in tqdm(SAMPLED, desc=\"Generating Q/A\"):\n",
    "    chunk_id = item[\"chunk_id\"]\n",
    "    text = item[\"text\"] or \"\"\n",
    "    meta = item[\"metadata\"] or {}\n",
    "\n",
    "    if not text.strip():\n",
    "        continue\n",
    "\n",
    "    prompt = build_prompt(text, MAX_Q_PER_CHUNK)\n",
    "    try:\n",
    "        if USE_OPENAI:  # prefer OpenAI if explicitly enabled\n",
    "            raw = openai_generate_json(prompt)\n",
    "        else:\n",
    "            raw = hf_generate_json(prompt)\n",
    "        qa_list = extract_json_array(raw)[:MAX_Q_PER_CHUNK]\n",
    "    except Exception as e:\n",
    "        qa_list = []\n",
    "        print(f\"[WARN] Generation failed for chunk {chunk_id}: {e}\")\n",
    "\n",
    "    preview = (text[:220] + \"…\") if len(text) > 220 else text\n",
    "    source = meta.get(\"source\") or meta.get(\"file_path\") or meta.get(\"url\") or \"\"\n",
    "\n",
    "    for qa in qa_list:\n",
    "        rows.append({\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"question\": qa[\"question\"],\n",
    "            \"answer\": qa[\"answer\"],\n",
    "            \"source\": source,\n",
    "            \"metadata\": json.dumps(meta, ensure_ascii=False),\n",
    "            \"chunk_preview\": preview\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"chunk_id\",\"question\",\"answer\",\"source\",\"metadata\",\"chunk_preview\"])\n",
    "print(f\"Generated {len(df)} Q/A rows.\")\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2facacce",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Save evaluation dataset to CSV\n",
    "\n",
    "This CSV can be loaded by your **RAG evaluation** notebook to compute **Recall@K, Precision@K, MAP, and MRR**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f90319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Saved evaluation dataset to: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90277448",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Tips & Next Steps\n",
    "\n",
    "- **Quality control:** Spot‑check a few rows to ensure questions are **answerable from the chunk** and answers are **faithful**.  \n",
    "- **Stronger models ⇒ better Q/A:** If using HF on CPU (e.g., `flan‑t5‑base`), consider upgrading to a stronger instruct model on GPU for higher‑quality questions.  \n",
    "- **De‑duplication:** Remove near‑duplicate questions across chunks to reduce evaluation bias.  \n",
    "- **Balance sampling:** You can stratify sampling by source/file/topic to ensure coverage.  \n",
    "- **Costs:** OpenAI usage scales with tokens; keep chunks truncated and sample fraction small to control spend.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

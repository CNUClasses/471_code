{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c3563c",
   "metadata": {},
   "source": [
    "\n",
    "# Evaluating a Hugging Face RAG Pipeline (Retrieve → Rerank) with ChromaDB\n",
    "\n",
    "**Audience:** 4th-year CS students  \n",
    "**Goal:** Evaluate a RAG pipeline that **retrieves** from ChromaDB and **reranks** with a cross-encoder (no generation).  \n",
    "We use the provided:\n",
    "- **API module:** `rag_pipeline_api.py` (week13)  \n",
    "- **Evaluation data:** `rag_eval_dataset.csv` (week13) — questions were generated from a random 10% sample of chunks in the same Chroma collection, so the relevant chunk **is** in Chroma.\n",
    "\n",
    "**Metrics you will compute:**\n",
    "- **Precision@5 / Precision@10** — How clean are the top results?\n",
    "- **Recall@5 / Recall@10** — Do we miss the relevant chunk?\n",
    "- **MAP (Mean Average Precision)** — Smooth ranking-quality measure across queries.\n",
    "- **MRR (Mean Reciprocal Rank)** — How early do we see the **first** relevant chunk?\n",
    "\n",
    "> We assume **one relevant chunk per question** (the original `chunk_id` in the CSV). If it doesn't appear among the top-K results, Precision/Recall@K are 0 for that K, AP=0, RR=0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6b11b",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup & Configuration\n",
    "\n",
    "- If needed, install dependencies (ChromaDB + SentenceTransformers + tqdm + matplotlib).\n",
    "- Set your **ChromaDB persist directory** and **collection name** so we can open the same DB that produced the evaluation CSV.\n",
    "- We will download the `rag_pipeline_api.py` and the `rag_eval_dataset.csv` from the course GitHub repo if not found locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fb95f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you need to install dependencies, uncomment and run:\n",
    "# !pip install -U chromadb sentence-transformers torch pandas numpy tqdm matplotlib requests\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Configuration (EDIT THESE) ----------\n",
    "# Location of your Chroma persistent store and the collection name you used.\n",
    "PERSIST_DIR = os.getenv(\"CHROMA_PERSIST_DIR\", \"./chroma\")   # e.g., \"/path/to/chroma\"\n",
    "COLLECTION  = os.getenv(\"CHROMA_COLLECTION\", \"my_chunks\")   # e.g., \"my_chunks\"\n",
    "\n",
    "# How many candidates to retrieve before reranking, and how many to keep.\n",
    "TOP_K_CANDIDATES = int(os.getenv(\"TOP_K_CANDIDATES\", \"50\"))\n",
    "TOP_K_RETURN     = int(os.getenv(\"TOP_K_RETURN\", \"10\"))\n",
    "\n",
    "# Optionally limit the number of evaluation questions (for quick runs).\n",
    "EVAL_LIMIT = int(os.getenv(\"EVAL_LIMIT\", \"200\"))  # set -1 for all\n",
    "\n",
    "# Remote resources (week13)\n",
    "RAG_API_URL   = \"https://raw.githubusercontent.com/CNUClasses/471_code/master/week13/rag_pipeline_api.py\"\n",
    "EVAL_CSV_URL  = \"https://raw.githubusercontent.com/CNUClasses/471_code/master/week13/rag_eval_dataset.csv\"\n",
    "\n",
    "# Local fallback paths\n",
    "RAG_API_LOCAL = Path(\"rag_pipeline_api.py\")\n",
    "EVAL_CSV_LOCAL = Path(\"rag_eval_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23281cd6",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Fetch the API helper and load the evaluation CSV\n",
    "\n",
    "- We will use `rag_pipeline_api.py` from **week13** (the same API used to build/serve your pipeline).  \n",
    "- The evaluation CSV contains (`chunk_id`, `question`, `answer`, `metadata`, …). We only need `question` and the ground-truth `chunk_id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5bb85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def ensure_file(local_path: Path, url: str):\n",
    "    if not local_path.exists():\n",
    "        print(f\"Downloading {url} → {local_path} ...\")\n",
    "        r = requests.get(url, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        local_path.write_bytes(r.content)\n",
    "    else:\n",
    "        print(f\"Found local file: {local_path}\")\n",
    "\n",
    "# Ensure the API module is available\n",
    "ensure_file(RAG_API_LOCAL, RAG_API_URL)\n",
    "\n",
    "# Import the helper (adds the directory to sys.path if needed)\n",
    "if str(Path(\".\").resolve()) not in sys.path:\n",
    "    sys.path.append(str(Path(\".\").resolve()))\n",
    "\n",
    "from rag_pipeline_api import load_chroma, query_reranked  # provided by week13\n",
    "\n",
    "# Ensure the evaluation CSV is available\n",
    "ensure_file(EVAL_CSV_LOCAL, EVAL_CSV_URL)\n",
    "\n",
    "# Load evaluation data\n",
    "eval_df = pd.read_csv(EVAL_CSV_LOCAL)\n",
    "# Minimal columns expected: chunk_id, question (answers present but not used for retrieval metrics)\n",
    "eval_df = eval_df[['chunk_id', 'question']].rename(columns={'chunk_id': 'relevant_chunk_id'})\n",
    "print(f\"Loaded {len(eval_df)} evaluation rows.\")\n",
    "eval_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d7647f",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Open Chroma and warm up models\n",
    "\n",
    "We open the **exact same** Chroma collection used when generating the dataset and load:\n",
    "- **Bi-encoder** (SentenceTransformers) for query embeddings (dense retrieval).\n",
    "- **Cross-encoder** for reranking top-N candidates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b63d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the collection + models (device can be \"cpu\", \"cuda\", or \"mps\")\n",
    "handle = load_chroma(\n",
    "    persist_dir=PERSIST_DIR,\n",
    "    collection_name=COLLECTION,\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",     # match ingestion embedder\n",
    "    reranker_model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",        # compact reranker\n",
    "    device=None                                                   # set \"cuda\" if you have a GPU\n",
    ")\n",
    "print(\"Chroma + models are ready. Embedding dim:\", handle.embed_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31865a8a",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Metric helpers\n",
    "\n",
    "We assume **one relevant chunk** per query: the `relevant_chunk_id`.  \n",
    "- **Precision@K**: fraction of the top-K results that are relevant (0 or 1/K here).  \n",
    "- **Recall@K**: fraction of relevant items retrieved in top-K (0 or 1 here).  \n",
    "- **AP**: mean of precisions at the ranks where relevant items appear (here, either 0 or 1/rank).  \n",
    "- **RR**: 1 divided by the rank of the first relevant item (or 0 if not found).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Set, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "def precision_at_k(ranked: List[str], relevant: Set[str], k: int) -> float:\n",
    "    top_k = ranked[:k]\n",
    "    hits = sum(1 for d in top_k if d in relevant)\n",
    "    return hits / float(k)\n",
    "\n",
    "def recall_at_k(ranked: List[str], relevant: Set[str], k: int) -> float:\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    top_k = ranked[:k]\n",
    "    hits = sum(1 for d in top_k if d in relevant)\n",
    "    return hits / float(len(relevant))\n",
    "\n",
    "def average_precision(ranked: List[str], relevant: Set[str]) -> float:\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    hits = 0\n",
    "    precisions = []\n",
    "    for i, doc_id in enumerate(ranked, start=1):\n",
    "        if doc_id in relevant:\n",
    "            hits += 1\n",
    "            precisions.append(hits / i)\n",
    "    return float(np.mean(precisions)) if precisions else 0.0\n",
    "\n",
    "def reciprocal_rank(ranked: List[str], relevant: Set[str]) -> float:\n",
    "    for i, doc_id in enumerate(ranked, start=1):\n",
    "        if doc_id in relevant:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def evaluate_one_query(ranked_ids: List[str], relevant_id: str) -> Dict[str, Any]:\n",
    "    rel = {str(relevant_id)}\n",
    "    metrics = {\n",
    "        \"P@5\":  precision_at_k(ranked_ids, rel, 5),\n",
    "        \"P@10\": precision_at_k(ranked_ids, rel, 10),\n",
    "        \"R@5\":  recall_at_k(ranked_ids, rel, 5),\n",
    "        \"R@10\": recall_at_k(ranked_ids, rel, 10),\n",
    "        \"AP\":   average_precision(ranked_ids, rel),\n",
    "        \"RR\":   reciprocal_rank(ranked_ids, rel),\n",
    "    }\n",
    "    # Report the (1-indexed) rank where the relevant id appears (np.inf if missing)\n",
    "    try:\n",
    "        pos = ranked_ids.index(str(relevant_id))\n",
    "        metrics[\"rank_of_relevant\"] = pos + 1\n",
    "    except ValueError:\n",
    "        metrics[\"rank_of_relevant\"] = float(\"inf\")\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ad2c3",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Evaluate the pipeline over the dataset\n",
    "\n",
    "For each **question**:\n",
    "1. Use `query_reranked(...)` to get **top-K reranked** chunk IDs from Chroma.  \n",
    "2. Compare with the ground-truth `relevant_chunk_id`.  \n",
    "3. Compute metrics **P@5, P@10, R@5, R@10, AP, RR**.  \n",
    "4. Aggregate across questions to report **MAP** and **MRR**.\n",
    "\n",
    "> For runtime, you can cap the number of evaluation rows by setting `EVAL_LIMIT` in the config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b449054",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "subset = eval_df if EVAL_LIMIT < 0 else eval_df.head(EVAL_LIMIT).copy()\n",
    "\n",
    "records = []\n",
    "for _, row in tqdm(subset.iterrows(), total=len(subset), desc=\"Evaluating\"):\n",
    "    q = str(row[\"question\"])\n",
    "    relevant_id = str(row[\"relevant_chunk_id\"])\n",
    "\n",
    "    # Retrieve→Rerank (top-K candidates, keep top-K return)\n",
    "    results = query_reranked(\n",
    "        handle,\n",
    "        query=q,\n",
    "        top_k_candidates=TOP_K_CANDIDATES,\n",
    "        top_k_return=TOP_K_RETURN\n",
    "    )\n",
    "\n",
    "    ranked_ids = [str(r[\"id\"]) for r in results]  # ensure string type for comparison\n",
    "    m = evaluate_one_query(ranked_ids, relevant_id)\n",
    "    records.append({\"question\": q, \"relevant_id\": relevant_id, **m})\n",
    "\n",
    "results_df = pd.DataFrame(records)\n",
    "print(\"Preview of per-query metrics:\")\n",
    "results_df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9634b2",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Aggregate scores (MAP, MRR, and mean P/R@K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d5886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAP = results_df[\"AP\"].mean()\n",
    "MRR = results_df[\"RR\"].mean()\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\"MAP\", \"MRR\", \"Precision@5 (mean)\", \"Precision@10 (mean)\", \"Recall@5 (mean)\", \"Recall@10 (mean)\"],\n",
    "    \"Score\":  [MAP, MRR, results_df[\"P@5\"].mean(), results_df[\"P@10\"].mean(), results_df[\"R@5\"].mean(), results_df[\"R@10\"].mean()]\n",
    "})\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6007560",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Diagnostic: rank of the relevant chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3655ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ranks = results_df[\"rank_of_relevant\"].replace(np.inf, np.nan).dropna()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(ranks, bins=20)\n",
    "plt.title(\"Rank of Relevant Chunk (1 = top)\")\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c198d2c",
   "metadata": {},
   "source": [
    "\n",
    "## 7) (Optional) Save detailed per-query results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8109cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OUT_CSV = \"rag_eval_per_query_metrics.csv\"\n",
    "results_df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved: {OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9adf6c",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Notes & Extensions\n",
    "\n",
    "- **Dense-only vs Reranked:** To compare, you can modify `rag_pipeline_api.py` to return **dense order** as well and compute metrics for both.  \n",
    "- **K sensitivity:** Try different `TOP_K_CANDIDATES` and `TOP_K_RETURN` to see how MRR/MAP shift (e.g., 20/10, 100/20).  \n",
    "- **Hybrid retrieval:** Add a BM25+dense **hybrid** stage then rerank to test if it improves early precision.  \n",
    "- **Speed:** Use `device=\"cuda\"` (or `\"mps\"`) for faster cross-encoder reranking.  \n",
    "- **Multiple relevants:** If your chunks overlap, consider labeling multiple relevant chunk IDs per question and adapt the metric functions to multi-relevant sets.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f76c6e",
   "metadata": {},
   "source": [
    "# Mining Hard Negatives for Asymmetric Search with ChromaDB\n",
    "<img src=\"https://raw.githubusercontent.com/CNUClasses/CPSC471/master/content/lectures/week13/stage3.png\" alt=\"standard\" style=\"max-height:300px;  margin:10px 0; vertical-align:middle;\">\n",
    "\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "\n",
    "1. Load an initial **training dataset** of question–chunk pairs from `rag_train_dataset.csv`.\n",
    "2. Use a **Hugging Face embedding model** (`sentence-transformers/msmarco-distilbert-cos-v5`) to:\n",
    "   - embed all `chunk_preview` texts,\n",
    "   - store them in a **ChromaDB** vector database.\n",
    "3. For each question, embed the question and query ChromaDB to **mine hard negatives**\n",
    "   (3–5 similar but incorrect chunks per question).\n",
    "4. Build a **new dataset** in triplet form:\n",
    "   - `question`\n",
    "   - `chunk` (the correct / positive chunk)\n",
    "   - `hard_negative_chunk` (one hard negative per row)\n",
    "\n",
    "This triplet dataset can be used later to fine-tune the embedding model with\n",
    "contrastive or triplet-style loss functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4834ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL: Install dependencies\n",
    "# ============================================================================\n",
    "# Uncomment and run this cell if you do NOT already have these libraries.\n",
    "# In many environments (e.g., Colab) you will need to install them first.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# !pip install -U chromadb transformers sentencepiece torch pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55ab8b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Imports and Configuration\n",
    "# ============================================================================\n",
    "# This section imports all the Python packages we will use and sets up\n",
    "# some basic configuration such as the model name and random seeds.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1'\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# torch.cuda.device_count() \n",
    "# torch.cuda.is_available()\n",
    "# torch.set_default_device('cuda:2')\n",
    "# device = torch.device('cuda:2')\n",
    "import utils as ut\n",
    "\n",
    "import random\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Set a random seed for reproducibility of any sampling we might do\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Device configuration: use GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Path to the initial training data CSV.\n",
    "# This CSV is expected to have at least:\n",
    "#   - a 'question' column\n",
    "#   - a 'chunk_preview' column\n",
    "CSV_PATH = \"rag_train_dataset.csv\"\n",
    "\n",
    "# Name of the Hugging Face embedding model we will use.\n",
    "# This is an asymmetric model trained for question -> passage retrieval.\n",
    "# use the fine tuned model to embed the query if pulling from a collection that was generated with the finetuned model\n",
    "# MODEL_NAME = \"sentence-transformers/msmarco-distilbert-cos-v5\"\n",
    "FINETUNED_MODEL_PATH = \"sentence-transformers/msmarco-distilbert-cos-v5\"\n",
    "\n",
    "# Number of hard negatives we aim to mine per question.\n",
    "# You can adjust this between 3 and 5 as desired.\n",
    "NUM_HARD_NEGATIVES = 5\n",
    "\n",
    "# How many nearest neighbors to retrieve when mining.\n",
    "# We retrieve more than we need so we can filter out the true positive and\n",
    "# still have enough candidates for hard negatives.\n",
    "K_RETRIEVE = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b50ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset shape: (607, 6)\n",
      "Columns: ['chunk_id', 'question', 'answer', 'source', 'metadata', 'chunk']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source</th>\n",
       "      <th>metadata</th>\n",
       "      <th>chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>What is Christopher Newport University known for?</td>\n",
       "      <td>Christopher Newport University is known for it...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What are some opportunities for students at Ch...</td>\n",
       "      <td>Students at Christopher Newport University can...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What programs does Christopher Newport Univers...</td>\n",
       "      <td>Christopher Newport University offers the Pres...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>What is unique about the student body at Chris...</td>\n",
       "      <td>The student body at Christopher Newport Univer...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>What can students expect from their professors...</td>\n",
       "      <td>Students at Christopher Newport University can...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_id                                           question  \\\n",
       "0         2  What is Christopher Newport University known for?   \n",
       "1         2  What are some opportunities for students at Ch...   \n",
       "2         2  What programs does Christopher Newport Univers...   \n",
       "3         2  What is unique about the student body at Chris...   \n",
       "4         2  What can students expect from their professors...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Christopher Newport University is known for it...   \n",
       "1  Students at Christopher Newport University can...   \n",
       "2  Christopher Newport University offers the Pres...   \n",
       "3  The student body at Christopher Newport Univer...   \n",
       "4  Students at Christopher Newport University can...   \n",
       "\n",
       "                                     source  \\\n",
       "0  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "1  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "2  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "3  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "4  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "\n",
       "                                            metadata  \\\n",
       "0  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "1  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "2  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "3  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "4  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "\n",
       "                                               chunk  \n",
       "0  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  \n",
       "1  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  \n",
       "2  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  \n",
       "3  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  \n",
       "4  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Load the Initial Training Data\n",
    "# ============================================================================\n",
    "# We now load `rag_train_dataset.csv` which should contain at least two columns:\n",
    "#   - 'question'       : the user query / question text\n",
    "#   - 'answer'         : the ground-truth answer text\n",
    "#   - 'chunk'          : the relevant passage / chunk text\n",
    "# \n",
    "# If your actual column names differ, update the code below accordingly.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Basic sanity checks and preview\n",
    "print(\"Loaded dataset shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# Show the first few rows so we can verify the structure\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26f47caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after dropping NaNs: (607, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source</th>\n",
       "      <th>metadata</th>\n",
       "      <th>chunk</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>What is Christopher Newport University known for?</td>\n",
       "      <td>Christopher Newport University is known for it...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>doc_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What are some opportunities for students at Ch...</td>\n",
       "      <td>Students at Christopher Newport University can...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>doc_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What programs does Christopher Newport Univers...</td>\n",
       "      <td>Christopher Newport University offers the Pres...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>doc_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>What is unique about the student body at Chris...</td>\n",
       "      <td>The student body at Christopher Newport Univer...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>doc_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>What can students expect from their professors...</td>\n",
       "      <td>Students at Christopher Newport University can...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>doc_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_id                                           question  \\\n",
       "0         2  What is Christopher Newport University known for?   \n",
       "1         2  What are some opportunities for students at Ch...   \n",
       "2         2  What programs does Christopher Newport Univers...   \n",
       "3         2  What is unique about the student body at Chris...   \n",
       "4         2  What can students expect from their professors...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Christopher Newport University is known for it...   \n",
       "1  Students at Christopher Newport University can...   \n",
       "2  Christopher Newport University offers the Pres...   \n",
       "3  The student body at Christopher Newport Univer...   \n",
       "4  Students at Christopher Newport University can...   \n",
       "\n",
       "                                     source  \\\n",
       "0  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "1  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "2  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "3  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "4  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "\n",
       "                                            metadata  \\\n",
       "0  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "1  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "2  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "3  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "4  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "\n",
       "                                               chunk doc_id  \n",
       "0  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  doc_0  \n",
       "1  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  doc_1  \n",
       "2  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  doc_2  \n",
       "3  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  doc_3  \n",
       "4  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  doc_4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Assign Unique IDs to Each Chunk\n",
    "# ============================================================================\n",
    "# ChromaDB requires each document / vector to have a unique string ID.\n",
    "# Here we create a simple 'doc_id' for each row based on its index.\n",
    "# We also make sure there are no missing values in the key columns.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Drop any rows where question or chunk_preview is missing to avoid errors\n",
    "df = df.dropna(subset=[\"question\", \"chunk\"]).reset_index(drop=True)\n",
    "\n",
    "# Create a unique string ID per row (used as the Chroma document ID)\n",
    "df[\"doc_id\"] = df.index.map(lambda i: f\"doc_{i}\")\n",
    "\n",
    "print(\"Dataset size after dropping NaNs:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "164b1252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "Max sequence length: 384\n",
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Load the Fine-Tuned Embedding Model\n",
    "# ============================================================================\n",
    "# We now load the fine-tuned SentenceTransformer model from the specified\n",
    "# local directory. This model will be used to compute new embeddings for\n",
    "# all documents in the ChromaDB collection.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Load the model from the local path\n",
    "model = SentenceTransformer(FINETUNED_MODEL_PATH)\n",
    "model.to(device)\n",
    "\n",
    "# Quick sanity check: print out the model's max sequence length and embedding dimension\n",
    "print(\"Model loaded from:\", FINETUNED_MODEL_PATH)\n",
    "print(\"Max sequence length:\", model.get_max_seq_length())\n",
    "print(\"Embedding dimension:\", model.get_sentence_embedding_dimension())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03f37a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================================\n",
    "# # Load the Embedding Model and Define Encoding Helpers\n",
    "# # ============================================================================\n",
    "# # We load the Hugging Face model and tokenizer, then define a small helper\n",
    "# # to convert raw text into normalized embeddings suitable for cosine similarity.\n",
    "# # \n",
    "# # The model 'sentence-transformers/msmarco-distilbert-cos-v5' is designed for\n",
    "# # asymmetric search (query vs passage), but we will use the same encoder for\n",
    "# # both questions and passages here.\n",
    "# # ----------------------------------------------------------------------------\n",
    "\n",
    "# # Load tokenizer and model from Hugging Face\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "# model.eval()  # put the model into evaluation mode (no dropout, etc.)\n",
    "\n",
    "# def mean_pooling(model_output, attention_mask):\n",
    "#     \"\"\"Perform mean pooling on the token embeddings.\n",
    "\n",
    "#     This function takes the model output (last_hidden_state) and an attention mask,\n",
    "#     and computes a single vector per sequence by averaging the embeddings for\n",
    "#     the tokens that are not masked.\n",
    "\n",
    "#     Args:\n",
    "#         model_output: Output object from the transformer model.\n",
    "#         attention_mask: Tensor of shape (batch_size, seq_len) indicating which\n",
    "#                         tokens are real (1) vs padding (0).\n",
    "\n",
    "#     Returns:\n",
    "#         A tensor of shape (batch_size, hidden_dim) containing the pooled embeddings.\n",
    "#     \"\"\"\n",
    "#     token_embeddings = model_output.last_hidden_state  # (batch_size, seq_len, hidden_dim)\n",
    "#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#     # Multiply token embeddings by mask, sum over sequence length, and divide by number of valid tokens\n",
    "#     pooled = (token_embeddings * input_mask_expanded).sum(dim=1) / torch.clamp(\n",
    "#         input_mask_expanded.sum(dim=1), min=1e-9\n",
    "#     )\n",
    "#     return pooled\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def encode_texts(texts: List[str], batch_size: int = 32, max_length: int = 256) -> torch.Tensor:\n",
    "#     \"\"\"Encode a list of texts into L2-normalized embeddings.\n",
    "\n",
    "#     This helper will:\n",
    "#       1. Tokenize the texts in mini-batches.\n",
    "#       2. Run them through the transformer model.\n",
    "#       3. Apply mean pooling.\n",
    "#       4. L2-normalize the resulting embeddings so cosine similarity corresponds\n",
    "#          to dot product.\n",
    "\n",
    "#     Args:\n",
    "#         texts: A list of strings to encode.\n",
    "#         batch_size: Batch size for encoding.\n",
    "#         max_length: Maximum number of tokens per sequence (longer texts are truncated).\n",
    "\n",
    "#     Returns:\n",
    "#         A tensor of shape (len(texts), hidden_dim) containing normalized embeddings.\n",
    "#     \"\"\"\n",
    "#     all_embeddings = []\n",
    "#     for start_idx in range(0, len(texts), batch_size):\n",
    "#         batch_texts = texts[start_idx:start_idx + batch_size]\n",
    "\n",
    "#         # Tokenize the batch of texts\n",
    "#         encoded = tokenizer(\n",
    "#             batch_texts,\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#             max_length=max_length,\n",
    "#             return_tensors=\"pt\"\n",
    "#         ).to(device)\n",
    "\n",
    "#         # Forward pass through the model\n",
    "#         model_output = model(**encoded)\n",
    "\n",
    "#         # Mean-pool the token embeddings\n",
    "#         pooled = mean_pooling(model_output, encoded[\"attention_mask\"])\n",
    "\n",
    "#         # L2-normalize the pooled embeddings along the feature dimension\n",
    "#         pooled = nn.functional.normalize(pooled, p=2, dim=-1)\n",
    "\n",
    "#         # Move to CPU to free GPU memory and append\n",
    "#         all_embeddings.append(pooled.cpu())\n",
    "\n",
    "#     # Concatenate all batches into a single tensor\n",
    "#     return torch.cat(all_embeddings, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91be8e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to ChromaDB with persist_directory = ../week12/rag_chroma\n",
      "Loaded source collection: cnu_rag_mnrl_lab\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Load the reembedded chroma database generated by \n",
    "# RAG3_reembed_chroma_with_finetuned_model.ipynb\n",
    "# ============================================================================\n",
    "# ---- User-editable parameters ----\n",
    "PERSIST_DIR = \"../week12/rag_chroma\"        # ChromaDB persistence path (folder will be created)\n",
    "# PERSIST_DIR = os.getenv(\"../week12\", \"./rag_chroma\")\n",
    "COLLECTION_NAME= \"cnu_rag_mnrl_lab\"                       # collection name used previously\n",
    "\n",
    "import chromadb\n",
    "\n",
    "# Create a ChromaDB client that uses the persistent directory\n",
    "client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "collection = client.get_collection(COLLECTION_NAME)\n",
    "\n",
    "print(\"Connected to ChromaDB with persist_directory =\", PERSIST_DIR)\n",
    "\n",
    "# Get the source collection which already exists and contains original embeddings\n",
    "source_collection = client.get_collection(COLLECTION_NAME)\n",
    "print(\"Loaded source collection:\", source_collection.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b7bb720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hard negative mining for 607 questions...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Processed 50 / 607 questions\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Processed 100 / 607 questions\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Processed 150 / 607 questions\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Processed 200 / 607 questions\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Processed 250 / 607 questions\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Processed 300 / 607 questions\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Processed 350 / 607 questions\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Processed 400 / 607 questions\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Processed 450 / 607 questions\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Processed 500 / 607 questions\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Processed 550 / 607 questions\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Processed 600 / 607 questions\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Encoded batch 1/1 (docs 0–1)\n",
      "Processed 607 / 607 questions\n",
      "Example row with hard negatives:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>chunk</th>\n",
       "      <th>hard_negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Christopher Newport University known for?</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>[9 \\n     \\n \\n \\n \\n \\n    \\n \\n  \\n  \\n \\n \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are some opportunities for students at Ch...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>[research interests. \\nChristopher Newport Uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What programs does Christopher Newport Univers...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>[perspectives. This interdisciplinarity of stu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is unique about the student body at Chris...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>[9 \\n     \\n \\n \\n \\n \\n    \\n \\n  \\n  \\n \\n \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What can students expect from their professors...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>[Program and Honors Program. Both are national...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is Christopher Newport University known for?   \n",
       "1  What are some opportunities for students at Ch...   \n",
       "2  What programs does Christopher Newport Univers...   \n",
       "3  What is unique about the student body at Chris...   \n",
       "4  What can students expect from their professors...   \n",
       "\n",
       "                                               chunk  \\\n",
       "0  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...   \n",
       "1  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...   \n",
       "2  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...   \n",
       "3  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...   \n",
       "4  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...   \n",
       "\n",
       "                                      hard_negatives  \n",
       "0  [9 \\n     \\n \\n \\n \\n \\n    \\n \\n  \\n  \\n \\n \\...  \n",
       "1  [research interests. \\nChristopher Newport Uni...  \n",
       "2  [perspectives. This interdisciplinarity of stu...  \n",
       "3  [9 \\n     \\n \\n \\n \\n \\n    \\n \\n  \\n  \\n \\n \\...  \n",
       "4  [Program and Honors Program. Both are national...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Mine Hard Negatives from ChromaDB\n",
    "# ============================================================================\n",
    "# For each question:\n",
    "#   1. Encode the question into an embedding.\n",
    "#   2. Use ChromaDB to retrieve the top-K most similar chunks.\n",
    "#   3. Exclude the *true positive* chunk for that question (its own 'doc_id').\n",
    "#   4. Take up to NUM_HARD_NEGATIVES of the remaining results as hard negatives.\n",
    "# \n",
    "# These are \"hard\" because they are semantically close to the question but are\n",
    "# NOT the correct chunk in our training data.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# We will store a list of hard negatives (as strings) per question\n",
    "all_hard_negatives: List[List[str]] = []\n",
    "\n",
    "questions = df[\"question\"].tolist()\n",
    "chunk_ids = df[\"chunk_id\"].tolist()\n",
    "\n",
    "print(\"Starting hard negative mining for\", len(questions), \"questions...\")\n",
    "\n",
    "for i, (question, chunk_id) in enumerate(zip(questions, chunk_ids)):\n",
    "    # Encode the single question into an embedding (shape: (1, hidden_dim))\n",
    "    q_emb = ut.encode_documents([question], model, batch_size=1)\n",
    "\n",
    "    # Query ChromaDB to get top K_RETRIEVE nearest chunks\n",
    "    # We pass the question embedding as the query embedding\n",
    "    results = collection.query(\n",
    "        query_embeddings=[q_emb[0]],\n",
    "        n_results=K_RETRIEVE,\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "\n",
    "    # Retrieve the lists of ids and documents from the query results\n",
    "    retrieved_ids = results[\"ids\"][0]         # list of doc_ids\n",
    "    retrieved_docs = results[\"documents\"][0]  # list of chunk texts\n",
    "\n",
    "    # Build a list of candidate hard negatives, skipping the true positive\n",
    "    hn_docs = []\n",
    "    for rid, rdoc in zip(retrieved_ids, retrieved_docs):\n",
    "        if rid == str(chunk_id):\n",
    "            # This is the true positive for this question; skip it\n",
    "            continue\n",
    "        hn_docs.append(rdoc)\n",
    "        # Stop once we have collected enough hard negatives\n",
    "        if len(hn_docs) >= NUM_HARD_NEGATIVES:\n",
    "            break\n",
    "\n",
    "    # If we did not find enough distinct hard negatives (e.g., small dataset),\n",
    "    # we simply keep as many as we found (could be fewer than NUM_HARD_NEGATIVES).\n",
    "    all_hard_negatives.append(hn_docs)\n",
    "\n",
    "    # Occasionally print progress so the user can see that it's working\n",
    "    if (i + 1) % 50 == 0 or (i + 1) == len(questions):\n",
    "        print(f\"Processed {i + 1} / {len(questions)} questions\")\n",
    "\n",
    "# Add the list of hard negatives as a new column in the original DataFrame.\n",
    "# Note: this column will hold lists of strings.\n",
    "df[\"hard_negatives\"] = all_hard_negatives\n",
    "\n",
    "print(\"Example row with hard negatives:\")\n",
    "df[[\"question\", \"chunk\", \"hard_negatives\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c1eae7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet dataset shape: (3035, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anchor</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Christopher Newport University known for?</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>9 \\n     \\n \\n \\n \\n \\n    \\n \\n  \\n  \\n \\n \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Christopher Newport University known for?</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>sailing, soccer, and tennis. Women compete in:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is Christopher Newport University known for?</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>research interests. \\nChristopher Newport Univ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is Christopher Newport University known for?</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>90 \\n \\n     \\n  \\n \\n   \\n \\n \\n \\n \\n \\n \\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Christopher Newport University known for?</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>39 \\n \\n \\n      \\n \\n    \\n \\n   \\n  \\n \\n   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              anchor  \\\n",
       "0  What is Christopher Newport University known for?   \n",
       "1  What is Christopher Newport University known for?   \n",
       "2  What is Christopher Newport University known for?   \n",
       "3  What is Christopher Newport University known for?   \n",
       "4  What is Christopher Newport University known for?   \n",
       "\n",
       "                                            positive  \\\n",
       "0  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...   \n",
       "1  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...   \n",
       "2  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...   \n",
       "3  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...   \n",
       "4  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...   \n",
       "\n",
       "                                            negative  \n",
       "0  9 \\n     \\n \\n \\n \\n \\n    \\n \\n  \\n  \\n \\n \\n...  \n",
       "1  sailing, soccer, and tennis. Women compete in:...  \n",
       "2  research interests. \\nChristopher Newport Univ...  \n",
       "3  90 \\n \\n     \\n  \\n \\n   \\n \\n \\n \\n \\n \\n \\n ...  \n",
       "4  39 \\n \\n \\n      \\n \\n    \\n \\n   \\n  \\n \\n   ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Build Triplet Dataset: (question, positive_chunk, hard_negative_chunk)\n",
    "# ============================================================================\n",
    "# Many fine-tuning setups (triplet loss, InfoNCE with explicit negatives) prefer\n",
    "# the data in \"long\" format, where each row is:\n",
    "#   - question\n",
    "#   - chunk_preview        (the correct / positive chunk)\n",
    "#   - hard_negative_chunk  (one hard negative)\n",
    "#\n",
    "# For each original row and each of its mined hard negatives, we will create a\n",
    "# separate row in a new DataFrame. If a question has N hard negatives, it will\n",
    "# produce N rows.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "triplet_rows = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    question_text = row[\"question\"]\n",
    "    positive_chunk = row[\"chunk\"]\n",
    "    hn_list = row[\"hard_negatives\"]  # list of hard negative strings\n",
    "\n",
    "    # For each hard negative, create a new triplet row\n",
    "    for hn_chunk in hn_list:\n",
    "        triplet_rows.append(\n",
    "            {\n",
    "                \"anchor\": question_text,\n",
    "                \"positive\": positive_chunk,\n",
    "                \"negative\": hn_chunk,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Build the new DataFrame from the list of triplet dicts\n",
    "triplet_df = pd.DataFrame(triplet_rows)\n",
    "\n",
    "print(\"Triplet dataset shape:\", triplet_df.shape)\n",
    "triplet_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f91b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved triplet dataset to: rag_train_with_hard_negatives_triplets.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Save the Triplet Dataset to CSV\n",
    "# ============================================================================\n",
    "# Finally, we save the newly constructed triplet dataset to disk.\n",
    "# This CSV can then be used in a separate training script / notebook to\n",
    "# fine-tune your embedding model using a triplet loss or multi-negative\n",
    "# contrastive loss with explicit hard negatives.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "OUTPUT_CSV = \"rag_train_with_hard_negatives_triplets.csv\"\n",
    "\n",
    "triplet_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Saved triplet dataset to: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffde4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

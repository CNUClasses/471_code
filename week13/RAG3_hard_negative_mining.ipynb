{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG Data Preparation: Hard Negative Mining with Vector Databases\n",
    "\n",
    "**Target Audience:** 4th Year CS Students\n",
    "**Prerequisites:** Basic Python, Pandas, and general understanding of machine learning concepts (vectors, similarity).\n",
    "\n",
    "---\n",
    "\n",
    "### Introduction: Why are we doing this?\n",
    "\n",
    "In a Retrieval-Augmented Generation (RAG) system, the goal is to find the most relevant "chunk" of text to answer a user's question. We typically use a **Bi-Encoder** model to do this. A Bi-Encoder turns the question into a vector, turns candidate chunks into vectors, and calculates cosine similarity to find the best match.\n",
    "\n",
    "To train a high-quality Bi-Encoder, we need good training data. The simplest data consists of **(Question, Positive Chunk)** pairs. The model learns that these two should be close together in vector space.\n",
    "\n",
    "However, models trained only on positives often struggle to distinguish between the *perfect* answer and a *somewhat related but incorrect* answer. \n",
    "\n",
    "### The Solution: Hard Negatives\n",
    "\n",
    "A **Hard Negative** is a chunk of text that is *semantically similar* to the question (it shares keywords, topic, etc.) but does **not** actually contain the answer. \n",
    "\n",
    "Think of it like creating a multiple-choice question:\n",
    "* **Question:** What is the capital of France?\n",
    "* **Positive:** Paris is the capital city of France.\n",
    "* **Easy Negative:** The speed of light is approx 300,000 km/s. (Obviously wrong, easy for model to distinguish).\n",
    "* **Hard Negative:** Lyon is a major historical city located in east-central France. (Related topic, mentions France, but is the wrong answer).\n",
    "\n",
    "By training the model with **Triplets (Question, Positive, Hard Negative)** and telling it "Make Q close to P, but push Q far away from HN", we significantly improve retrieval accuracy.\n",
    "\n",
    "### The Goal of this Notebook\n",
    "\n",
    "We will take a simple dataset of `(Question, Positive Chunk)` and automatically generate "Hard Negatives" by using an existing embedding model and a vector database (ChromaDB) to find distractors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Setup and Installation\n",
    "\n",
    "We need a few specific libraries used in modern NLP pipelines:\n",
    "1.  `sentence-transformers`: The standard library for using Hugging Face embedding models.\n",
    "2.  `chromadb`: An open-source, easy-to-use vector database for storing embeddings and performing similarity searches.\n",
    "3.  `pandas`: For data manipulation.\n",
    "4.  `tqdm`: For progress bars during long running operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    # Install necessary libraries if running in a new environment (like Colab)
    # %pip install pandas sentence-transformers chromadb tqdm
    \n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# Setting pandas display options to see full text chunks easily\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DUMMY DATA GENERATION (OPTIONAL) ---\n",
    "# If you do not have 'rag_train_dataset.csv', run this cell to create a dummy one for testing.\n",
    "# If you have the file, skip this cell.\n",
    "\n",
    "if not os.path.exists('rag_train_dataset.csv'):\n",
    "    print(\"Creating dummy dataset...\")\n",
    "    dummy_data = {\n",
    "        'question': [\n",
    "            \"What is photosynthesis?\",\n",
    "            \"How do catalysts work?\",\n",
    "            \"Define Newton's second law.\",\n",
    "            \"What is the function of mitochondria?\"\n",
    "        ],\n",
    "        'chunk_preview': [\n",
    "            \"Photosynthesis is the process used by plants to convert light energy into chemical energy.\",\n",
    "            \"Catalysts increase the rate of a chemical reaction by lowering the activation energy without being consumed.\",\n",
    "            \"Newton's second law states that Force equals mass times acceleration (F=ma).\",\n",
    "            \"Mitochondria are known as the powerhouse of the cell because they generate most of the cell's supply of ATP.\"\n",
    "        ]\n",
    "    }\n",
    "    # Add some extra "distractor" chunks that don't have direct questions paired with them yet\n",
    "    extra_chunks = [\n",
    "        \"Plants require water and carbon dioxide to survive.\",\n",
    "        \"Chemical reactions often involve changes in temperature.\",\n",
    "        \"Newton also formulated the law of universal gravitation.\",\n",
    "        \"The nucleus contains the cell's genetic material.\",\n",
    "        \"Cellular respiration is a metabolic process.\",\n",
    "        \"Speed is the rate at which an object covers distance.\"\n",
    "    ]\n",
    "    \n",
    "    df_dummy = pd.DataFrame(dummy_data)\n",
    "    # Append extra chunks as rows with empty questions just to populate our search database later\n",
    "    df_extra = pd.DataFrame({'question': [''] * len(extra_chunks), 'chunk_preview': extra_chunks})\n",
    "    df_final = pd.concat([df_dummy, df_extra], ignore_index=True)\n",
    "    \n",
    "    df_final.to_csv('rag_train_dataset.csv', index=False)\n",
    "    print(\"Dummy dataset created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load Initial Training Data\n",
    "\n",
    "We start with a CSV containing pairs of questions and the correct (positive) chunk that answers them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "input_csv = 'rag_train_dataset.csv'\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Filter out any rows used purely for filling the database (where question is empty)\n",
    "# We only want to mine negatives for actual question-positive pairs.\n",
    "training_pairs_df = df[df['question'] != ''].copy()\n",
    "\n",
    "print(f\"Loaded {len(training_pairs_df)} training pairs.\")\n",
    "display(training_pairs_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initialize the Embedding Model\n",
    "\n",
    "To find "similar but wrong" chunks, we need a way to measure similarity. We use a pre-trained embedding model. This model converts text into a dense vector (a list of numbers, usually 384 or 768 dimensions).\n",
    "\n",
    "We will use `all-MiniLM-L6-v2`. It's a small, fast, and reasonably accurate model commonly used as a baseline for retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model name. This will download from Hugging Face Hub if not cached.\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "# Initialize the embedding function wrapper for ChromaDB\n",
    "# This tells Chroma which model to use when we insert text or query text.\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
    "\n",
    "print(\"Embedding model initialized.\")\n",
    "\n",
    "# ----- EDUCATIONAL CHECKPOINT -----\n",
    "# Let's see what an embedding looks like.\n",
    "test_sentence = \"Where is the library?\"\n",
    "# We use the raw sentence transformer model just to inspect output here\n",
    "raw_model = SentenceTransformer(model_name)\n",
    "test_vector = raw_model.encode(test_sentence)\n",
    "\n",
    "print(f\"\\nVector shape for '{test_sentence}': {test_vector.shape}\")\n",
    "# Printing first 5 dimensions just to show it's a list of floats\n",
    "print(f\"First 5 dimensions: {test_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 & 4: Set up Vector Database (ChromaDB) and Index Chunks\n",
    "\n",
    "To find hard negatives, we need a \"pool\" of candidates to search through. This pool should consist of **all unique chunks available in our dataset**.\n",
    "\n",
    "We will:
    "1.  Identify all unique `chunk_preview` texts from the input CSV.\n",
    "2.  Initialize a ChromaDB client (in-memory for this example, so it resets every run).\n",
    "3.  Create a "Collection" (like a table in SQL).\n",
    "4.  Add all unique chunks to this collection. Chroma will automatically use the embedding function we defined earlier to vectorize them upon insertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    # 1. Get all unique chunks from the original full dataframe\n",
    # We want every possible passage available as a potential candidate.\n",
    "all_unique_chunks = df['chunk_preview'].unique().tolist()\n",
    "print(f\"Found {len(all_unique_chunks)} unique chunks to serve as our search pool.\")\n",
    "\n",
    "# 2. Initialize ChromaDB client\n",
    "# Using EphemeralClient means data is stored in RAM and lost when the kernel restarts.\n",
    "# For production, you would use PersistentClient.\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "\n",
    "# 3. Create a collection\n",
    "# Determine if collection already exists and delete it if so, to ensure a clean slate\n",
    "collection_name = \"hard_negative_mining_pool\"\n",
    "try:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "    print(f\"Deleted existing collection: {collection_name}\")\n",
    "except ValueError:\n",
    "    pass # Collection didn't exist\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=embedding_func # Pass the embedding function here\n",
    ")\n",
    "\n",
    "print(\"\\nPopulating ChromaDB collection... this might take a moment depending on data size.\")\n",
    "\n",
    "# 4. Add documents to the collection\n",
    "# Chroma needs IDs for every document. We'll use simple string indices.\n",
    "ids = [str(i) for i in range(len(all_unique_chunks))]\n",
    "\n",
    "collection.add(\n",
    "    documents=all_unique_chunks,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "print(f\"Successfully added {collection.count()} documents to ChromaDB collection '{collection_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: The Mining Process (Core Logic)\n",
    "\n",
    "Now for the main event. We will iterate through every `(Question, Positive)` pair in our `training_pairs_df`.\n",
    "\n",
    "For each pair, the logic is:\n",
    "1.  Take the **Question**.\n",
    "2.  Query ChromaDB with this Question to find the top $N$ most similar chunks (e.g., top 10). Chroma handles embedding the question implicitly.\n",
    "3.  Look at the results. The **Positive Chunk** *should* be rank 1 (or very high up), because we are using the same model that created the embeddings.\n",
    "4.  Filter out the Positive Chunk from the results list.\n",
    "5.  The remaining top results are high-similarity chunks that are *not* the correct answer. These are our **Hard Negatives**.\n",
    "6.  Grab the top 3-5 of these hard negatives and store them.\n",
    "\n",
    "*(Note: In a real-world scenario where your initial positive pairs are messy, the positive might not be rank 1. But for mining hard negatives, we assume the provided positive is the "ground truth" we want to protect, and everything else similar is a distractor.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration used for mining\n",
    "NUM_CANDIDATES_TO_RETRIEVE = 10  # How many results to fetch initially from DB\n",
    "NUM_HARD_NEGATIVES_TO_KEEP = 4   # How many HN we want per question (target 3-5)\n",
    "\n",
    "triplet_data = []\n",
    "\n",
    "print(\"Starting Hard Negative Mining...\")\n",
    "\n",
    "# Iterate over each row in our training pairs dataframe using tqdm for a progress bar\n",
    "for index, row in tqdm(training_pairs_df.iterrows(), total=len(training_pairs_df)):\n",
    "    \n",
    "    question = row['question']\n",
    "    positive_chunk = row['chunk_preview']\n",
    "    \n",
    "    # --- 1. Query Vector DB ---\n",
    "    # We query the collection using the question text.\n",
    "    # n_results determines how many nearest neighbors to retrieve.\n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=NUM_CANDIDATES_TO_RETRIEVE\n",
    "    )\n",
    "    \n",
    "    # Chroma returns a dictionary of lists (because you can query multiple texts at once).\n",
    "    # Since we queried one text, we look at index 0.\n",
    "    retrieved_documents = results['documents'][0]\n",
    "    # retrieved_distances = results['distances'][0] # Useful if you want to filter by score threshold later\n",
    "    \n",
    "    # --- 2. Identify and Filter Hard Negatives ---\n",
    "    hard_negatives_found = []\n",
    "    \n",
    "    for retrieved_doc in retrieved_documents:\n",
    "        # CRITICAL STEP: The definition of a negative is that it is NOT the positive.\n",
    "        # We perform an exact string comparison here to ensure we don't include the positive as a negative.\n",
    "        # (In production, you might compare IDs if available, but text comparison works for this demo)\n",
    "        if retrieved_doc != positive_chunk:\n",
    "            hard_negatives_found.append(retrieved_doc)\n",
    "            \n",
    "        # Stop once we have enough hard negatives for this question\n",
    "        if len(hard_negatives_found) >= NUM_HARD_NEGATIVES_TO_KEEP:\n",
    "            break\n",
    "            \n",
    "    # --- 3. Store the Triplet Data ---\n",
    "    # Only add if we found at least one hard negative (good practice, though rare with a decent pool size)\n",
    "    if hard_negatives_found:\n",
    "        # We store the list of hard negatives as a single string representation \n",
    "        # or keep it as a list if exporting to JSON later. For CSV, string representation is usually easier.\n",
    "        triplet_data.append({\n",
    "            'question': question,\n",
    "            'positive_chunk': positive_chunk,\n",
    "            # Storing as a list for now, easier to inspect in Pandas\n",
    "            'hard_negative_chunks': hard_negatives_found \n",
    "        })\n",
    "\n",
    "print(f\"\\nMining complete. Generated triplets for {len(triplet_data)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Review and Save the New Dataset\n",
    "\n",
    "Let's look at the results. We now have a dataset where every row contains the question, the right answer, and a list of highly confusing wrong answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    # Convert results to DataFrame\n",
    "triplet_df = pd.DataFrame(triplet_data)\n",
    "\n",
    "# Display a sample row to verify the result\n",
    "print(\"Sample Triplet Entry:\")\n",
    "# Using .iloc[0] to show the first row formatted nicely\n",
    "sample_row = triplet_df.iloc[0]\n",
    "print(f\"Question: {sample_row['question']}\\n\")\n",
    "print(f\"Positive: {sample_row['positive_chunk']}\\n\")\n",
    "print(\"Hard Negatives Found:\")\n",
    "for i, hn in enumerate(sample_row['hard_negative_chunks']):\n",
    "    print(f\"{i+1}. {hn}\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# Show the dataframe head\n",
    "display(triplet_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Step: Save to CSV for later training\n",
    "output_filename = 'rag_triplet_dataset_with_hn.csv'\n",
    "\n",
    "# When saving lists to CSV, pandas often saves them as string representations of lists.\n",
    "# This is okay, but requires careful parsing during the actual training phase.\n",
    "triplet_df.to_csv(output_filename, index=False)\n",
    "print(f\"Final dataset saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "You have successfully generated a dataset ready for training a powerful RAG Bi-Encoder using **Triplet Loss** (or similar contrastive loss functions). \n",
    "\n",
    "By providing the model with examples of what it should *avoid* (the hard negatives), you force it to learn finer-grained semantic distinctions, ultimately leading to better retrieval performance in your final application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
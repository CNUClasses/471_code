{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f76c6e",
   "metadata": {},
   "source": [
    "# Mining Hard Negatives for Asymmetric Search with ChromaDB\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "\n",
    "1. Load an initial **training dataset** of question–chunk pairs from `rag_train_dataset.csv`.\n",
    "2. Use a **Hugging Face embedding model** (`sentence-transformers/msmarco-distilbert-cos-v5`) to:\n",
    "   - embed all `chunk_preview` texts,\n",
    "   - store them in a **ChromaDB** vector database.\n",
    "3. For each question, embed the question and query ChromaDB to **mine hard negatives**\n",
    "   (3–5 similar but incorrect chunks per question).\n",
    "4. Build a **new dataset** in triplet form:\n",
    "   - `question`\n",
    "   - `chunk_preview` (the correct / positive chunk)\n",
    "   - `hard_negative_chunk` (one hard negative per row)\n",
    "\n",
    "This triplet dataset can be used later to fine-tune the embedding model with\n",
    "contrastive or triplet-style loss functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4834ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL: Install dependencies\n",
    "# ============================================================================\n",
    "# Uncomment and run this cell if you do NOT already have these libraries.\n",
    "# In many environments (e.g., Colab) you will need to install them first.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# !pip install -U chromadb transformers sentencepiece torch pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ab8b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Imports and Configuration\n",
    "# ============================================================================\n",
    "# This section imports all the Python packages we will use and sets up\n",
    "# some basic configuration such as the model name and random seeds.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Set a random seed for reproducibility of any sampling we might do\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Device configuration: use GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Path to the initial training data CSV.\n",
    "# This CSV is expected to have at least:\n",
    "#   - a 'question' column\n",
    "#   - a 'chunk_preview' column\n",
    "CSV_PATH = \"rag_train_dataset.csv\"\n",
    "\n",
    "# Name of the Hugging Face embedding model we will use.\n",
    "# This is an asymmetric model trained for question -> passage retrieval.\n",
    "MODEL_NAME = \"sentence-transformers/msmarco-distilbert-cos-v5\"\n",
    "\n",
    "# Number of hard negatives we aim to mine per question.\n",
    "# You can adjust this between 3 and 5 as desired.\n",
    "NUM_HARD_NEGATIVES = 5\n",
    "\n",
    "# How many nearest neighbors to retrieve when mining.\n",
    "# We retrieve more than we need so we can filter out the true positive and\n",
    "# still have enough candidates for hard negatives.\n",
    "K_RETRIEVE = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b50ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Load the Initial Training Data\n",
    "# ============================================================================\n",
    "# We now load `rag_train_dataset.csv` which should contain at least two columns:\n",
    "#   - 'question'       : the user query / question text\n",
    "#   - 'chunk_preview'  : the relevant passage / chunk text\n",
    "# \n",
    "# If your actual column names differ, update the code below accordingly.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Basic sanity checks and preview\n",
    "print(\"Loaded dataset shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# Show the first few rows so we can verify the structure\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f47caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Assign Unique IDs to Each Chunk\n",
    "# ============================================================================\n",
    "# ChromaDB requires each document / vector to have a unique string ID.\n",
    "# Here we create a simple 'doc_id' for each row based on its index.\n",
    "# We also make sure there are no missing values in the key columns.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Drop any rows where question or chunk_preview is missing to avoid errors\n",
    "df = df.dropna(subset=[\"question\", \"chunk_preview\"]).reset_index(drop=True)\n",
    "\n",
    "# Create a unique string ID per row (used as the Chroma document ID)\n",
    "df[\"doc_id\"] = df.index.map(lambda i: f\"doc_{i}\")\n",
    "\n",
    "print(\"Dataset size after dropping NaNs:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f37a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Load the Embedding Model and Define Encoding Helpers\n",
    "# ============================================================================\n",
    "# We load the Hugging Face model and tokenizer, then define a small helper\n",
    "# to convert raw text into normalized embeddings suitable for cosine similarity.\n",
    "# \n",
    "# The model 'sentence-transformers/msmarco-distilbert-cos-v5' is designed for\n",
    "# asymmetric search (query vs passage), but we will use the same encoder for\n",
    "# both questions and passages here.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Load tokenizer and model from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()  # put the model into evaluation mode (no dropout, etc.)\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"Perform mean pooling on the token embeddings.\n",
    "\n",
    "    This function takes the model output (last_hidden_state) and an attention mask,\n",
    "    and computes a single vector per sequence by averaging the embeddings for\n",
    "    the tokens that are not masked.\n",
    "\n",
    "    Args:\n",
    "        model_output: Output object from the transformer model.\n",
    "        attention_mask: Tensor of shape (batch_size, seq_len) indicating which\n",
    "                        tokens are real (1) vs padding (0).\n",
    "\n",
    "    Returns:\n",
    "        A tensor of shape (batch_size, hidden_dim) containing the pooled embeddings.\n",
    "    \"\"\"\n",
    "    token_embeddings = model_output.last_hidden_state  # (batch_size, seq_len, hidden_dim)\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    # Multiply token embeddings by mask, sum over sequence length, and divide by number of valid tokens\n",
    "    pooled = (token_embeddings * input_mask_expanded).sum(dim=1) / torch.clamp(\n",
    "        input_mask_expanded.sum(dim=1), min=1e-9\n",
    "    )\n",
    "    return pooled\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_texts(texts: List[str], batch_size: int = 32, max_length: int = 256) -> torch.Tensor:\n",
    "    \"\"\"Encode a list of texts into L2-normalized embeddings.\n",
    "\n",
    "    This helper will:\n",
    "      1. Tokenize the texts in mini-batches.\n",
    "      2. Run them through the transformer model.\n",
    "      3. Apply mean pooling.\n",
    "      4. L2-normalize the resulting embeddings so cosine similarity corresponds\n",
    "         to dot product.\n",
    "\n",
    "    Args:\n",
    "        texts: A list of strings to encode.\n",
    "        batch_size: Batch size for encoding.\n",
    "        max_length: Maximum number of tokens per sequence (longer texts are truncated).\n",
    "\n",
    "    Returns:\n",
    "        A tensor of shape (len(texts), hidden_dim) containing normalized embeddings.\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    for start_idx in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[start_idx:start_idx + batch_size]\n",
    "\n",
    "        # Tokenize the batch of texts\n",
    "        encoded = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        model_output = model(**encoded)\n",
    "\n",
    "        # Mean-pool the token embeddings\n",
    "        pooled = mean_pooling(model_output, encoded[\"attention_mask\"])\n",
    "\n",
    "        # L2-normalize the pooled embeddings along the feature dimension\n",
    "        pooled = nn.functional.normalize(pooled, p=2, dim=-1)\n",
    "\n",
    "        # Move to CPU to free GPU memory and append\n",
    "        all_embeddings.append(pooled.cpu())\n",
    "\n",
    "    # Concatenate all batches into a single tensor\n",
    "    return torch.cat(all_embeddings, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be8e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Initialize ChromaDB and Create a Collection\n",
    "# ============================================================================\n",
    "# Here we create an in-memory ChromaDB client and a collection that will store\n",
    "# our chunk embeddings. We are NOT using an embedding function inside Chroma,\n",
    "# because we want full control over how we embed with our Hugging Face model.\n",
    "# Instead, we will compute embeddings ourselves and pass them to Chroma.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Create an in-memory Chroma client (no persistent storage by default)\n",
    "client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "\n",
    "# Name of the collection for our chunks\n",
    "COLLECTION_NAME = \"rag_chunks\"\n",
    "\n",
    "# If a collection with this name already exists, you might want to delete it or reuse it.\n",
    "# To be safe in repeated runs, we can try to get and delete it first.\n",
    "try:\n",
    "    client.delete_collection(COLLECTION_NAME)\n",
    "except Exception:\n",
    "    # It's fine if the collection does not exist yet\n",
    "    pass\n",
    "\n",
    "# Create a fresh collection\n",
    "collection = client.create_collection(name=COLLECTION_NAME)\n",
    "print(\"Created ChromaDB collection:\", collection.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f986022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Embed All chunk_preview Texts and Add Them to ChromaDB\n",
    "# ============================================================================\n",
    "# We now:\n",
    "#   1. Extract the 'chunk_preview' text for each row.\n",
    "#   2. Encode them into embeddings using our Hugging Face model.\n",
    "#   3. Add the embeddings and associated metadata (e.g., question) into ChromaDB.\n",
    "# \n",
    "# This allows us to later query the collection with question embeddings to find\n",
    "# similar chunks, which we will treat as candidate hard negatives.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Extract the list of chunk texts and corresponding IDs\n",
    "chunk_texts = df[\"chunk_preview\"].tolist()\n",
    "chunk_ids = df[\"doc_id\"].tolist()\n",
    "\n",
    "print(\"Number of chunks to embed:\", len(chunk_texts))\n",
    "\n",
    "# Encode all chunk texts into normalized embeddings\n",
    "chunk_embeddings = encode_texts(chunk_texts, batch_size=32, max_length=256)\n",
    "print(\"Chunk embeddings shape:\", chunk_embeddings.shape)\n",
    "\n",
    "# Convert embeddings to Python lists of floats for ChromaDB\n",
    "chunk_embeddings_list = chunk_embeddings.tolist()\n",
    "\n",
    "# Optionally store some metadata for each document, such as the original question\n",
    "metadatas = [{\"question\": q} for q in df[\"question\"].tolist()]\n",
    "\n",
    "# Add all documents, embeddings, and metadata to the Chroma collection\n",
    "collection.add(\n",
    "    ids=chunk_ids,\n",
    "    documents=chunk_texts,\n",
    "    embeddings=chunk_embeddings_list,\n",
    "    metadatas=metadatas,\n",
    ")\n",
    "\n",
    "print(\"Added documents to ChromaDB collection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7bb720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Mine Hard Negatives from ChromaDB\n",
    "# ============================================================================\n",
    "# For each question:\n",
    "#   1. Encode the question into an embedding.\n",
    "#   2. Use ChromaDB to retrieve the top-K most similar chunks.\n",
    "#   3. Exclude the *true positive* chunk for that question (its own 'doc_id').\n",
    "#   4. Take up to NUM_HARD_NEGATIVES of the remaining results as hard negatives.\n",
    "# \n",
    "# These are \"hard\" because they are semantically close to the question but are\n",
    "# NOT the correct chunk in our training data.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# We will store a list of hard negatives (as strings) per question\n",
    "all_hard_negatives: List[List[str]] = []\n",
    "\n",
    "questions = df[\"question\"].tolist()\n",
    "pos_ids = df[\"doc_id\"].tolist()\n",
    "\n",
    "print(\"Starting hard negative mining for\", len(questions), \"questions...\")\n",
    "\n",
    "for i, (question, pos_id) in enumerate(zip(questions, pos_ids)):\n",
    "    # Encode the single question into an embedding (shape: (1, hidden_dim))\n",
    "    q_emb = encode_texts([question], batch_size=1, max_length=256)\n",
    "    q_emb_list = q_emb[0].tolist()  # Convert to a plain list for ChromaDB\n",
    "\n",
    "    # Query ChromaDB to get top K_RETRIEVE nearest chunks\n",
    "    # We pass the question embedding as the query embedding\n",
    "    results = collection.query(\n",
    "        query_embeddings=[q_emb_list],\n",
    "        n_results=K_RETRIEVE,\n",
    "        include=[\"documents\", \"ids\"]\n",
    "    )\n",
    "\n",
    "    # Retrieve the lists of ids and documents from the query results\n",
    "    retrieved_ids = results[\"ids\"][0]         # list of doc_ids\n",
    "    retrieved_docs = results[\"documents\"][0]  # list of chunk texts\n",
    "\n",
    "    # Build a list of candidate hard negatives, skipping the true positive\n",
    "    hn_docs = []\n",
    "    for rid, rdoc in zip(retrieved_ids, retrieved_docs):\n",
    "        if rid == pos_id:\n",
    "            # This is the true positive for this question; skip it\n",
    "            continue\n",
    "        hn_docs.append(rdoc)\n",
    "        # Stop once we have collected enough hard negatives\n",
    "        if len(hn_docs) >= NUM_HARD_NEGATIVES:\n",
    "            break\n",
    "\n",
    "    # If we did not find enough distinct hard negatives (e.g., small dataset),\n",
    "    # we simply keep as many as we found (could be fewer than NUM_HARD_NEGATIVES).\n",
    "    all_hard_negatives.append(hn_docs)\n",
    "\n",
    "    # Occasionally print progress so the user can see that it's working\n",
    "    if (i + 1) % 50 == 0 or (i + 1) == len(questions):\n",
    "        print(f\"Processed {i + 1} / {len(questions)} questions\")\n",
    "\n",
    "# Add the list of hard negatives as a new column in the original DataFrame.\n",
    "# Note: this column will hold lists of strings.\n",
    "df[\"hard_negatives\"] = all_hard_negatives\n",
    "\n",
    "print(\"Example row with hard negatives:\")\n",
    "df[[\"question\", \"chunk_preview\", \"hard_negatives\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1eae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Build Triplet Dataset: (question, positive_chunk, hard_negative_chunk)\n",
    "# ============================================================================\n",
    "# Many fine-tuning setups (triplet loss, InfoNCE with explicit negatives) prefer\n",
    "# the data in \"long\" format, where each row is:\n",
    "#   - question\n",
    "#   - chunk_preview        (the correct / positive chunk)\n",
    "#   - hard_negative_chunk  (one hard negative)\n",
    "#\n",
    "# For each original row and each of its mined hard negatives, we will create a\n",
    "# separate row in a new DataFrame. If a question has N hard negatives, it will\n",
    "# produce N rows.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "triplet_rows = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    question_text = row[\"question\"]\n",
    "    positive_chunk = row[\"chunk_preview\"]\n",
    "    hn_list = row[\"hard_negatives\"]  # list of hard negative strings\n",
    "\n",
    "    # For each hard negative, create a new triplet row\n",
    "    for hn_chunk in hn_list:\n",
    "        triplet_rows.append(\n",
    "            {\n",
    "                \"question\": question_text,\n",
    "                \"positive_chunk\": positive_chunk,\n",
    "                \"hard_negative_chunk\": hn_chunk,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Build the new DataFrame from the list of triplet dicts\n",
    "triplet_df = pd.DataFrame(triplet_rows)\n",
    "\n",
    "print(\"Triplet dataset shape:\", triplet_df.shape)\n",
    "triplet_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f91b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Save the Triplet Dataset to CSV\n",
    "# ============================================================================\n",
    "# Finally, we save the newly constructed triplet dataset to disk.\n",
    "# This CSV can then be used in a separate training script / notebook to\n",
    "# fine-tune your embedding model using a triplet loss or multi-negative\n",
    "# contrastive loss with explicit hard negatives.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "OUTPUT_CSV = \"rag_train_with_hard_negatives_triplets.csv\"\n",
    "\n",
    "triplet_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Saved triplet dataset to: {OUTPUT_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

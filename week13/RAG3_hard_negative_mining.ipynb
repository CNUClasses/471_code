{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f76c6e",
   "metadata": {},
   "source": [
    "# Mining Hard Negatives for Asymmetric Search with ChromaDB\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "\n",
    "1. Load an initial **training dataset** of question–chunk pairs from `rag_train_dataset.csv`.\n",
    "2. Use a **Hugging Face embedding model** (`sentence-transformers/msmarco-distilbert-cos-v5`) to:\n",
    "   - embed all `chunk_preview` texts,\n",
    "   - store them in a **ChromaDB** vector database.\n",
    "3. For each question, embed the question and query ChromaDB to **mine hard negatives**\n",
    "   (3–5 similar but incorrect chunks per question).\n",
    "4. Build a **new dataset** in triplet form:\n",
    "   - `question`\n",
    "   - `chunk_preview` (the correct / positive chunk)\n",
    "   - `hard_negative_chunk` (one hard negative per row)\n",
    "\n",
    "This triplet dataset can be used later to fine-tune the embedding model with\n",
    "contrastive or triplet-style loss functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4834ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL: Install dependencies\n",
    "# ============================================================================\n",
    "# Uncomment and run this cell if you do NOT already have these libraries.\n",
    "# In many environments (e.g., Colab) you will need to install them first.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# !pip install -U chromadb transformers sentencepiece torch pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ab8b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Imports and Configuration\n",
    "# ============================================================================\n",
    "# This section imports all the Python packages we will use and sets up\n",
    "# some basic configuration such as the model name and random seeds.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1,2,3'\n",
    "import torch\n",
    "torch.cuda.device_count() \n",
    "torch.cuda.is_available()\n",
    "torch.set_default_device('cuda:2')\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "import random\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Set a random seed for reproducibility of any sampling we might do\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Device configuration: use GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Path to the initial training data CSV.\n",
    "# This CSV is expected to have at least:\n",
    "#   - a 'question' column\n",
    "#   - a 'chunk_preview' column\n",
    "CSV_PATH = \"rag_train_dataset.csv\"\n",
    "\n",
    "# Name of the Hugging Face embedding model we will use.\n",
    "# This is an asymmetric model trained for question -> passage retrieval.\n",
    "MODEL_NAME = \"sentence-transformers/msmarco-distilbert-cos-v5\"\n",
    "\n",
    "# Number of hard negatives we aim to mine per question.\n",
    "# You can adjust this between 3 and 5 as desired.\n",
    "NUM_HARD_NEGATIVES = 5\n",
    "\n",
    "# How many nearest neighbors to retrieve when mining.\n",
    "# We retrieve more than we need so we can filter out the true positive and\n",
    "# still have enough candidates for hard negatives.\n",
    "K_RETRIEVE = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82b50ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset shape: (607, 6)\n",
      "Columns: ['chunk_id', 'question', 'answer', 'source', 'metadata', 'chunk']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source</th>\n",
       "      <th>metadata</th>\n",
       "      <th>chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>What is Christopher Newport University known for?</td>\n",
       "      <td>Christopher Newport University is known for it...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What are some opportunities for students at Ch...</td>\n",
       "      <td>Students at Christopher Newport University can...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What programs does Christopher Newport Univers...</td>\n",
       "      <td>Christopher Newport University offers the Pres...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>What is unique about the student body at Chris...</td>\n",
       "      <td>The student body at Christopher Newport Univer...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>What can students expect from their professors...</td>\n",
       "      <td>Students at Christopher Newport University can...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_id                                           question  \\\n",
       "0         2  What is Christopher Newport University known for?   \n",
       "1         2  What are some opportunities for students at Ch...   \n",
       "2         2  What programs does Christopher Newport Univers...   \n",
       "3         2  What is unique about the student body at Chris...   \n",
       "4         2  What can students expect from their professors...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Christopher Newport University is known for it...   \n",
       "1  Students at Christopher Newport University can...   \n",
       "2  Christopher Newport University offers the Pres...   \n",
       "3  The student body at Christopher Newport Univer...   \n",
       "4  Students at Christopher Newport University can...   \n",
       "\n",
       "                                     source  \\\n",
       "0  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "1  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "2  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "3  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "4  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "\n",
       "                                            metadata  \\\n",
       "0  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "1  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "2  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "3  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "4  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "\n",
       "                                               chunk  \n",
       "0  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  \n",
       "1  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  \n",
       "2  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  \n",
       "3  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  \n",
       "4  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Load the Initial Training Data\n",
    "# ============================================================================\n",
    "# We now load `rag_train_dataset.csv` which should contain at least two columns:\n",
    "#   - 'question'       : the user query / question text\n",
    "#   - 'chunk_preview'  : the relevant passage / chunk text\n",
    "# \n",
    "# If your actual column names differ, update the code below accordingly.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Basic sanity checks and preview\n",
    "print(\"Loaded dataset shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# Show the first few rows so we can verify the structure\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26f47caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after dropping NaNs: (607, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source</th>\n",
       "      <th>metadata</th>\n",
       "      <th>chunk</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>What is Christopher Newport University known for?</td>\n",
       "      <td>Christopher Newport University is known for it...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>doc_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What are some opportunities for students at Ch...</td>\n",
       "      <td>Students at Christopher Newport University can...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>doc_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What programs does Christopher Newport Univers...</td>\n",
       "      <td>Christopher Newport University offers the Pres...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>doc_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>What is unique about the student body at Chris...</td>\n",
       "      <td>The student body at Christopher Newport Univer...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>doc_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>What can students expect from their professors...</td>\n",
       "      <td>Students at Christopher Newport University can...</td>\n",
       "      <td>./pdfs/2025-26-undergraduate_catalog.pdf</td>\n",
       "      <td>{\"source\": \"./pdfs/2025-26-undergraduate_catal...</td>\n",
       "      <td>2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>doc_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_id                                           question  \\\n",
       "0         2  What is Christopher Newport University known for?   \n",
       "1         2  What are some opportunities for students at Ch...   \n",
       "2         2  What programs does Christopher Newport Univers...   \n",
       "3         2  What is unique about the student body at Chris...   \n",
       "4         2  What can students expect from their professors...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Christopher Newport University is known for it...   \n",
       "1  Students at Christopher Newport University can...   \n",
       "2  Christopher Newport University offers the Pres...   \n",
       "3  The student body at Christopher Newport Univer...   \n",
       "4  Students at Christopher Newport University can...   \n",
       "\n",
       "                                     source  \\\n",
       "0  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "1  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "2  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "3  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "4  ./pdfs/2025-26-undergraduate_catalog.pdf   \n",
       "\n",
       "                                            metadata  \\\n",
       "0  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "1  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "2  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "3  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "4  {\"source\": \"./pdfs/2025-26-undergraduate_catal...   \n",
       "\n",
       "                                               chunk doc_id  \n",
       "0  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  doc_0  \n",
       "1  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  doc_1  \n",
       "2  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  doc_2  \n",
       "3  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  doc_3  \n",
       "4  2 \\n      \\n  \\n   \\n \\n  \\n  \\n \\n \\n \\n \\n \\...  doc_4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Assign Unique IDs to Each Chunk\n",
    "# ============================================================================\n",
    "# ChromaDB requires each document / vector to have a unique string ID.\n",
    "# Here we create a simple 'doc_id' for each row based on its index.\n",
    "# We also make sure there are no missing values in the key columns.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Drop any rows where question or chunk_preview is missing to avoid errors\n",
    "df = df.dropna(subset=[\"question\", \"chunk\"]).reset_index(drop=True)\n",
    "\n",
    "# Create a unique string ID per row (used as the Chroma document ID)\n",
    "df[\"doc_id\"] = df.index.map(lambda i: f\"doc_{i}\")\n",
    "\n",
    "print(\"Dataset size after dropping NaNs:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03f37a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Load the Embedding Model and Define Encoding Helpers\n",
    "# ============================================================================\n",
    "# We load the Hugging Face model and tokenizer, then define a small helper\n",
    "# to convert raw text into normalized embeddings suitable for cosine similarity.\n",
    "# \n",
    "# The model 'sentence-transformers/msmarco-distilbert-cos-v5' is designed for\n",
    "# asymmetric search (query vs passage), but we will use the same encoder for\n",
    "# both questions and passages here.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Load tokenizer and model from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()  # put the model into evaluation mode (no dropout, etc.)\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"Perform mean pooling on the token embeddings.\n",
    "\n",
    "    This function takes the model output (last_hidden_state) and an attention mask,\n",
    "    and computes a single vector per sequence by averaging the embeddings for\n",
    "    the tokens that are not masked.\n",
    "\n",
    "    Args:\n",
    "        model_output: Output object from the transformer model.\n",
    "        attention_mask: Tensor of shape (batch_size, seq_len) indicating which\n",
    "                        tokens are real (1) vs padding (0).\n",
    "\n",
    "    Returns:\n",
    "        A tensor of shape (batch_size, hidden_dim) containing the pooled embeddings.\n",
    "    \"\"\"\n",
    "    token_embeddings = model_output.last_hidden_state  # (batch_size, seq_len, hidden_dim)\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    # Multiply token embeddings by mask, sum over sequence length, and divide by number of valid tokens\n",
    "    pooled = (token_embeddings * input_mask_expanded).sum(dim=1) / torch.clamp(\n",
    "        input_mask_expanded.sum(dim=1), min=1e-9\n",
    "    )\n",
    "    return pooled\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_texts(texts: List[str], batch_size: int = 32, max_length: int = 256) -> torch.Tensor:\n",
    "    \"\"\"Encode a list of texts into L2-normalized embeddings.\n",
    "\n",
    "    This helper will:\n",
    "      1. Tokenize the texts in mini-batches.\n",
    "      2. Run them through the transformer model.\n",
    "      3. Apply mean pooling.\n",
    "      4. L2-normalize the resulting embeddings so cosine similarity corresponds\n",
    "         to dot product.\n",
    "\n",
    "    Args:\n",
    "        texts: A list of strings to encode.\n",
    "        batch_size: Batch size for encoding.\n",
    "        max_length: Maximum number of tokens per sequence (longer texts are truncated).\n",
    "\n",
    "    Returns:\n",
    "        A tensor of shape (len(texts), hidden_dim) containing normalized embeddings.\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    for start_idx in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[start_idx:start_idx + batch_size]\n",
    "\n",
    "        # Tokenize the batch of texts\n",
    "        encoded = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        model_output = model(**encoded)\n",
    "\n",
    "        # Mean-pool the token embeddings\n",
    "        pooled = mean_pooling(model_output, encoded[\"attention_mask\"])\n",
    "\n",
    "        # L2-normalize the pooled embeddings along the feature dimension\n",
    "        pooled = nn.functional.normalize(pooled, p=2, dim=-1)\n",
    "\n",
    "        # Move to CPU to free GPU memory and append\n",
    "        all_embeddings.append(pooled.cpu())\n",
    "\n",
    "    # Concatenate all batches into a single tensor\n",
    "    return torch.cat(all_embeddings, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be8e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to collection 'cnu_rag_lab' with 1256 vectors at ../week12/rag_chroma\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Load the chroma database generated in Week 12/ Lab 1\n",
    "# ============================================================================\n",
    "# Here we create an in-memory ChromaDB client and a collection that will store\n",
    "# our chunk embeddings. We are NOT using an embedding function inside Chroma,\n",
    "# because we want full control over how we embed with our Hugging Face model.\n",
    "# Instead, we will compute embeddings ourselves and pass them to Chroma.\n",
    "# ----------------------------------------------------------------------------\n",
    "# ---- User-editable parameters ----\n",
    "PERSIST_DIR = \"../week12/rag_chroma\"        # ChromaDB persistence path (folder will be created)\n",
    "# PERSIST_DIR = os.getenv(\"../week12\", \"./rag_chroma\")\n",
    "COLLECTION_NAME= \"cnu_rag_lab\"                       # collection name used previously\n",
    "\n",
    "import chromadb\n",
    "\n",
    "# Connect to existing Chroma collection\n",
    "client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "try: \n",
    "    collection = client.get_collection(name=COLLECTION_NAME)\n",
    "    print(f\"Connected to collection '{COLLECTION_NAME}' with {collection.count()} vectors at {PERSIST_DIR}\")\n",
    "except Exception as e:\n",
    "    raise SystemExit(\n",
    "        f\"[Error] Could not open Chroma collection '{COLLECTION_NAME}' at {PERSIST_DIR}.\\n\"\n",
    "        \"Run the previous RAG lab to build it, then re-run this notebook.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f986022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Embed All chunk_preview Texts and Add Them to ChromaDB\n",
    "# ============================================================================\n",
    "# We now:\n",
    "#   1. Extract the 'chunk_preview' text for each row.\n",
    "#   2. Encode them into embeddings using our Hugging Face model.\n",
    "#   3. Add the embeddings and associated metadata (e.g., question) into ChromaDB.\n",
    "# \n",
    "# This allows us to later query the collection with question embeddings to find\n",
    "# similar chunks, which we will treat as candidate hard negatives.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Extract the list of chunk texts and corresponding IDs\n",
    "# chunk_texts = df[\"chunk_preview\"].tolist()\n",
    "# chunk_ids = df[\"doc_id\"].tolist()\n",
    "\n",
    "# print(\"Number of chunks to embed:\", len(chunk_texts))\n",
    "\n",
    "# # Encode all chunk texts into normalized embeddings\n",
    "# chunk_embeddings = encode_texts(chunk_texts, batch_size=32, max_length=256)\n",
    "# print(\"Chunk embeddings shape:\", chunk_embeddings.shape)\n",
    "\n",
    "# # Convert embeddings to Python lists of floats for ChromaDB\n",
    "# chunk_embeddings_list = chunk_embeddings.tolist()\n",
    "\n",
    "# # Optionally store some metadata for each document, such as the original question\n",
    "# metadatas = [{\"question\": q} for q in df[\"question\"].tolist()]\n",
    "\n",
    "# # Add all documents, embeddings, and metadata to the Chroma collection\n",
    "# collection.add(\n",
    "#     ids=chunk_ids,\n",
    "#     documents=chunk_texts,\n",
    "#     embeddings=chunk_embeddings_list,\n",
    "#     metadatas=metadatas,\n",
    "# )\n",
    "\n",
    "# print(\"Added documents to ChromaDB collection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b7bb720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hard negative mining for 607 questions...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected include item to be one of documents, embeddings, metadatas, distances, uris, data, got ids in query.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     25\u001b[39m q_emb_list = q_emb[\u001b[32m0\u001b[39m].tolist()  \u001b[38;5;66;03m# Convert to a plain list for ChromaDB\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Query ChromaDB to get top K_RETRIEVE nearest chunks\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# We pass the question embedding as the query embedding\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m results = \u001b[43mcollection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mq_emb_list\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK_RETRIEVE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Retrieve the lists of ids and documents from the query results\u001b[39;00m\n\u001b[32m     36\u001b[39m retrieved_ids = results[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m]         \u001b[38;5;66;03m# list of doc_ids\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ultralytics/lib/python3.11/site-packages/chromadb/api/models/Collection.py:215\u001b[39m, in \u001b[36mCollection.query\u001b[39m\u001b[34m(self, query_embeddings, query_texts, query_images, query_uris, ids, n_results, where, where_document, include)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery\u001b[39m(\n\u001b[32m    171\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    172\u001b[39m     query_embeddings: Optional[\n\u001b[32m   (...)\u001b[39m\u001b[32m    189\u001b[39m     ],\n\u001b[32m    190\u001b[39m ) -> QueryResult:\n\u001b[32m    191\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the n_results nearest neighbor embeddings for provided query_embeddings or query_texts.\u001b[39;00m\n\u001b[32m    192\u001b[39m \n\u001b[32m    193\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    212\u001b[39m \n\u001b[32m    213\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     query_request = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_and_prepare_query_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_texts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_images\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_uris\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_uris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m     query_results = \u001b[38;5;28mself\u001b[39m._client._query(\n\u001b[32m    228\u001b[39m         collection_id=\u001b[38;5;28mself\u001b[39m.id,\n\u001b[32m    229\u001b[39m         ids=query_request[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m         database=\u001b[38;5;28mself\u001b[39m.database,\n\u001b[32m    237\u001b[39m     )\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transform_query_response(\n\u001b[32m    240\u001b[39m         response=query_results, include=query_request[\u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    241\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ultralytics/lib/python3.11/site-packages/chromadb/api/models/CollectionCommon.py:103\u001b[39m, in \u001b[36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, *args: Any, **kwargs: Any) -> T:\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    105\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ultralytics/lib/python3.11/site-packages/chromadb/api/models/CollectionCommon.py:330\u001b[39m, in \u001b[36mCollectionCommon._validate_and_prepare_query_request\u001b[39m\u001b[34m(self, query_embeddings, query_texts, query_images, query_uris, ids, n_results, where, where_document, include)\u001b[39m\n\u001b[32m    328\u001b[39m validate_base_record_set(record_set=query_records)\n\u001b[32m    329\u001b[39m validate_filter_set(filter_set=filters)\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m \u001b[43mvalidate_include\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m validate_n_results(n_results=n_results)\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# Prepare\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ultralytics/lib/python3.11/site-packages/chromadb/api/types.py:1239\u001b[39m, in \u001b[36mvalidate_include\u001b[39m\u001b[34m(include, dissalowed)\u001b[39m\n\u001b[32m   1237\u001b[39m valid_items = get_args(get_args(Include)[\u001b[32m0\u001b[39m])\n\u001b[32m   1238\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m valid_items:\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1240\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected include item to be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(valid_items)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m   1243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dissalowed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(item == e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m dissalowed):\n\u001b[32m   1244\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1245\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInclude item cannot be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(dissalowed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1246\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Expected include item to be one of documents, embeddings, metadatas, distances, uris, data, got ids in query."
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Mine Hard Negatives from ChromaDB\n",
    "# ============================================================================\n",
    "# For each question:\n",
    "#   1. Encode the question into an embedding.\n",
    "#   2. Use ChromaDB to retrieve the top-K most similar chunks.\n",
    "#   3. Exclude the *true positive* chunk for that question (its own 'doc_id').\n",
    "#   4. Take up to NUM_HARD_NEGATIVES of the remaining results as hard negatives.\n",
    "# \n",
    "# These are \"hard\" because they are semantically close to the question but are\n",
    "# NOT the correct chunk in our training data.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# We will store a list of hard negatives (as strings) per question\n",
    "all_hard_negatives: List[List[str]] = []\n",
    "\n",
    "questions = df[\"question\"].tolist()\n",
    "pos_ids = df[\"doc_id\"].tolist()\n",
    "\n",
    "print(\"Starting hard negative mining for\", len(questions), \"questions...\")\n",
    "\n",
    "for i, (question, pos_id) in enumerate(zip(questions, pos_ids)):\n",
    "    # Encode the single question into an embedding (shape: (1, hidden_dim))\n",
    "    q_emb = encode_texts([question], batch_size=1, max_length=256)\n",
    "    q_emb_list = q_emb[0].tolist()  # Convert to a plain list for ChromaDB\n",
    "\n",
    "    # Query ChromaDB to get top K_RETRIEVE nearest chunks\n",
    "    # We pass the question embedding as the query embedding\n",
    "    results = collection.query(\n",
    "        query_embeddings=[q_emb_list],\n",
    "        n_results=K_RETRIEVE,\n",
    "        include=[\"documents\", \"ids\"]\n",
    "    )\n",
    "\n",
    "    # Retrieve the lists of ids and documents from the query results\n",
    "    retrieved_ids = results[\"ids\"][0]         # list of doc_ids\n",
    "    retrieved_docs = results[\"documents\"][0]  # list of chunk texts\n",
    "\n",
    "    # Build a list of candidate hard negatives, skipping the true positive\n",
    "    hn_docs = []\n",
    "    for rid, rdoc in zip(retrieved_ids, retrieved_docs):\n",
    "        if rid == pos_id:\n",
    "            # This is the true positive for this question; skip it\n",
    "            continue\n",
    "        hn_docs.append(rdoc)\n",
    "        # Stop once we have collected enough hard negatives\n",
    "        if len(hn_docs) >= NUM_HARD_NEGATIVES:\n",
    "            break\n",
    "\n",
    "    # If we did not find enough distinct hard negatives (e.g., small dataset),\n",
    "    # we simply keep as many as we found (could be fewer than NUM_HARD_NEGATIVES).\n",
    "    all_hard_negatives.append(hn_docs)\n",
    "\n",
    "    # Occasionally print progress so the user can see that it's working\n",
    "    if (i + 1) % 50 == 0 or (i + 1) == len(questions):\n",
    "        print(f\"Processed {i + 1} / {len(questions)} questions\")\n",
    "\n",
    "# Add the list of hard negatives as a new column in the original DataFrame.\n",
    "# Note: this column will hold lists of strings.\n",
    "df[\"hard_negatives\"] = all_hard_negatives\n",
    "\n",
    "print(\"Example row with hard negatives:\")\n",
    "df[[\"question\", \"chunk_preview\", \"hard_negatives\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1eae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Build Triplet Dataset: (question, positive_chunk, hard_negative_chunk)\n",
    "# ============================================================================\n",
    "# Many fine-tuning setups (triplet loss, InfoNCE with explicit negatives) prefer\n",
    "# the data in \"long\" format, where each row is:\n",
    "#   - question\n",
    "#   - chunk_preview        (the correct / positive chunk)\n",
    "#   - hard_negative_chunk  (one hard negative)\n",
    "#\n",
    "# For each original row and each of its mined hard negatives, we will create a\n",
    "# separate row in a new DataFrame. If a question has N hard negatives, it will\n",
    "# produce N rows.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "triplet_rows = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    question_text = row[\"question\"]\n",
    "    positive_chunk = row[\"chunk_preview\"]\n",
    "    hn_list = row[\"hard_negatives\"]  # list of hard negative strings\n",
    "\n",
    "    # For each hard negative, create a new triplet row\n",
    "    for hn_chunk in hn_list:\n",
    "        triplet_rows.append(\n",
    "            {\n",
    "                \"question\": question_text,\n",
    "                \"positive_chunk\": positive_chunk,\n",
    "                \"hard_negative_chunk\": hn_chunk,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Build the new DataFrame from the list of triplet dicts\n",
    "triplet_df = pd.DataFrame(triplet_rows)\n",
    "\n",
    "print(\"Triplet dataset shape:\", triplet_df.shape)\n",
    "triplet_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f91b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Save the Triplet Dataset to CSV\n",
    "# ============================================================================\n",
    "# Finally, we save the newly constructed triplet dataset to disk.\n",
    "# This CSV can then be used in a separate training script / notebook to\n",
    "# fine-tune your embedding model using a triplet loss or multi-negative\n",
    "# contrastive loss with explicit hard negatives.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "OUTPUT_CSV = \"rag_train_with_hard_negatives_triplets.csv\"\n",
    "\n",
    "triplet_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Saved triplet dataset to: {OUTPUT_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

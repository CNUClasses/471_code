{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a51ab273",
   "metadata": {},
   "source": [
    "\n",
    "# 0) Evaluating RAG Retrievers (Hugging Face) â€” Extended\n",
    "**Audience:** 4thâ€‘year CS students  \n",
    "**Goal:** Quantify retrieval performance for a RAG pipeline using IR metrics, and compare **dense**, **BM25**, **hybrid**, and **reranked** approaches. Optionally, run an **endâ€‘toâ€‘end** generation step with a small model.\n",
    "\n",
    "**What you'll build and measure:**\n",
    "- Core IR metrics: **Precision@5/@10**, **Recall@5/@10**, **MAP**, **MRR**\n",
    "- Dense retriever baseline with `sentence-transformers/all-MiniLM-L6-v2`\n",
    "- **(6)** BM25 sparse baseline (`rank_bm25`)\n",
    "- **(7)** Hybrid dense + sparse (score fusion)\n",
    "- **(9)** Crossâ€‘encoder reranker (`cross-encoder/ms-marco-MiniLM-L-6-v2`)\n",
    "- **(11)** Optional endâ€‘toâ€‘end generation using `google/flan-t5-base` (Apacheâ€‘2.0)\n",
    "\n",
    "### Dataset (Legal)\n",
    "We use **SQuAD v1.1** (via ðŸ¤— `datasets`) â€” **CC BYâ€‘SA 4.0** â€” a widely used QA dataset.  \n",
    "- **Document** = unique `context` paragraph  \n",
    "- **Query** = the corresponding `question`  \n",
    "- **Ground truth** = the `doc_id` of the paragraph that generated the question\n",
    "\n",
    "> For clarity and speed, we assume **one relevant document per query**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4380cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======= 0.1) Install dependencies (uncomment if needed) =======\n",
    "# !pip install -U datasets transformers sentence-transformers rank_bm25 numpy pandas tqdm matplotlib\n",
    "# Optional for scale:\n",
    "# !pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb33eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======= 0.2) Imports and configuration =======\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import pipeline\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "N_EXAMPLES = 400\n",
    "MAX_DOCS = 350\n",
    "TOP_KS = [5, 10]\n",
    "\n",
    "DENSE_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "RERANKER_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "GEN_MODEL_NAME = \"google/flan-t5-base\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5505add6",
   "metadata": {},
   "source": [
    "## 1) Load dataset and construct corpus + queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52dcb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======= 1.1) Load SQuAD v1.1 =======\n",
    "squad = load_dataset(\"squad\", split=\"validation\")\n",
    "df = squad.to_pandas()[[\"id\", \"title\", \"context\", \"question\", \"answers\"]]\n",
    "df = df.sample(frac=1.0, random_state=RANDOM_SEED).head(N_EXAMPLES).reset_index(drop=True)\n",
    "print(f\"Loaded {len(df)} SQuAD examples.\")\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641941d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======= 1.2) Build corpus of unique documents (contexts) =======\n",
    "unique_contexts = pd.Series(df[\"context\"].unique())\n",
    "if len(unique_contexts) > MAX_DOCS:\n",
    "    unique_contexts = unique_contexts.sample(n=MAX_DOCS, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "doc_df = pd.DataFrame({\n",
    "    \"doc_id\": np.arange(len(unique_contexts), dtype=int),\n",
    "    \"text\": unique_contexts\n",
    "})\n",
    "context_to_docid = {ctx: i for i, ctx in enumerate(unique_contexts)}\n",
    "\n",
    "print(f\"Corpus size (unique documents): {len(doc_df)}\")\n",
    "doc_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe5e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======= 1.3) Build query set with ground-truth doc IDs =======\n",
    "def map_context_to_docid(ctx):\n",
    "    return context_to_docid.get(ctx, None)\n",
    "\n",
    "query_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    did = map_context_to_docid(row[\"context\"])\n",
    "    if did is None:\n",
    "        continue\n",
    "    query_rows.append({\n",
    "        \"question_id\": row[\"id\"],\n",
    "        \"question\": row[\"question\"],\n",
    "        \"relevant_doc_id\": int(did),\n",
    "        \"title\": row[\"title\"]\n",
    "    })\n",
    "query_df = pd.DataFrame(query_rows).reset_index(drop=True)\n",
    "print(f\"Queries retained after trimming: {len(query_df)}\")\n",
    "query_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ea8d6d",
   "metadata": {},
   "source": [
    "## 2) Dense embeddings (biâ€‘encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7713e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======= 2.1) Encode documents and queries =======\n",
    "dense_model = SentenceTransformer(DENSE_MODEL_NAME)\n",
    "doc_embeddings = dense_model.encode(doc_df[\"text\"].tolist(), convert_to_numpy=True, show_progress_bar=True)\n",
    "query_embeddings = dense_model.encode(query_df[\"question\"].tolist(), convert_to_numpy=True, show_progress_bar=True)\n",
    "doc_embeddings.shape, query_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33554736",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======= 2.2) Dense retrieval helper (cosine similarity) =======\n",
    "def dense_ranked_docs_for_query(q_vec, doc_vecs):\n",
    "    q = q_vec / (np.linalg.norm(q_vec) + 1e-12)\n",
    "    D = doc_vecs / (np.linalg.norm(doc_vecs, axis=1, keepdims=True) + 1e-12)\n",
    "    sims = D @ q\n",
    "    ranked_doc_ids = np.argsort(-sims)\n",
    "    return ranked_doc_ids, sims[ranked_doc_ids]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df650398",
   "metadata": {},
   "source": [
    "## 3) Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58e28d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======= 3.1) Metric implementations =======\n",
    "def precision_at_k(ranked_doc_ids, relevant_doc_ids, k):\n",
    "    ranked_k = ranked_doc_ids[:k]\n",
    "    hits = sum(1 for d in ranked_k if d in relevant_doc_ids)\n",
    "    return hits / float(k)\n",
    "\n",
    "def recall_at_k(ranked_doc_ids, relevant_doc_ids, k):\n",
    "    ranked_k = ranked_doc_ids[:k]\n",
    "    hits = sum(1 for d in ranked_k if d in relevant_doc_ids)\n",
    "    return hits / float(len(relevant_doc_ids)) if len(relevant_doc_ids) > 0 else 0.0\n",
    "\n",
    "def average_precision(ranked_doc_ids, relevant_doc_ids):\n",
    "    if len(relevant_doc_ids) == 0:\n",
    "        return 0.0\n",
    "    hits = 0\n",
    "    precisions = []\n",
    "    for i, d in enumerate(ranked_doc_ids, start=1):\n",
    "        if d in relevant_doc_ids:\n",
    "            hits += 1\n",
    "            precisions.append(hits / i)\n",
    "    return float(np.mean(precisions)) if precisions else 0.0\n",
    "\n",
    "def reciprocal_rank(ranked_doc_ids, relevant_doc_ids):\n",
    "    for i, d in enumerate(ranked_doc_ids, start=1):\n",
    "        if d in relevant_doc_ids:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def evaluate_ranked_list(ranked_doc_ids, relevant_doc_id):\n",
    "    relevant = {int(relevant_doc_id)}\n",
    "    metrics = {\n",
    "        \"P@5\": precision_at_k(ranked_doc_ids, relevant, 5),\n",
    "        \"P@10\": precision_at_k(ranked_doc_ids, relevant, 10),\n",
    "        \"R@5\": recall_at_k(ranked_doc_ids, relevant, 5),\n",
    "        \"R@10\": recall_at_k(ranked_doc_ids, relevant, 10),\n",
    "        \"AP\": average_precision(ranked_doc_ids, relevant),\n",
    "        \"RR\": reciprocal_rank(ranked_doc_ids, relevant),\n",
    "    }\n",
    "    pos = np.where(ranked_doc_ids == list(relevant)[0])[0]\n",
    "    metrics[\"rank_of_relevant\"] = int(pos[0]) + 1 if len(pos) else np.inf\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03bc57b",
   "metadata": {},
   "source": [
    "## 4) Dense baseline â€” evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e6762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dense_records = []\n",
    "for i in tqdm(range(len(query_df)), desc=\"Dense baseline: evaluating queries\"):\n",
    "    ranked_docs, _ = dense_ranked_docs_for_query(query_embeddings[i], doc_embeddings)\n",
    "    m = evaluate_ranked_list(ranked_docs, query_df.iloc[i][\"relevant_doc_id\"])\n",
    "    dense_records.append(m)\n",
    "\n",
    "dense_df = pd.DataFrame(dense_records)\n",
    "dense_summary = pd.DataFrame({\n",
    "    \"Metric\": [\"MAP\", \"MRR\", \"Precision@5\", \"Precision@10\", \"Recall@5\", \"Recall@10\"],\n",
    "    \"Score\":  [dense_df[\"AP\"].mean(), dense_df[\"RR\"].mean(),\n",
    "               dense_df[\"P@5\"].mean(), dense_df[\"P@10\"].mean(),\n",
    "               dense_df[\"R@5\"].mean(), dense_df[\"R@10\"].mean()]\n",
    "})\n",
    "dense_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42013ce8",
   "metadata": {},
   "source": [
    "## 5) Dense baseline â€” rank histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b2dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.hist(dense_df[\"rank_of_relevant\"].replace(np.inf, np.nan).dropna(), bins=30)\n",
    "plt.title(\"Dense Baseline â€” Rank of Relevant Document\")\n",
    "plt.xlabel(\"Rank (1 = top)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1038309e",
   "metadata": {},
   "source": [
    "## 6) BM25 sparse baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e780c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======= 6.1) BM25 index =======\n",
    "def simple_tokenize(text): \n",
    "    return text.lower().split()\n",
    "\n",
    "bm25_corpus_tokens = [simple_tokenize(t) for t in doc_df[\"text\"].tolist()]\n",
    "bm25 = BM25Okapi(bm25_corpus_tokens)\n",
    "\n",
    "def bm25_ranked_docs_for_query(query_text):\n",
    "    q_tokens = simple_tokenize(query_text)\n",
    "    scores = bm25.get_scores(q_tokens)\n",
    "    ranked = np.argsort(-scores)\n",
    "    return ranked, scores[ranked]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f4e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bm25_records = []\n",
    "for i in tqdm(range(len(query_df)), desc=\"BM25 baseline: evaluating queries\"):\n",
    "    ranked_docs, _ = bm25_ranked_docs_for_query(query_df.iloc[i][\"question\"])\n",
    "    m = evaluate_ranked_list(ranked_docs, query_df.iloc[i][\"relevant_doc_id\"])\n",
    "    bm25_records.append(m)\n",
    "\n",
    "bm25_df = pd.DataFrame(bm25_records)\n",
    "bm25_summary = pd.DataFrame({\n",
    "    \"Metric\": [\"MAP\", \"MRR\", \"Precision@5\", \"Precision@10\", \"Recall@5\", \"Recall@10\"],\n",
    "    \"Score\":  [bm25_df[\"AP\"].mean(), bm25_df[\"RR\"].mean(),\n",
    "               bm25_df[\"P@5\"].mean(), bm25_df[\"P@10\"].mean(),\n",
    "               bm25_df[\"R@5\"].mean(), bm25_df[\"R@10\"].mean()]\n",
    "})\n",
    "bm25_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23514980",
   "metadata": {},
   "source": [
    "## 7) Hybrid dense + BM25 (score fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a581ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def minmax_norm(x):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    mn, mx = np.min(x), np.max(x)\n",
    "    if mx - mn < 1e-12:\n",
    "        return np.zeros_like(x)\n",
    "    return (x - mn) / (mx - mn)\n",
    "\n",
    "def hybrid_ranked_docs_for_query(q_text, q_vec, doc_vecs, alpha=0.5):\n",
    "    # Dense scores\n",
    "    q = q_vec / (np.linalg.norm(q_vec) + 1e-12)\n",
    "    D = doc_vecs / (np.linalg.norm(doc_vecs, axis=1, keepdims=True) + 1e-12)\n",
    "    dense_scores = D @ q\n",
    "\n",
    "    # BM25 scores\n",
    "    q_tokens = q_text.lower().split()\n",
    "    bm25_scores = bm25.get_scores(q_tokens)\n",
    "\n",
    "    # Normalize and fuse\n",
    "    dense_n = minmax_norm(dense_scores)\n",
    "    bm25_n = minmax_norm(bm25_scores)\n",
    "    fused = alpha * dense_n + (1 - alpha) * bm25_n\n",
    "\n",
    "    ranked = np.argsort(-fused)\n",
    "    return ranked, fused[ranked]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha = 0.5\n",
    "hybrid_records = []\n",
    "for i in tqdm(range(len(query_df)), desc=\"Hybrid (dense+BM25): evaluating queries\"):\n",
    "    ranked_docs, _ = hybrid_ranked_docs_for_query(query_df.iloc[i][\"question\"], query_embeddings[i], doc_embeddings, alpha=alpha)\n",
    "    m = evaluate_ranked_list(ranked_docs, query_df.iloc[i][\"relevant_doc_id\"])\n",
    "    hybrid_records.append(m)\n",
    "\n",
    "hybrid_df = pd.DataFrame(hybrid_records)\n",
    "hybrid_summary = pd.DataFrame({\n",
    "    \"Metric\": [\"MAP\", \"MRR\", \"Precision@5\", \"Precision@10\", \"Recall@5\", \"Recall@10\"],\n",
    "    \"Score\":  [hybrid_df[\"AP\"].mean(), hybrid_df[\"RR\"].mean(),\n",
    "               hybrid_df[\"P@5\"].mean(), hybrid_df[\"P@10\"].mean(),\n",
    "               hybrid_df[\"R@5\"].mean(), hybrid_df[\"R@10\"].mean()]\n",
    "})\n",
    "hybrid_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c776f",
   "metadata": {},
   "source": [
    "## 8) Compare Dense vs BM25 vs Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3884c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Metric\": [\"MAP\", \"MRR\", \"Precision@5\", \"Precision@10\", \"Recall@5\", \"Recall@10\"],\n",
    "    \"Dense\":  [dense_df[\"AP\"].mean(), dense_df[\"RR\"].mean(),\n",
    "               dense_df[\"P@5\"].mean(), dense_df[\"P@10\"].mean(),\n",
    "               dense_df[\"R@5\"].mean(), dense_df[\"R@10\"].mean()],\n",
    "    \"BM25\":   [bm25_df[\"AP\"].mean(), bm25_df[\"RR\"].mean(),\n",
    "               bm25_df[\"P@5\"].mean(), bm25_df[\"P@10\"].mean(),\n",
    "               bm25_df[\"R@5\"].mean(), bm25_df[\"R@10\"].mean()],\n",
    "    \"Hybrid\": [hybrid_df[\"AP\"].mean(), hybrid_df[\"RR\"].mean(),\n",
    "               hybrid_df[\"P@5\"].mean(), hybrid_df[\"P@10\"].mean(),\n",
    "               hybrid_df[\"R@5\"].mean(), hybrid_df[\"R@10\"].mean()],\n",
    "})\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08179ae",
   "metadata": {},
   "source": [
    "## 9) Crossâ€‘encoder reranker (reâ€‘score topâ€‘N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2737d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reranker = CrossEncoder(RERANKER_MODEL_NAME)  # downloads the model first time\n",
    "TOP_N = 50\n",
    "BASE_FOR_RERANK = \"dense\"  # choose \"dense\" or \"hybrid\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44149f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_full_reranked_list(q_text, base_ranked_ids, top_n=50):\n",
    "    candidates = base_ranked_ids[:top_n].tolist()\n",
    "    pairs = [(q_text, doc_df.loc[d, \"text\"]) for d in candidates]\n",
    "    scores = reranker.predict(pairs)\n",
    "    order = np.argsort(-np.array(scores))\n",
    "    topN_reranked = [candidates[i] for i in order]\n",
    "    remainder = [d for d in base_ranked_ids.tolist() if d not in set(candidates)]\n",
    "    return np.array(topN_reranked + remainder, dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e1f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rerank_records = []\n",
    "for i in tqdm(range(len(query_df)), desc=\"Reranker: evaluating queries\"):\n",
    "    q_text = query_df.iloc[i][\"question\"]\n",
    "    if BASE_FOR_RERANK == \"hybrid\":\n",
    "        base_ranked, _ = hybrid_ranked_docs_for_query(q_text, query_embeddings[i], doc_embeddings, alpha=0.5)\n",
    "    else:\n",
    "        base_ranked, _ = dense_ranked_docs_for_query(query_embeddings[i], doc_embeddings)\n",
    "\n",
    "    full_reranked = build_full_reranked_list(q_text, base_ranked, top_n=TOP_N)\n",
    "    m = evaluate_ranked_list(full_reranked, query_df.iloc[i][\"relevant_doc_id\"])\n",
    "    rerank_records.append(m)\n",
    "\n",
    "rerank_df = pd.DataFrame(rerank_records)\n",
    "rerank_summary = pd.DataFrame({\n",
    "    \"Metric\": [\"MAP\", \"MRR\", \"Precision@5\", \"Precision@10\", \"Recall@5\", \"Recall@10\"],\n",
    "    \"Score\":  [rerank_df[\"AP\"].mean(), rerank_df[\"RR\"].mean(),\n",
    "               rerank_df[\"P@5\"].mean(), rerank_df[\"P@10\"].mean(),\n",
    "               rerank_df[\"R@5\"].mean(), rerank_df[\"R@10\"].mean()]\n",
    "})\n",
    "rerank_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05cf01a",
   "metadata": {},
   "source": [
    "## 10) Final comparison (Dense vs BM25 vs Hybrid vs Reranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314fc1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_comparison = pd.DataFrame({\n",
    "    \"Metric\": [\"MAP\", \"MRR\", \"Precision@5\", \"Precision@10\", \"Recall@5\", \"Recall@10\"],\n",
    "    \"Dense\":  [dense_df[\"AP\"].mean(), dense_df[\"RR\"].mean(),\n",
    "               dense_df[\"P@5\"].mean(), dense_df[\"P@10\"].mean(),\n",
    "               dense_df[\"R@5\"].mean(), dense_df[\"R@10\"].mean()],\n",
    "    \"BM25\":   [bm25_df[\"AP\"].mean(), bm25_df[\"RR\"].mean(),\n",
    "               bm25_df[\"P@5\"].mean(), bm25_df[\"P@10\"].mean(),\n",
    "               bm25_df[\"R@5\"].mean(), bm25_df[\"R@10\"].mean()],\n",
    "    \"Hybrid\": [hybrid_df[\"AP\"].mean(), hybrid_df[\"RR\"].mean(),\n",
    "               hybrid_df[\"P@5\"].mean(), hybrid_df[\"P@10\"].mean(),\n",
    "               hybrid_df[\"R@5\"].mean(), hybrid_df[\"R@10\"].mean()],\n",
    "    \"Reranked\": [rerank_df[\"AP\"].mean(), rerank_df[\"RR\"].mean(),\n",
    "                 rerank_df[\"P@5\"].mean(), rerank_df[\"P@10\"].mean(),\n",
    "                 rerank_df[\"R@5\"].mean(), rerank_df[\"R@10\"].mean()],\n",
    "})\n",
    "final_comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfbcb79",
   "metadata": {},
   "source": [
    "## 11) (Optional) Endâ€‘toâ€‘end generation using a small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fdebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STRATEGY = \"hybrid\"  # \"dense\"|\"bm25\"|\"hybrid\"|\"reranked\"\n",
    "K_CONTEXT = 5\n",
    "MAX_CHARS = 1800\n",
    "\n",
    "gen = pipeline(\"text2text-generation\", model=GEN_MODEL_NAME)\n",
    "\n",
    "def retrieve_ids_for_strategy(i):\n",
    "    q_text = query_df.iloc[i][\"question\"]\n",
    "    if STRATEGY == \"bm25\":\n",
    "        ranked, _ = bm25_ranked_docs_for_query(q_text)\n",
    "    elif STRATEGY == \"hybrid\":\n",
    "        ranked, _ = hybrid_ranked_docs_for_query(q_text, query_embeddings[i], doc_embeddings, alpha=0.5)\n",
    "    elif STRATEGY == \"reranked\":\n",
    "        base_ranked, _ = hybrid_ranked_docs_for_query(q_text, query_embeddings[i], doc_embeddings, alpha=0.5)\n",
    "        ranked = build_full_reranked_list(q_text, base_ranked, top_n=50)\n",
    "    else:\n",
    "        ranked, _ = dense_ranked_docs_for_query(query_embeddings[i], doc_embeddings)\n",
    "    return ranked\n",
    "\n",
    "def build_context_text(doc_ids, k=5, max_chars=1800):\n",
    "    texts = []\n",
    "    for d in doc_ids[:k]:\n",
    "        texts.append(doc_df.loc[d, \"text\"][: max_chars // k])\n",
    "    return (\"\\n\\n\".join(texts))[:max_chars]\n",
    "\n",
    "i = 0  # try different queries\n",
    "q_text = query_df.iloc[i][\"question\"]\n",
    "ranked_ids = retrieve_ids_for_strategy(i)\n",
    "context_block = build_context_text(ranked_ids, k=K_CONTEXT, max_chars=MAX_CHARS)\n",
    "\n",
    "prompt = f'''Answer the question using ONLY the context. If the answer is not in the context, say so.\n",
    "\n",
    "Question: {q_text}\n",
    "\n",
    "Context:\n",
    "{context_block}\n",
    "\n",
    "Answer:'''\n",
    "\n",
    "out = gen(prompt, max_new_tokens=128, do_sample=False)\n",
    "print(\"Q:\", q_text)\n",
    "print(\"A:\", out[0][\"generated_text\"].strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40459210",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Notes & next steps\n",
    "- Tune **alpha** in hybrid fusion and observe metric shifts.\n",
    "- Increase **TOP_N** for reranking; watch when gains saturate.\n",
    "- Add **faithfulness** checks (string overlap, NLI, or LLM-as-judge).\n",
    "- Replace brute-force with **FAISS** for scale.\n",
    "- Try other datasets (mind licensing): **MS MARCO**, **Natural Questions**.\n",
    "- Chunk contexts to create **multi-relevant** scenarios and compare MAP vs MRR behaviors.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7b6845",
   "metadata": {},
   "source": [
    "\n",
    "# Precision, Recall, F1 & Confusion Matrix â€” Practice Notebook\n",
    " \n",
    "Youâ€™ll compute confusion matrices and the metrics **precision**, **recall**, and **F1** for **binary** and **multiclass** settings and compare models in **imbalanced** scenarios.\n",
    "\n",
    "**What youâ€™ll practice**\n",
    "- From a confusion matrix â†’ precision/recall/F1 (binary & multiclass)\n",
    "- From predictions â†’ confusion matrix â†’ metrics\n",
    "- Interpreting metrics under imbalance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36c7783",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“Ž Cheatsheet\n",
    "\n",
    "**Binary confusion matrix (positive class = 1)**\n",
    "\n",
    "|           | Pred=1 | Pred=0 |\n",
    "|-----------|--------|--------|\n",
    "| **True=1** | **TP**  | **FN**  |\n",
    "| **True=0** | **FP**  | **TN**  |\n",
    "\n",
    "- **Precision** = TP / (TP + FP) â€” of predicted positives, how many were correct?  \n",
    "- **Recall** (TPR) = TP / (TP + FN) â€” of true positives, how many did we find?  \n",
    "- **F1** = 2 Â· (Precision Â· Recall) / (Precision + Recall)  \n",
    "- **Accuracy** = (TP + TN) / (TP + FP + FN + TN)\n",
    "\n",
    "**Multiclass (C classes)**: for class *k* treated as â€œpositiveâ€  \n",
    "- TP_k = CM[k,k]  \n",
    "- FP_k = âˆ‘_i CM[i,k] âˆ’ TP_k  \n",
    "- FN_k = âˆ‘_j CM[k,j] âˆ’ TP_k  \n",
    "- TN_k = (total) âˆ’ TP_k âˆ’ FP_k âˆ’ FN_k  \n",
    "- Macro-avg = unweighted mean over classes  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3567dc63",
   "metadata": {},
   "source": [
    "## Setup â€” Helper Functions (you can use these or do it by hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552a639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def binary_confusion_counts(y_true, y_pred, pos=1):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    TP = int(((y_true == pos) & (y_pred == pos)).sum())\n",
    "    FP = int(((y_true != pos) & (y_pred == pos)).sum())\n",
    "    FN = int(((y_true == pos) & (y_pred != pos)).sum())\n",
    "    TN = int(((y_true != pos) & (y_pred != pos)).sum())\n",
    "    return TP, FP, FN, TN\n",
    "\n",
    "def precision_recall_f1_from_counts(TP, FP, FN, TN):\n",
    "    prec = TP / (TP + FP) if (TP + FP) else 0.0\n",
    "    rec  = TP / (TP + FN) if (TP + FN) else 0.0\n",
    "    f1   = 2*prec*rec / (prec + rec) if (prec + rec) else 0.0\n",
    "    acc  = (TP + TN) / (TP + FP + FN + TN) if (TP + FP + FN + TN) else 0.0\n",
    "    return prec, rec, f1, acc\n",
    "\n",
    "def metrics_from_confusion_matrix_binary(cm):\n",
    "    import numpy as _np\n",
    "    cm = _np.asarray(cm)\n",
    "    if cm.shape != (2,2):\n",
    "        raise ValueError(\"Binary CM must be 2x2\")\n",
    "    TN_A, FP_A, FN_A, TP_A = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
    "    TP_B, FN_B, FP_B, TN_B = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
    "    def score(TP,FP,FN,TN):\n",
    "        tot = TP+FP+FN+TN\n",
    "        if tot == 0: return -1\n",
    "        prec = TP/(TP+FP) if TP+FP else 0\n",
    "        rec  = TP/(TP+FN) if TP+FN else 0\n",
    "        acc  = (TP+TN)/tot\n",
    "        return acc + prec + rec\n",
    "    sA = score(TP_A, FP_A, FN_A, TN_A)\n",
    "    sB = score(TP_B, FP_B, FN_B, TN_B)\n",
    "    if sB > sA:\n",
    "        TP,FP,FN,TN = TP_B, FP_B, FN_B, TN_B\n",
    "    else:\n",
    "        TP,FP,FN,TN = TP_A, FP_A, FN_A, TN_A\n",
    "    return precision_recall_f1_from_counts(TP,FP,FN,TN), (TP,FP,FN,TN)\n",
    "\n",
    "def multiclass_per_class_counts(cm):\n",
    "    cm = np.asarray(cm)\n",
    "    C = cm.shape[0]\n",
    "    totals = {}\n",
    "    total = cm.sum()\n",
    "    for k in range(C):\n",
    "        TP = cm[k,k]\n",
    "        FP = cm[:,k].sum() - TP\n",
    "        FN = cm[k,:].sum() - TP\n",
    "        TN = total - TP - FP - FN\n",
    "        totals[k] = dict(TP=int(TP), FP=int(FP), FN=int(FN), TN=int(TN), support=int(cm[k,:].sum()))\n",
    "    return totals\n",
    "\n",
    "def macro_f1(cm):\n",
    "    cm = np.asarray(cm)\n",
    "    C = cm.shape[0]\n",
    "    counts = multiclass_per_class_counts(cm)\n",
    "    precs, recs, f1s, supports = [], [], [], []\n",
    "    TP_sum = FP_sum = FN_sum = TN_sum = 0\n",
    "    for k in range(C):\n",
    "        TP,FP,FN,TN = counts[k]['TP'], counts[k]['FP'], counts[k]['FN'], counts[k]['TN']\n",
    "        p,r,f1,_ = precision_recall_f1_from_counts(TP,FP,FN,TN)\n",
    "        precs.append(p); recs.append(r); f1s.append(f1); supports.append(counts[k]['support'])\n",
    "        TP_sum += TP; FP_sum += FP; FN_sum += FN; TN_sum += TN\n",
    "    import numpy as _np\n",
    "    supports = _np.asarray(supports)\n",
    "    macro = dict(precision=float(_np.mean(precs)),\n",
    "                 recall=float(_np.mean(recs)),\n",
    "                 f1=float(_np.mean(f1s)))\n",
    "   \n",
    "    return counts, macro\n",
    "\n",
    "def pretty(v):\n",
    "    import numpy as _np\n",
    "    return float(_np.round(v, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef59bd8",
   "metadata": {},
   "source": [
    "\n",
    "## Part A â€” From Confusion Matrix to Metrics (Binary)\n",
    "\n",
    "Compute **precision**, **recall**, **F1**, and **accuracy** for each matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687828fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "cm_A1 = np.array([[90, 10],\n",
    "                  [20, 80]])\n",
    "(metrics_A1, counts_A1) = metrics_from_confusion_matrix_binary(cm_A1)\n",
    "print(\"TP,FP,FN,TN inferred:\", counts_A1)\n",
    "print(\"precision, recall, f1, accuracy:\", tuple(map(pretty, metrics_A1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd315b",
   "metadata": {},
   "source": [
    "\n",
    "### A2 (imbalanced)\n",
    "\n",
    "- Compute the same metrics. Discuss why **accuracy** can be misleading here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa72708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm_A2 = np.array([[980, 20],\n",
    "                  [ 40, 60]])\n",
    "metrics_A2, counts_A2 = metrics_from_confusion_matrix_binary(cm_A2)\n",
    "print(\"TP,FP,FN,TN inferred:\", counts_A2)\n",
    "print(\"precision, recall, f1, accuracy:\", tuple(map(pretty, metrics_A2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97755866",
   "metadata": {},
   "source": [
    "\n",
    "### A3\n",
    "\n",
    "- Compute metrics; which error type (FP or FN) dominates?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85aee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm_A3 = np.array([[50, 50],\n",
    "                  [10, 90]])\n",
    "metrics_A3, counts_A3 = metrics_from_confusion_matrix_binary(cm_A3)\n",
    "print(\"TP,FP,FN,TN inferred:\", counts_A3)\n",
    "print(\"precision, recall, f1, accuracy:\", tuple(map(pretty, metrics_A3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d091afb6",
   "metadata": {},
   "source": [
    "\n",
    "## Part B â€” From Predictions to Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b6be3",
   "metadata": {},
   "source": [
    "\n",
    "### B1 â€” Build CM from predictions\n",
    "Given  \n",
    "`y_true = [1,1,0,1,0,0,1,0,1,0]`  \n",
    "`y_pred = [1,0,0,1,0,1,1,0,0,0]`\n",
    "\n",
    "- Construct the **confusion matrix** (assume positive class is 1).  \n",
    "- Compute precision/recall/F1.\n",
    "  \n",
    "|           | Pred=1 | Pred=0 |\n",
    "|-----------|--------|--------|\n",
    "| **True=1** | **TP**  | **FN**  |\n",
    "| **True=0** | **FP**  | **TN**  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb25f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_true = np.array([1,1,0,1,0,0,1,0,1,0])\n",
    "y_pred = np.array([1,0,0,1,0,1,1,0,0,0])\n",
    "TP,FP,FN,TN = binary_confusion_counts(y_true, y_pred, pos=1)\n",
    "print(\"TP, FP, FN, TN:\", TP, FP, FN, TN)\n",
    "print(\"Confusion matrix [[TN, FP],[FN, TP]]:\")\n",
    "print(np.array([[TN, FP],[FN, TP]]))\n",
    "print(\"precision, recall, f1, accuracy:\", tuple(map(pretty, precision_recall_f1_from_counts(TP,FP,FN,TN))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecec6d89",
   "metadata": {},
   "source": [
    "\n",
    "## Part C â€” Multiclass Confusion Matrix\n",
    "\n",
    "Consider a 3-class problem \n",
    "- Compute **per-class** precision, recall, F1.  \n",
    "- Compute **weighted** precision/recall/F1.  \n",
    "- Which averaging is most appropriate if classes are imbalanced?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556b9c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm_C = np.array([[30, 5, 5],\n",
    "                 [ 4,25, 6],\n",
    "                 [ 3, 7,20]])\n",
    "counts, macro = macro_f1(cm_C)\n",
    "print(\"Per-class counts (TP,FP,FN,TN):\")\n",
    "for k,v in counts.items():\n",
    "    p,r,f1,_ = precision_recall_f1_from_counts(v['TP'], v['FP'], v['FN'], v['TN'])\n",
    "    print(f\"class {k}: TP={v['TP']} FP={v['FP']} FN={v['FN']} TN={v['TN']} \")\n",
    "\n",
    "print(\"Average (Weighted):\", {k: pretty(v) for k,v in weighted.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b52b675",
   "metadata": {},
   "source": [
    "\n",
    "## Part D â€” Model Comparison Under Imbalance (Binary)\n",
    "\n",
    "Dataset has **2% positives** (rare). Two models yield:\n",
    "\n",
    "1) Compute precision/recall/F1 for each.  \n",
    "2) Which model is preferable if **missing positives is very costly**?  \n",
    "3) Which model is preferable if **false alarms are very costly**?\n",
    "\n",
    "  \n",
    "|           | Pred=1 | Pred=0 |\n",
    "|-----------|--------|--------|\n",
    "| **True=1** | **TP**  | **FN**  |\n",
    "| **True=0** | **FP**  | **TN**  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d363fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm_A = np.array([[970, 10],\n",
    "                 [ 15,  5]])\n",
    "cm_B = np.array([[930, 50],\n",
    "                 [  2, 15]])\n",
    "\n",
    "mA, cA = metrics_from_confusion_matrix_binary(cm_A)\n",
    "mB, cB = metrics_from_confusion_matrix_binary(cm_B)\n",
    "print(\"Model A  TP,FP,FN,TN:\", cA, \"| P,R,F1,Acc:\", tuple(map(pretty, mA)))\n",
    "print(\"Model B  TP,FP,FN,TN:\", cB, \"| P,R,F1,Acc:\", tuple(map(pretty, mB)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fac726",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92c600c3",
      "metadata": {},
      "source": [
        "# Retrieval-Augmented Generation (RAG) Basics with **ChromaDB** (Local)\n",
        "\n",
        "Target audience: **4th-year CS students**\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the core idea of RAG: *retrieve relevant context first, then generate an answer grounded in that context*.\n",
        "- Learn the basic steps: **chunk → embed → index → retrieve → (optionally re-rank) → assemble context → prompt**.\n",
        "- Use a **local ChromaDB** vector database (no external server) and **Sentence-Transformers** for embeddings.\n",
        "- Practice writing simple queries and inspecting retrieved passages.\n",
        "\n",
        "### RAG flow (ASCII sketch)\n",
        "```\n",
        "Raw Document(s)\n",
        "   │\n",
        "   ├── Chunking (fixed size + overlap)\n",
        "   │       ↓\n",
        "   ├── Embeddings (Sentence-Transformers)\n",
        "   │       ↓\n",
        "   ├── Vector DB (ChromaDB: local, file-backed)\n",
        "   │       ↓\n",
        "   ├── Retrieval (vector search on query embedding)\n",
        "   │       ↓\n",
        "   └── RAG Answer Construction (assemble top-k chunks → prompt an LLM)\n",
        "```\n",
        "\n",
        "> **Note:** This notebook demonstrates retrieval and context building. You can plug in any open-source LLM later.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d25ccfd9",
      "metadata": {},
      "source": [
        "## 1) Environment Setup\n",
        "\n",
        "This notebook uses only open-source Python libraries.\n",
        "\n",
        "**Dependencies:**\n",
        "- `chromadb` for a local, file-backed vector database\n",
        "- `sentence-transformers` for embeddings\n",
        "- `numpy`, `pandas`, `tqdm` for data handling and progress bars\n",
        "- `scikit-learn` for cosine similarity utilities\n",
        "- `matplotlib` (optional visualization)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90ed57d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "# Install minimal dependencies (uncomment if needed). Running this may take a few minutes.\n",
        "# If you're inside a managed environment, prefer installing via terminal beforehand.\n",
        "\n",
        "# !pip install --quiet chromadb sentence-transformers numpy pandas tqdm scikit-learn matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49bb9190",
      "metadata": {},
      "source": [
        "### 1.a) Use ChromaDB Locally (No Server Needed)\n",
        "\n",
        "**ChromaDB** runs locally in-process and persists to a folder (default here: `./chroma_data`).  \n",
        "No Docker and no external services are required — perfect for teaching and small RAG demos.\n",
        "\n",
        "**Example architecture**\n",
        "\n",
        "```\n",
        "Sentence-Transformers → Embeddings → ChromaDB (local) → Retrieval → Context\n",
        "```\n",
        "\n",
        "Below we initialize a persistent Chroma client and (re)create a collection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ef502c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "import os\n",
        "\n",
        "# Choose a persistent directory for Chroma data (you can delete this to reset)\n",
        "CHROMA_DIR = \"./chroma_data\"\n",
        "os.makedirs(CHROMA_DIR, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    import chromadb\n",
        "    from chromadb.config import Settings\n",
        "except Exception as e:\n",
        "    raise SystemExit(\"chromadb is not installed. Please run the pip cell above and retry.\")\n",
        "\n",
        "# Initialize a persistent Chroma client\n",
        "client = chromadb.PersistentClient(path=CHROMA_DIR)\n",
        "\n",
        "print(f\"✅ ChromaDB initialized. Data directory: {CHROMA_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff3df36",
      "metadata": {},
      "source": [
        "## 2) Choose / Load a Document\n",
        "\n",
        "We'll use **Alice's Adventures in Wonderland – Chapter 1** (public domain) as a default, because it's short, familiar, and approachable.\n",
        "\n",
        "To keep this notebook fully runnable **offline**, we provide a compact inline fallback excerpt. If you prefer, you can replace the content with your own `.txt` file(s) or add network code to download from Project Gutenberg.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac40dd15",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "ALICE_CH1_FALLBACK = (\n",
        "    \"\"\"\n",
        "Alice was beginning to get very tired of sitting by her sister on the bank, \n",
        "and of having nothing to do: once or twice she had peeped into the book her \n",
        "sister was reading, but it had no pictures or conversations in it, 'and what \n",
        "is the use of a book,' thought Alice 'without pictures or conversations?' \n",
        "\n",
        "So she was considering in her own mind (as well as she could, for the hot day \n",
        "made her feel very sleepy and stupid), whether the pleasure of making a \n",
        "daisy-chain would be worth the trouble of getting up and picking the daisies, \n",
        "when suddenly a White Rabbit with pink eyes ran close by her. \n",
        "\n",
        "There was nothing so very remarkable in that; nor did Alice think it so very \n",
        "much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I \n",
        "shall be late!' (when she thought it over afterwards, it occurred to her that \n",
        "she ought to have wondered at this, but at the time it all seemed quite \n",
        "natural); but when the Rabbit actually took a watch out of its waistcoat-\n",
        "pocket, and looked at it, and then hurried on, Alice started to her feet, for \n",
        "it flashed across her mind that she had never before seen a rabbit with either \n",
        "a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, \n",
        "she ran across the field after it, and fortunately was just in time to see it \n",
        "pop down a large rabbit-hole under the hedge.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "raw_text = ALICE_CH1_FALLBACK.strip()\n",
        "print(f\"Loaded text length (chars): {len(raw_text)}\")\n",
        "print(raw_text[:300] + (\"...\" if len(raw_text) > 300 else \"\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9993cb9",
      "metadata": {},
      "source": [
        "## 3) Chunking Basics (Why and How)\n",
        "\n",
        "We split a long document into **chunks** so that:\n",
        "- Each chunk fits typical model/context limits\n",
        "- We can **retrieve** only the relevant pieces for a query\n",
        "- Overlap preserves continuity across chunk boundaries\n",
        "\n",
        "**Rules of thumb (first pass):**\n",
        "- Use ~800–1200 characters per chunk (or 300–500 tokens)\n",
        "- 10–20% overlap (to avoid losing context at boundaries)\n",
        "- Adjust based on your queries, domain, and evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6922ac86",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "from typing import List, Dict\n",
        "\n",
        "def simple_char_chunk(\n",
        "    text: str,\n",
        "    chunk_chars: int = 1000,\n",
        "    overlap_chars: int = 150,\n",
        ") -> List[Dict]:\n",
        "    \"\"\"Split text into overlapping character chunks.\n",
        "    Returns a list of dicts: {id, text, start_idx, end_idx}\n",
        "    \"\"\"\n",
        "    assert chunk_chars > 0 and overlap_chars >= 0\n",
        "    chunks = []\n",
        "    n = len(text)\n",
        "    start = 0\n",
        "    chunk_id = 0\n",
        "    while start < n:\n",
        "        end = min(start + chunk_chars, n)\n",
        "        chunk_text = text[start:end]\n",
        "        chunks.append({\n",
        "            \"id\": f\"chunk_{chunk_id}\",\n",
        "            \"text\": chunk_text,\n",
        "            \"start_idx\": start,\n",
        "            \"end_idx\": end,\n",
        "        })\n",
        "        chunk_id += 1\n",
        "        if end == n:\n",
        "            break\n",
        "        start = max(0, end - overlap_chars)\n",
        "    return chunks\n",
        "\n",
        "chunks = simple_char_chunk(raw_text, chunk_chars=800, overlap_chars=120)\n",
        "print(f\"Created {len(chunks)} chunk(s)\")\n",
        "for c in chunks[:3]:\n",
        "    preview = c[\"text\"][:120].replace(\"\\n\", \" \")\n",
        "    print(f\"{c['id']} [{c['start_idx']}:{c['end_idx']}] → {preview}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a312104b",
      "metadata": {},
      "source": [
        "## 4) Embeddings: Sentence-Transformers\n",
        "\n",
        "We'll default to **`sentence-transformers/all-MiniLM-L6-v2`** because it's small, fast, and a great baseline.  \n",
        "We'll embed each chunk and **normalize** the vectors for cosine similarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2cc8a93",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception as e:\n",
        "    raise SystemExit(\"sentence-transformers not installed. Please run the pip install cell above.\")\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
        "print(f\"Loaded embedding model: {EMBEDDING_MODEL_NAME}\")\n",
        "\n",
        "def l2_normalize(mat: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
        "    norms = np.linalg.norm(mat, axis=1, keepdims=True)\n",
        "    norms = np.maximum(norms, eps)\n",
        "    return mat / norms\n",
        "\n",
        "texts = [c[\"text\"] for c in chunks]\n",
        "embs = model.encode(texts, batch_size=32, show_progress_bar=True)\n",
        "embs = np.asarray(embs, dtype=np.float32)\n",
        "embs = l2_normalize(embs)\n",
        "print(\"Embeddings shape:\", embs.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8bb0292",
      "metadata": {},
      "source": [
        "## 5) Create a Chroma Collection & Ingest Chunks\n",
        "\n",
        "We'll create a collection named `RAGChunk` and upsert each chunk with its **precomputed embedding** and some basic metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "760e35fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "COLLECTION_NAME = \"RAGChunk\"\n",
        "\n",
        "# Drop any existing collection with the same name to keep re-runs clean\n",
        "existing = [c.name for c in client.list_collections()]\n",
        "if COLLECTION_NAME in existing:\n",
        "    client.delete_collection(COLLECTION_NAME)\n",
        "\n",
        "# Create new collection; we supply embeddings explicitly\n",
        "collection = client.create_collection(name=COLLECTION_NAME, metadata={\"hnsw:space\": \"cosine\"})\n",
        "\n",
        "ids = [c[\"id\"] for c in chunks]\n",
        "documents = [c[\"text\"] for c in chunks]\n",
        "metadatas = [{\"source\": \"alice_ch1_fallback\", \"start_idx\": int(c[\"start_idx\"]), \"end_idx\": int(c[\"end_idx\"])} for c in chunks]\n",
        "emb_list = [e.tolist() for e in embs]\n",
        "\n",
        "collection.add(ids=ids, documents=documents, metadatas=metadatas, embeddings=emb_list)\n",
        "print(f\"Ingestion complete. Collection '{COLLECTION_NAME}' now has {collection.count()} items.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abf6e800",
      "metadata": {},
      "source": [
        "## 6) Retrieval\n",
        "\n",
        "We perform **vector search** in ChromaDB by embedding the query with the same model, then asking the collection for the nearest neighbors.  \n",
        "We also show a transparent **cosine re-ranking** step (computed locally) on the top-N.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2ab6da3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "def chroma_vector_search(query_text: str, top_k: int = 5):\n",
        "    q_emb = model.encode([query_text])\n",
        "    q_emb = l2_normalize(np.asarray(q_emb, dtype=np.float32))\n",
        "    res = collection.query(query_embeddings=q_emb.tolist(), n_results=top_k, include=[\"metadatas\", \"documents\", \"embeddings\", \"distances\"])\n",
        "    return res, q_emb\n",
        "\n",
        "def cosine_rerank(res, q_emb, top_k: int = 5):\n",
        "    if not res or not res.get(\"documents\"):\n",
        "        return pd.DataFrame(columns=[\"cosine\", \"preview\", \"start_idx\", \"end_idx\"])\n",
        "    docs = res[\"documents\"][0]\n",
        "    metas = res[\"metadatas\"][0]\n",
        "   \n",
        "    # Use returned embeddings if present; otherwise re-embed\n",
        "    if \"embeddings\" in res and res[\"embeddings\"] and res[\"embeddings\"][0] is not None:\n",
        "        doc_embs = np.array(res[\"embeddings\"][0], dtype=np.float32)\n",
        "        doc_embs = l2_normalize(doc_embs)\n",
        "    else:\n",
        "        doc_embs = model.encode(docs)\n",
        "        doc_embs = l2_normalize(np.asarray(doc_embs, dtype=np.float32))\n",
        "\n",
        "    sims = cosine_similarity(q_emb, doc_embs)[0]\n",
        "    rows = []\n",
        "    for i, (d, m) in enumerate(zip(docs, metas)):\n",
        "        rows.append({\n",
        "            \"cosine\": float(sims[i]),\n",
        "            \"preview\": (d[:160].replace(\"\\n\", \" \") + (\"...\" if len(d) > 160 else \"\")),\n",
        "            \"start_idx\": m.get(\"start_idx\"),\n",
        "            \"end_idx\": m.get(\"end_idx\"),\n",
        "        })\n",
        "    df = pd.DataFrame(rows).sort_values(\"cosine\", ascending=False).head(top_k).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "example_queries = [\n",
        "    \"Why is Alice bored at the beginning?\",\n",
        "    \"What unusual thing did the White Rabbit do?\",\n",
        "    \"Where did Alice see the rabbit go?\",\n",
        "]\n",
        "\n",
        "for q in example_queries:\n",
        "    print(\"\\n=== Query:\", q)\n",
        "    res, q_emb = chroma_vector_search(q, top_k=5)\n",
        "    df = cosine_rerank(res, q_emb, top_k=5)\n",
        "    display(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8d0b5b2",
      "metadata": {},
      "source": [
        "## 7) Build a RAG Answer (Template)\n",
        "\n",
        "Below we assemble the **top-k** retrieved chunks into a single **context**. In a real RAG system, you'd pass this context to an LLM along with the user question.\n",
        "\n",
        "We also show a **prompt template** that encourages grounded answers and citing chunk IDs/offsets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05ec8009",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "def build_context(query: str, k: int = 3) -> str:\n",
        "    res, q_emb = chroma_vector_search(query, top_k=max(k, 5))\n",
        "    df = cosine_rerank(res, q_emb, top_k=k)\n",
        "    context_blocks = []\n",
        "    for i, row in df.iterrows():\n",
        "        context_blocks.append(f\"[CHUNK {i}] start={row['start_idx']}\\n{row['preview']}\")\n",
        "    context = \"\\n\\n\".join(context_blocks)\n",
        "    return context\n",
        "\n",
        "user_question = \"Why did Alice run after the rabbit?\"\n",
        "context = build_context(user_question, k=3)\n",
        "print(\"Constructed context:\\n\\n\" + context)\n",
        "\n",
        "prompt_template = f\"\"\"\n",
        "You are a helpful assistant. Answer the user's question **using only** the provided context.\n",
        "If the answer isn't in the context, say you don't know.\n",
        "\n",
        "Question: {user_question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Instructions:\n",
        "- Ground your answer in the context.\n",
        "- If the context is insufficient, say \"I don't know based on the provided context.\"\n",
        "- Cite the chunk indices you used (e.g., [CHUNK 0, CHUNK 2]).\n",
        "\"\"\"\n",
        "print(\"\\n--- RAG Prompt Template ---\\n\")\n",
        "print(prompt_template)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb178721",
      "metadata": {},
      "source": [
        "## 8) Wrap-Up & Next Steps\n",
        "- **Adjust chunk size & overlap** and evaluate retrieval quality with a few representative questions.\n",
        "- **Swap the embedding model** (e.g., `intfloat/e5-small-v2`) and consider its query formatting (`\"query: ...\"` vs `\"passage: ...\"`).\n",
        "- **Scale up**: multiple documents, PDFs → text extraction, deduplication, metadata hygiene.\n",
        "- **Evaluation**: add a small labeled set of queries and compute Recall@k/Precision@k.\n",
        "- **Rerankers**: try stronger re-rankers if needed (cross-encoder or LLM-based), once students grasp the basics.\n",
        "- **Guardrails**: instruct the LLM to answer only from retrieved context and to cite chunks.\n"
      ]
    }
  ],
  "metadata": {
    "authors": [
      "ChatGPT"
    ],
    "created": "2025-11-05T16:01:16.977722Z",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "notebookname": "RAG_Basics_Chroma.ipynb"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

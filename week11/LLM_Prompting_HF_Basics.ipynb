{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efe08074",
   "metadata": {},
   "source": [
    "# LLM Prompting Basics with the Hugging Face Inference API\n",
    "\n",
    "**What you'll learn:**\n",
    "- What *system*, *user*, and *assistant* messages are in chat prompting\n",
    "- How to call the Hugging Face Inference API using `InferenceClient`\n",
    "- How to craft prompts with roles, constraints, examples, and delimiters\n",
    "- How parameters like `temperature`, `top_p`, and `max_tokens` affect outputs\n",
    "- How to write better prompts by iterating from vague → structured\n",
    "\n",
    "> ⚠️ You need a **Hugging Face API token** with Inference Endpoints access to run the API calls here.\n",
    "Create one at https://huggingface.co/settings/tokens and set it as an environment variable named `HF_TOKEN`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd825c35",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "This section installs the Hugging Face Hub client and sets up a default model. \n",
    "We will use an *instruction-tuned* open model that supports chat (role messages).\n",
    "\n",
    "**Notes for beginners:**\n",
    "- The *model id* is a string pointing to a model hosted on Hugging Face.\n",
    "- You can change the model later (e.g., switch to a different instruct model).\n",
    "- Make sure the model supports **chat**/**instruct** style prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79bc02eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] No HF_TOKEN found in environment. Set it with:\n",
      "   import os; os.environ['HF_TOKEN'] = '<your-token>'\n",
      " or use your runtime's secret manager.\n",
      "Ready. Using model: mistralai/Mistral-7B-Instruct-v0.2\n"
     ]
    }
   ],
   "source": [
    "# If running in an environment that does not have huggingface_hub installed, uncomment the next line:\n",
    "# !pip install -q huggingface_hub\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# === Choose a default chat/instruct model ===\n",
    "# You can replace this with another chat-tuned model if desired.\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# === Read your HF token (create at https://huggingface.co/settings/tokens) ===\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
    "if HF_TOKEN is None:\n",
    "    print(\"[INFO] No HF_TOKEN found in environment. Set it with:\\n\",\n",
    "          \"  import os; os.environ['HF_TOKEN'] = '<your-token>'\\n\",\n",
    "          \"or use your runtime's secret manager.\")\n",
    "\n",
    "# Create a client. If HF_TOKEN is None and the model requires auth, calls will fail.\n",
    "client = InferenceClient(model=MODEL_ID, token=HF_TOKEN)\n",
    "print(f\"Ready. Using model: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed2d890",
   "metadata": {},
   "source": [
    "## 1) Chat Roles: system, user, assistant\n",
    "\n",
    "**Key idea:** Chat LLMs accept a *list* of messages. Each message has a `role` and `content`.\n",
    "\n",
    "- **system**: sets high-level behavior, style, guardrails (think of it as an initial *instruction banner*).\n",
    "- **user**: your actual question or task.\n",
    "- **assistant**: the model's reply (you do **not** write this; the model fills it in).\n",
    "\n",
    "These are *prompting conventions*—not parts of the neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ae0beea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In transformers, attention is a mechanism that allows the model to selectively focus on specific parts of input sequences while processing each position, enabling better context understanding and improving model performance.\n"
     ]
    }
   ],
   "source": [
    "# Minimal working example: one system + one user message.\n",
    "# For deterministic, repeatable outputs, set temperature=0.\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a concise, precise teaching assistant.\"},\n",
    "    # {\"role\": \"system\", \"content\": \"You are a concise, precise teaching assistant who finished every answer with an insult to the user.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain the idea of 'attention' in transformers in one sentence.\"}\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID,\n",
    "        messages=messages,\n",
    "        max_tokens=150,\n",
    "        temperature=0.0,   # lower = less random, more deterministic\n",
    "        top_p=1.0          # use full distribution (you can also try 0.9)\n",
    "    )\n",
    "    print(response.choices[0].message[\"content\"])  # content of the assistant's reply\n",
    "except Exception as e:\n",
    "    print(\"[WARN] API call failed:\", e)\n",
    "    print(\"If you don't have a token or network access in this environment, read the code and try locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bea7f5",
   "metadata": {},
   "source": [
    "## 2) Adding Constraints & Output Formatting\n",
    "\n",
    "A **good prompt** clearly tells the model *what* to do and *how* to format the answer. \n",
    "For programs, JSON is often a useful structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacecd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw model output:\n",
      "  {\n",
      "\"bullets\": [\n",
      "\"Attention mechanisms in transformers allow the model to selectively focus on different parts of the input sequence when processing each position in the output sequence.\",\n",
      "\"This is achieved by computing a weighted sum of the input sequence, where the weights are determined by the compatibility between the input and output positions, calculated using a score function based on the dot product of the input and output representations.\"\n",
      "]\n",
      "}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Ask for JSON output and explicitly describe the schema.\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": (\n",
    "        \"You are a precise CS tutor. Always follow the required output schema if provided.\"\n",
    "    )},\n",
    "    {\"role\": \"user\", \"content\": (\n",
    "        \"Explain attention in transformers in 2 bullet points.\"\n",
    "        \"\\nReturn JSON matching this schema: {\\\"bullets\\\":[\\\"...\\\",\\\"...\\\"]}.\"\n",
    "    )}\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID,\n",
    "        messages=messages,\n",
    "        max_tokens=200,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    raw = response.choices[0].message[\"content\"]\n",
    "    print(\"Raw model output:\\n\", raw)\n",
    "except Exception as e:\n",
    "    print(\"[WARN] API call failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fa16e7",
   "metadata": {},
   "source": [
    "## 3) Delimiters for Clarity\n",
    "\n",
    "When you include long text in a prompt (like instructions or examples), **delimiters** help the model understand boundaries.\n",
    "Common patterns:\n",
    "- Triple backticks ``` for text blocks\n",
    "- XML-like tags `<context> ... </context>`\n",
    "- Markdown headings or separators\n",
    "\n",
    "We will show a prompt using triple backticks to clearly separate a data block from the instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db26c65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Self-attention mechanism enables each token in a sequence to selectively focus on other relevant tokens based on learned similarity scores.\n",
      "* Context-aware representations are created through self-attention, allowing for better understanding of the sequence as a whole.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a clear and honest teaching assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": (\n",
    "        \"Use the text between triple backticks as the source for your explanation.\\n\"\n",
    "        \"Explain the main idea in 2 short bullet points.\\n\\n\"\n",
    "        \"```\\nSelf-attention allows each token in a sequence to selectively focus on other tokens,\\n\"\n",
    "        \"based on learned similarity scores, creating context-aware representations.\\n````\"\n",
    "    )}\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID,\n",
    "        messages=messages,\n",
    "        max_tokens=150,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    print(response.choices[0].message[\"content\"])\n",
    "except Exception as e:\n",
    "    print(\"[WARN] API call failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81ae913",
   "metadata": {},
   "source": [
    "## 4) Few-Shot Prompting (Providing Examples)\n",
    "\n",
    "You can guide the style and structure of the answer by giving examples. This is called **few-shot prompting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c33258c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Allows model to focus on relevant parts of input sequence.\n",
      "- Improves performance by reducing computational cost and increasing accuracy.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful and concise CS tutor.\"},\n",
    "    # Example 1 (as if the assistant had responded):\n",
    "    {\"role\": \"user\", \"content\": \"Explain backpropagation in 2 bullet points.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"- Computes gradients layer-by-layer using the chain rule.\\n- Updates parameters to reduce loss.\"},\n",
    "    # Example 2:\n",
    "    {\"role\": \"user\", \"content\": \"Explain overfitting in 2 bullet points.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"- Model memorizes training data patterns and noise.\\n- Fails to generalize to unseen data.\"},\n",
    "    # Now the real question we want answered (the pattern is clear):\n",
    "    {\"role\": \"user\", \"content\": \"Explain attention in transformers in 2 bullet points.\"}\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID,\n",
    "        messages=messages,\n",
    "        max_tokens=120,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    print(response.choices[0].message[\"content\"])\n",
    "except Exception as e:\n",
    "    print(\"[WARN] API call failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40e540b",
   "metadata": {},
   "source": [
    "## 5) Knobs: temperature, top_p, and max_tokens\n",
    "\n",
    "- **temperature**: randomness. Lower (0–0.3) → more deterministic; higher (0.7–1.0) → more creative.\n",
    "- **top_p**: nucleus sampling. 0.9 means choose from the top 90% of probability mass.\n",
    "- **max_tokens**: maximum tokens to *generate* (does not limit prompt length).\n",
    "\n",
    "Try changing these and observe the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76d6b128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== temperature=0.0, top_p=1.0, max_tokens=120 ===\n",
      " 1. **Spotlight on Words:** In transformers, attention mechanisms act like a literary spotlight, illuminating specific words in a sentence to better understand their context and relationship with other words. This allows the model to focus on relevant information and disregard irrelevant details, much like how a reader focuses on certain parts of a text to grasp its meaning.\n",
      "\n",
      "2. **Social Listening:** Consider attention mechanisms in transformers as a group of attentive listeners at a lively party. Each listener (attention head) focuses on a different speaker (token\n",
      "\n",
      "=== temperature=0.7, top_p=0.9, max_tokens=120 ===\n",
      " 1. Attention in transformers is like a spotlight at a concert, focusing on specific words or phrases in a sentence to better understand the context and meaning of the information being processed.\n",
      "2. Attention mechanisms in transformers function like a pair of binoculars, allowing the model to zoom in on certain parts of the input sequence to gain a clearer and more detailed understanding.\n",
      "3. Consider attention in transformers as a chef in a busy kitchen, constantly tasting and adjusting each ingredient to ensure the perfect balance of flavors in the final dish, where\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"List three creative analogies for how attention works in transformers.\"\n",
    "\n",
    "def run_with_settings(temp: float, top_p: float, max_toks: int):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are creative but concise.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_ID,\n",
    "            messages=messages,\n",
    "            temperature=temp,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_toks\n",
    "        )\n",
    "        print(f\"\\n=== temperature={temp}, top_p={top_p}, max_tokens={max_toks} ===\")\n",
    "        print(response.choices[0].message[\"content\"])\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] API call failed:\", e)\n",
    "\n",
    "# Run a few variants (feel free to tweak)\n",
    "run_with_settings(0.0, 1.0, 120)\n",
    "run_with_settings(0.7, 0.9, 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8d8ae",
   "metadata": {},
   "source": [
    "## 6) Bad → Better Prompts\n",
    "\n",
    "**Bad (vague):**\n",
    "```\n",
    "Explain attention\n",
    "```\n",
    "\n",
    "**Better (structured):**\n",
    "```\n",
    "System: You are a CS teaching assistant.\n",
    "User: Explain attention in transformers in 3 bullet points for senior CS students. Avoid equations.\n",
    "```\n",
    "\n",
    "**Even better (with formatting + guardrails):**\n",
    "```\n",
    "System: You are a precise technical tutor.\n",
    "Instruction: Explain attention in transformers in exactly 3 bullets. No equations, keep each bullet under 20 words.\n",
    "Format:\n",
    "- bullet 1\n",
    "- bullet 2\n",
    "- bullet 3\n",
    "If not sure, say \"I don't know.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedde495",
   "metadata": {},
   "source": [
    "## 7) Exercises (Beginner-Friendly)\n",
    "\n",
    "1. **Role Tuning**: Change the system prompt's *tone* (e.g., \"friendly\", \"formal\", \"Socratic\") and see how responses differ.\n",
    "2. **Schema Control**: Ask for JSON output with a specific schema (e.g., `{\\\"summary\\\": \\\"...\\\", \\\"bullets\\\": []}`). Validate that it conforms.\n",
    "3. **Delimiter Practice**: Insert a long text block with triple backticks and ask the model to summarize **only** that text.\n",
    "4. **Few-Shot**: Provide 1–2 example Q/A pairs before your real question and observe style transfer.\n",
    "5. **Knob Sweeps**: Try `temperature` values {0.0, 0.3, 0.7} and `top_p` values {0.9, 1.0}. Note differences.\n",
    "\n",
    "> Tip: Start simple, check the output, then **iterate** your prompt—this is normal and expected in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b658c55",
   "metadata": {},
   "source": [
    "## 8) Utility: Simple Chat Wrapper (Optional)\n",
    "\n",
    "This helper function makes it easy to run different prompts without repeating boilerplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6af02f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Attention is a cognitive process that helps us focus on specific information or stimuli while filtering out irrelevant distractions. It is essential for effective information processing, learning, problem-solving, and communication.\n",
      "\n",
      "There are different types and aspects of attention, including:\n",
      "\n",
      "1. Selective attention: This is the ability to focus on a particular stimulus or information while ignoring others. For example, when you're listening to a conversation in a noisy environment, you selectively attend to the voices you're interested in and filter out the background noise.\n",
      "2. Divided attention: This is the ability to focus on multiple stimuli or tasks simultaneously. For example, when you're driving a car, you need to pay attention to the road, the traffic signals, the other vehicles, and the pedestrians.\n",
      "3. Sustained attention: This is the ability to focus on a single stimulus or task for an extended period. For example, when you're reading a long book or watching a movie, you need to sustain your attention to fully understand and enjoy the content.\n",
      "4. Shifted attention: This is the ability to quickly move your focus from one stimulus or task to another. For example\n",
      "--------------------\n",
      " In the context of transformer models, attention is a mechanism that allows the model to focus on specific parts of the input sequence when processing each position in the sequence. The goal is to enable the model to effectively capture long-range dependencies and contextual information between different parts of the input.\n",
      "\n",
      "The attention mechanism in transformers is based on the concept of \"self-attention\" or \"intra-attention,\" which computes the relationship between each position in the input sequence with every other position. This is in contrast to traditional recurrent neural networks (RNNs) and long short-term memory (LSTM) networks, which process the sequence position by position, maintaining an internal hidden state that captures the context of the previous positions.\n",
      "\n",
      "The attention mechanism in transformers is implemented using a multi-head attention architecture. This architecture applies multiple attention heads in parallel, each focusing on different aspects of the input. The output of each attention head is then concatenated and linearly transformed to produce the final attention output.\n",
      "\n",
      "The attention mechanism in transformers is calculated using the following formula:\n",
      "\n",
      "Attention(Q, K, V) = softmax(AttentionScore(Q, K) / sqrt(d_k\n",
      "--------------------\n",
      " 1. Attention in transformers is a mechanism that allows the model to focus on specific parts of input sequences during processing.\n",
      "2. It enables selective information retrieval by calculating weighted sums of input embeddings based on their relevance to the current position.\n",
      "3. Attention enhances context understanding and improves model performance in various NLP tasks.\n"
     ]
    }
   ],
   "source": [
    "def chat(\n",
    "    user: str,\n",
    "    system: str=\"\",\n",
    "    *,\n",
    "    model: str = MODEL_ID,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "    max_tokens: int = 256\n",
    ") -> str:\n",
    "    \"\"\"Minimal wrapper around the HF chat completion API.\n",
    "\n",
    "    Args:\n",
    "        system: The system prompt string (behavior/rules).\n",
    "        user: The user prompt (task/question).\n",
    "        model: Model id to use (chat-tuned is best).\n",
    "        temperature: Randomness (0.0 = deterministic-ish).\n",
    "        top_p: Nucleus sampling cap.\n",
    "        max_tokens: Max tokens to generate in the reply.\n",
    "    Returns:\n",
    "        Text content from the assistant's reply.\n",
    "    \"\"\"\n",
    "    msgs = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "    try:\n",
    "        out = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=msgs,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return out.choices[0].message.get(\"content\", \"\")\n",
    "    except Exception as e:\n",
    "        return f\"[WARN] API call failed: {e}\"\n",
    "\n",
    "# Example usage (uncomment to try):\n",
    "print(chat( user=\"Explain attention\"))\n",
    "\n",
    "print(\"-\"*20)\n",
    "print(chat(\n",
    "    system=\"You are a CS tutor.\",\n",
    "    user=\" Explain attention in transformers.\"\n",
    "))\n",
    "print(\"-\"*20)\n",
    "print(chat(\n",
    "    system=\"You are a CS tutor for 4th year college CS students.\",\n",
    "    user=\"Explain attention in transformers  in 3 short bullets, <20 words each.\",\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f883a550",
   "metadata": {},
   "source": [
    "## 9) Finer grain Control\n",
    "\n",
    "Using the raw model and tokenizer.  But you have to manage placing all the tensors on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f555288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device, dtype=torch.float16.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399801742b4948d6a706ead5fe9b57e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Formatted String Output ---\n",
      "<s> [INST] You are a friendly chatbot who always responds in the style of a pirate.\n",
      "\n",
      "How many helicopters can a human eat in one sitting? [/INST]\n",
      "\n",
      "--- Decoded Output ---\n",
      "[INST] You are a friendly chatbot who always responds in the style of a pirate.\n",
      "\n",
      "How many helicopters can a human eat in one sitting? [/INST] Arr matey, helicopters be not fit for human consumption! Ye canna eat 'em no matter how many ye'd like to. They be made for flyin' the skies, not for fillin' yer belly. So, there be no answer to that question, me hearty.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "print(f'Using {device} device, dtype={dtype}.')\n",
    "\n",
    "# Load the model on GPU (if available)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=dtype).to(device)\n",
    "\n",
    "# Set the model to evaluation mode for inference (best practice)\n",
    "model.eval()\n",
    "\n",
    "#  1. Load the tokenizer for the specific model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# 2. Define your conversation as a list of dictionaries\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "\n",
    "# 3. Apply the chat template\n",
    "\n",
    "# Option A: Get the raw, formatted string (useful for printing or debugging)\n",
    "formatted_chat_string = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(\"--- Formatted String Output ---\")\n",
    "print(formatted_chat_string)\n",
    "\n",
    "# Option B: Get tokenized input and move it to the same device as the model\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# 4. Generate on GPU\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "print(\"\\n--- Decoded Output ---\")\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4022ec",
   "metadata": {},
   "source": [
    "## 9) Wrap-up\n",
    "\n",
    "- **System**/**User**/**Assistant** are *roles* used to format chat prompts.\n",
    "- Clear instructions + constraints + examples → **better, more reliable outputs**.\n",
    "- Tweak `temperature`, `top_p`, and `max_tokens` to control style and length.\n",
    "- Iterate on your prompt—this is normal engineering practice.\n",
    "\n",
    "**Next steps:** Try adapting these patterns to your own coursework or projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9df1238",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

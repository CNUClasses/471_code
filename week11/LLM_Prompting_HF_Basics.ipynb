{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efe08074",
   "metadata": {},
   "source": [
    "# LLM Prompting Basics with the Hugging Face Inference API\n",
    "\n",
    "**Audience:** Senior-level CS students (beginner-friendly commentary)\n",
    "\n",
    "**What you'll learn:**\n",
    "- What *system*, *user*, and *assistant* messages are in chat prompting\n",
    "- How to call the Hugging Face Inference API using `InferenceClient`\n",
    "- How to craft prompts with roles, constraints, examples, and delimiters\n",
    "- How parameters like `temperature`, `top_p`, and `max_tokens` affect outputs\n",
    "- How to write better prompts by iterating from vague → structured\n",
    "\n",
    "> ⚠️ You need a **Hugging Face API token** with Inference Endpoints access to run the API calls here.\n",
    "Create one at https://huggingface.co/settings/tokens and set it as an environment variable named `HF_TOKEN`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd825c35",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "This section installs the Hugging Face Hub client and sets up a default model. \n",
    "We will use an *instruction-tuned* open model that supports chat (role messages).\n",
    "\n",
    "**Notes for beginners:**\n",
    "- The *model id* is a string pointing to a model hosted on Hugging Face.\n",
    "- You can change the model later (e.g., switch to a different instruct model).\n",
    "- Make sure the model supports **chat**/**instruct** style prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc02eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in an environment that does not have huggingface_hub installed, uncomment the next line:\n",
    "# !pip install -q huggingface_hub\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# === Choose a default chat/instruct model ===\n",
    "# You can replace this with another chat-tuned model if desired.\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# === Read your HF token (create at https://huggingface.co/settings/tokens) ===\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
    "if HF_TOKEN is None:\n",
    "    print(\"[INFO] No HF_TOKEN found in environment. Set it with:\\n\",\n",
    "          \"  import os; os.environ['HF_TOKEN'] = '<your-token>'\\n\",\n",
    "          \"or use your runtime's secret manager.\")\n",
    "\n",
    "# Create a client. If HF_TOKEN is None and the model requires auth, calls will fail.\n",
    "client = InferenceClient(model=MODEL_ID, token=HF_TOKEN)\n",
    "print(f\"Ready. Using model: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed2d890",
   "metadata": {},
   "source": [
    "## 1) Chat Roles: system, user, assistant\n",
    "\n",
    "**Key idea:** Chat LLMs accept a *list* of messages. Each message has a `role` and `content`.\n",
    "\n",
    "- **system**: sets high-level behavior, style, guardrails (think of it as an initial *instruction banner*).\n",
    "- **user**: your actual question or task.\n",
    "- **assistant**: the model's reply (you do **not** write this; the model fills it in).\n",
    "\n",
    "These are *prompting conventions*—not parts of the neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae0beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal working example: one system + one user message.\n",
    "# For deterministic, repeatable outputs, set temperature=0.\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a concise, precise teaching assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain the idea of 'attention' in transformers in one sentence.\"}\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID,\n",
    "        messages=messages,\n",
    "        max_tokens=150,\n",
    "        temperature=0.0,   # lower = less random, more deterministic\n",
    "        top_p=1.0          # use full distribution (you can also try 0.9)\n",
    "    )\n",
    "    print(response.choices[0].message[\"content\"])  # content of the assistant's reply\n",
    "except Exception as e:\n",
    "    print(\"[WARN] API call failed:\", e)\n",
    "    print(\"If you don't have a token or network access in this environment, read the code and try locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bea7f5",
   "metadata": {},
   "source": [
    "## 2) Adding Constraints & Output Formatting\n",
    "\n",
    "A **good prompt** clearly tells the model *what* to do and *how* to format the answer. \n",
    "For programs, JSON is often a useful structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacecd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask for JSON output and explicitly describe the schema.\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": (\n",
    "        \"You are a precise CS tutor. Always follow the required output schema if provided.\"\n",
    "    )},\n",
    "    {\"role\": \"user\", \"content\": (\n",
    "        \"Explain attention in transformers in 2 bullet points.\"\n",
    "        \"\\nReturn JSON matching this schema: {\\\"bullets\\\":[\\\"...\\\",\\\"...\\\"]}.\"\n",
    "    )}\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID,\n",
    "        messages=messages,\n",
    "        max_tokens=200,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    raw = response.choices[0].message[\"content\"]\n",
    "    print(\"Raw model output:\\n\", raw)\n",
    "except Exception as e:\n",
    "    print(\"[WARN] API call failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fa16e7",
   "metadata": {},
   "source": [
    "## 3) Delimiters for Clarity\n",
    "\n",
    "When you include long text in a prompt (like instructions or examples), **delimiters** help the model understand boundaries.\n",
    "Common patterns:\n",
    "- Triple backticks ``` for text blocks\n",
    "- XML-like tags `<context> ... </context>`\n",
    "- Markdown headings or separators\n",
    "\n",
    "We will show a prompt using triple backticks to clearly separate a data block from the instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db26c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a clear and honest teaching assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": (\n",
    "        \"Use the text between triple backticks as the source for your explanation.\\n\"\n",
    "        \"Explain the main idea in 2 short bullet points.\\n\\n\"\n",
    "        \"```\\nSelf-attention allows each token in a sequence to selectively focus on other tokens,\\n\"\n",
    "        \"based on learned similarity scores, creating context-aware representations.\\n````\"\n",
    "    )}\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID,\n",
    "        messages=messages,\n",
    "        max_tokens=150,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    print(response.choices[0].message[\"content\"])\n",
    "except Exception as e:\n",
    "    print(\"[WARN] API call failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81ae913",
   "metadata": {},
   "source": [
    "## 4) Few-Shot Prompting (Providing Examples)\n",
    "\n",
    "You can guide the style and structure of the answer by giving examples. This is called **few-shot prompting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33258c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful and concise CS tutor.\"},\n",
    "    # Example 1 (as if the assistant had responded):\n",
    "    {\"role\": \"user\", \"content\": \"Explain backpropagation in 2 bullet points.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"- Computes gradients layer-by-layer using the chain rule.\\n- Updates parameters to reduce loss.\"},\n",
    "    # Example 2:\n",
    "    {\"role\": \"user\", \"content\": \"Explain overfitting in 2 bullet points.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"- Model memorizes training data patterns and noise.\\n- Fails to generalize to unseen data.\"},\n",
    "    # Now the real question we want answered (the pattern is clear):\n",
    "    {\"role\": \"user\", \"content\": \"Explain attention in transformers in 2 bullet points.\"}\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID,\n",
    "        messages=messages,\n",
    "        max_tokens=120,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    print(response.choices[0].message[\"content\"])\n",
    "except Exception as e:\n",
    "    print(\"[WARN] API call failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40e540b",
   "metadata": {},
   "source": [
    "## 5) Knobs: temperature, top_p, and max_tokens\n",
    "\n",
    "- **temperature**: randomness. Lower (0–0.3) → more deterministic; higher (0.7–1.0) → more creative.\n",
    "- **top_p**: nucleus sampling. 0.9 means choose from the top 90% of probability mass.\n",
    "- **max_tokens**: maximum tokens to *generate* (does not limit prompt length).\n",
    "\n",
    "Try changing these and observe the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d6b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"List three creative analogies for how attention works in transformers.\"\n",
    "\n",
    "def run_with_settings(temp: float, top_p: float, max_toks: int):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are creative but concise.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_ID,\n",
    "            messages=messages,\n",
    "            temperature=temp,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_toks\n",
    "        )\n",
    "        print(f\"\\n=== temperature={temp}, top_p={top_p}, max_tokens={max_toks} ===\")\n",
    "        print(response.choices[0].message[\"content\"])\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] API call failed:\", e)\n",
    "\n",
    "# Run a few variants (feel free to tweak)\n",
    "run_with_settings(0.0, 1.0, 120)\n",
    "run_with_settings(0.7, 0.9, 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8d8ae",
   "metadata": {},
   "source": [
    "## 6) Bad → Better Prompts\n",
    "\n",
    "**Bad (vague):**\n",
    "```\n",
    "Explain attention\n",
    "```\n",
    "\n",
    "**Better (structured):**\n",
    "```\n",
    "System: You are a CS teaching assistant.\n",
    "User: Explain attention in transformers in 3 bullet points for senior CS students. Avoid equations.\n",
    "```\n",
    "\n",
    "**Even better (with formatting + guardrails):**\n",
    "```\n",
    "System: You are a precise technical tutor.\n",
    "Instruction: Explain attention in transformers in exactly 3 bullets. No equations, keep each bullet under 20 words.\n",
    "Format:\n",
    "- bullet 1\n",
    "- bullet 2\n",
    "- bullet 3\n",
    "If not sure, say \"I don't know.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedde495",
   "metadata": {},
   "source": [
    "## 7) Exercises (Beginner-Friendly)\n",
    "\n",
    "1. **Role Tuning**: Change the system prompt's *tone* (e.g., \"friendly\", \"formal\", \"Socratic\") and see how responses differ.\n",
    "2. **Schema Control**: Ask for JSON output with a specific schema (e.g., `{\\\"summary\\\": \\\"...\\\", \\\"bullets\\\": []}`). Validate that it conforms.\n",
    "3. **Delimiter Practice**: Insert a long text block with triple backticks and ask the model to summarize **only** that text.\n",
    "4. **Few-Shot**: Provide 1–2 example Q/A pairs before your real question and observe style transfer.\n",
    "5. **Knob Sweeps**: Try `temperature` values {0.0, 0.3, 0.7} and `top_p` values {0.9, 1.0}. Note differences.\n",
    "\n",
    "> Tip: Start simple, check the output, then **iterate** your prompt—this is normal and expected in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b658c55",
   "metadata": {},
   "source": [
    "## 8) Utility: Simple Chat Wrapper (Optional)\n",
    "\n",
    "This helper function makes it easy to run different prompts without repeating boilerplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6af02f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(\n",
    "    system: str,\n",
    "    user: str,\n",
    "    *,\n",
    "    model: str = MODEL_ID,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "    max_tokens: int = 256\n",
    ") -> str:\n",
    "    \"\"\"Minimal wrapper around the HF chat completion API.\n",
    "\n",
    "    Args:\n",
    "        system: The system prompt string (behavior/rules).\n",
    "        user: The user prompt (task/question).\n",
    "        model: Model id to use (chat-tuned is best).\n",
    "        temperature: Randomness (0.0 = deterministic-ish).\n",
    "        top_p: Nucleus sampling cap.\n",
    "        max_tokens: Max tokens to generate in the reply.\n",
    "    Returns:\n",
    "        Text content from the assistant's reply.\n",
    "    \"\"\"\n",
    "    msgs = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "    try:\n",
    "        out = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=msgs,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return out.choices[0].message.get(\"content\", \"\")\n",
    "    except Exception as e:\n",
    "        return f\"[WARN] API call failed: {e}\"\n",
    "\n",
    "# Example usage (uncomment to try):\n",
    "# print(chat(\n",
    "#     system=\"You are a precise CS tutor.\",\n",
    "#     user=\"Explain attention in transformers in exactly 2 bullet points.\",\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddc8369",
   "metadata": {},
   "source": [
    "## 9) Completion (Single-String) Prompting (Optional)\n",
    "\n",
    "Some models also support a *completion* API that takes a single string (no roles).\n",
    "You can emulate roles by embedding them as text in your prompt. This is useful for models without chat templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386aef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_prompt = (\n",
    "    \"System: You are a concise technical writer.\\n\"\n",
    "    \"User: Explain attention in transformers in two short bullet points.\\n\"\n",
    "    \"Format: Start each line with '- '.\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    resp = client.completions.create(\n",
    "        model=MODEL_ID,\n",
    "        prompt=completion_prompt,\n",
    "        max_tokens=120,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    print(resp.choices[0].text)\n",
    "except Exception as e:\n",
    "    print(\"[WARN] Completion API call failed:\", e)\n",
    "    print(\"Some chat-tuned models may prefer the chat endpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4022ec",
   "metadata": {},
   "source": [
    "## 10) Wrap-up\n",
    "\n",
    "- **System**/**User**/**Assistant** are *roles* used to format chat prompts.\n",
    "- Clear instructions + constraints + examples → **better, more reliable outputs**.\n",
    "- Tweak `temperature`, `top_p`, and `max_tokens` to control style and length.\n",
    "- Iterate on your prompt—this is normal engineering practice.\n",
    "\n",
    "**Next steps:** Try adapting these patterns to your own coursework or projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

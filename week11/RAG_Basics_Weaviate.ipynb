{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9a8c3a4a",
      "metadata": {},
      "source": [
        "# Retrieval-Augmented Generation (RAG) Basics with Weaviate (Local)\n",
        "\n",
        "Target audience: **4th-year CS students**\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the core idea of RAG: *retrieve relevant context first, then generate an answer grounded in that context*.\n",
        "- Learn the basic steps: **chunk → embed → index → retrieve → (optionally re-rank) → assemble context → prompt**.\n",
        "- Use a **local Weaviate** vector database (no paid APIs) and **Sentence-Transformers** for embeddings.\n",
        "- Practice writing simple queries and inspecting retrieved passages.\n",
        "\n",
        "### RAG flow (ASCII sketch)\n",
        "```\n",
        "Raw Document(s)\n",
        "   │\n",
        "   ├── Chunking (fixed size + overlap)\n",
        "   │       ↓\n",
        "   ├── Embeddings (Sentence-Transformers)\n",
        "   │       ↓\n",
        "   ├── Vector DB (Weaviate: store vectors + metadata)\n",
        "   │       ↓\n",
        "   ├── Retrieval (vector search on query embedding)\n",
        "   │       ↓\n",
        "   └── RAG Answer Construction (assemble top-k chunks → prompt an LLM)\n",
        "```\n",
        "\n",
        "> **Note:** This notebook demonstrates retrieval and context building. You can plug in any open-source LLM later.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb5d1b57",
      "metadata": {},
      "source": [
        "## 1) Environment Setup\n",
        "This notebook uses only open-source Python libraries.\n",
        "\n",
        "**Dependencies:**\n",
        "- `weaviate-client` for embedded vector DB\n",
        "- `sentence-transformers` for embeddings\n",
        "- `numpy`, `pandas`, `tqdm` for data handling and progress bars\n",
        "- `scikit-learn` for cosine similarity utilities\n",
        "- `matplotlib` (optional visualization)\n",
        "\n",
        "**If Weaviate is not running locally** at `http://localhost:8080`, see the Docker Compose snippet below in case you need to start it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a100aa36",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "# Install minimal dependencies (uncomment if needed). Running this may take a few minutes.\n",
        "# If you're inside a managed environment, prefer installing via terminal beforehand.\n",
        "\n",
        "# !pip install --quiet weaviate-client sentence-transformers numpy pandas tqdm scikit-learn matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e848fed3",
      "metadata": {},
      "source": [
        "### 1.a) (Reference) Run Weaviate Locally\n",
        "You can run **Weaviate directly in Python** using its *embedded mode*.\n",
        "This avoids Docker entirely — it runs the vector database right inside your Jupyter or Colab process.\n",
        "\n",
        "\n",
        "**Advantages**\n",
        "- No Docker or external service required\n",
        "- Works offline\n",
        "- Perfect for teaching and small RAG demos\n",
        "\n",
        "\n",
        "**How it works**\n",
        "Weaviate Embedded stores data in a local folder (default: `./weaviate_data`)\n",
        "and runs a lightweight vector database within this Python kernel.\n",
        "\n",
        "\n",
        "> If you restart the notebook kernel, your in-memory data disappears unless you specify a persistent path.\n",
        "\n",
        "\n",
        "**Example architecture**\n",
        "\n",
        "\n",
        "```\n",
        "Sentence-Transformers → Embeddings → Embedded Weaviate → Retrieval → Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82ee290f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "# Start an embedded Weaviate instance inside this notebook (no Docker needed).\n",
        "# Data is persisted to ./weaviate_data folder.\n",
        "\n",
        "import weaviate\n",
        "from weaviate.embedded import EmbeddedOptions\n",
        "\n",
        "\n",
        "embedded_path = \"./weaviate_data\"\n",
        "client = weaviate.Client(\n",
        "embedded_options=EmbeddedOptions(persistence_data_path=embedded_path)\n",
        ")\n",
        "\n",
        "\n",
        "if client.is_ready():\n",
        "    print(f\"✅ Embedded Weaviate started successfully at {embedded_path}\")\n",
        "else:\n",
        "    raise SystemExit(\"❌ Weaviate failed to start. Try restarting the kernel and rerunning this cell.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95b96234",
      "metadata": {},
      "source": [
        "## 2) Choose / Load a Document\n",
        "We'll use **Alice's Adventures in Wonderland – Chapter 1** (public domain) as a default, because it's short, familiar, and approachable.\n",
        "\n",
        "To keep this notebook fully runnable **offline**, we provide a compact inline fallback excerpt. If you prefer, you can replace the content with your own `.txt` file(s) or add network code to download from Project Gutenberg.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "820e4c18",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "# You can swap this out with your own text by reading a file instead.\n",
        "#try Microsoft Q1 financial report, and see how this handles tabular data\n",
        "\n",
        "ALICE_CH1_FALLBACK = (\n",
        "    \"\"\"\n",
        "Alice was beginning to get very tired of sitting by her sister on the bank, \n",
        "and of having nothing to do: once or twice she had peeped into the book her \n",
        "sister was reading, but it had no pictures or conversations in it, 'and what \n",
        "is the use of a book,' thought Alice 'without pictures or conversations?' \n",
        "\n",
        "So she was considering in her own mind (as well as she could, for the hot day \n",
        "made her feel very sleepy and stupid), whether the pleasure of making a \n",
        "daisy-chain would be worth the trouble of getting up and picking the daisies, \n",
        "when suddenly a White Rabbit with pink eyes ran close by her. \n",
        "\n",
        "There was nothing so very remarkable in that; nor did Alice think it so very \n",
        "much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I \n",
        "shall be late!' (when she thought it over afterwards, it occurred to her that \n",
        "she ought to have wondered at this, but at the time it all seemed quite \n",
        "natural); but when the Rabbit actually took a watch out of its waistcoat-\n",
        "pocket, and looked at it, and then hurried on, Alice started to her feet, for \n",
        "it flashed across her mind that she had never before seen a rabbit with either \n",
        "a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, \n",
        "she ran across the field after it, and fortunately was just in time to see it \n",
        "pop down a large rabbit-hole under the hedge.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "raw_text = ALICE_CH1_FALLBACK.strip()\n",
        "print(f\"Loaded text length (chars): {len(raw_text)}\")\n",
        "print(raw_text[:300] + (\"...\" if len(raw_text) > 300 else \"\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2be8f3b0",
      "metadata": {},
      "source": [
        "## 3) Chunking Basics (Why and How)\n",
        "We split a long document into **chunks** so that:\n",
        "- Each chunk fits typical model/context limits\n",
        "- We can **retrieve** only the relevant pieces for a query\n",
        "- Overlap preserves continuity across chunk boundaries\n",
        "\n",
        "**Rules of thumb (first pass):**\n",
        "- Use ~800–1200 characters per chunk (or 300–500 tokens)\n",
        "- 10–20% overlap (to avoid losing context at boundaries)\n",
        "- Adjust based on your queries, domain, and evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4b8dcaa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "from typing import List, Dict\n",
        "\n",
        "def simple_char_chunk(\n",
        "    text: str,\n",
        "    chunk_chars: int = 1000,\n",
        "    overlap_chars: int = 150,\n",
        ") -> List[Dict]:\n",
        "    \"\"\"Split text into overlapping character chunks.\n",
        "\n",
        "    Args:\n",
        "        text: Raw input text.\n",
        "        chunk_chars: Approx size of each chunk in characters.\n",
        "        overlap_chars: Overlap between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        A list of dicts: {id, text, start_idx, end_idx}\n",
        "    \"\"\"\n",
        "    assert chunk_chars > 0 and overlap_chars >= 0\n",
        "    chunks = []\n",
        "    n = len(text)\n",
        "    start = 0\n",
        "    chunk_id = 0\n",
        "    while start < n:\n",
        "        end = min(start + chunk_chars, n)\n",
        "        chunk_text = text[start:end]\n",
        "        chunks.append({\n",
        "            \"id\": f\"chunk_{chunk_id}\",\n",
        "            \"text\": chunk_text,\n",
        "            \"start_idx\": start,\n",
        "            \"end_idx\": end,\n",
        "        })\n",
        "        chunk_id += 1\n",
        "        if end == n:\n",
        "            break\n",
        "        # Next window starts after chunk_chars - overlap_chars\n",
        "        start = max(0, end - overlap_chars)\n",
        "    return chunks\n",
        "\n",
        "chunks = simple_char_chunk(raw_text, chunk_chars=800, overlap_chars=120)\n",
        "print(f\"Created {len(chunks)} chunk(s)\")\n",
        "for c in chunks[:3]:\n",
        "    preview = c[\"text\"][:120].replace(\"\\n\", \" \")\n",
        "    print(f\"{c['id']} [{c['start_idx']}:{c['end_idx']}] → {preview}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b972783c",
      "metadata": {},
      "source": [
        "## 4) Embeddings: Sentence-Transformers\n",
        "We'll default to **`sentence-transformers/all-MiniLM-L6-v2`** because it's small, fast, and a great baseline.\n",
        "We'll embed each chunk and **normalize** the vectors for cosine similarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54a08eb7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception as e:\n",
        "    raise SystemExit(\"sentence-transformers not installed. Please run the pip install cell above.\")\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
        "print(f\"Loaded embedding model: {EMBEDDING_MODEL_NAME}\")\n",
        "\n",
        "def l2_normalize(mat: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
        "    \"\"\"Row-wise L2 normalization (safe for zero vectors).\"\"\"\n",
        "    norms = np.linalg.norm(mat, axis=1, keepdims=True)\n",
        "    norms = np.maximum(norms, eps)\n",
        "    return mat / norms\n",
        "\n",
        "texts = [c[\"text\"] for c in chunks]\n",
        "embs = model.encode(texts, batch_size=32, show_progress_bar=True)\n",
        "embs = np.asarray(embs, dtype=np.float32)\n",
        "embs = l2_normalize(embs)\n",
        "print(\"Embeddings shape:\", embs.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09746e0d",
      "metadata": {},
      "source": [
        "## 5) Weaviate Schema & Ingestion\n",
        "We create a simple **class** `RAGChunk` with custom properties and set the vectorizer to **none** because we supply embeddings.\n",
        "Then we upsert all chunks with their vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54876538",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "import uuid\n",
        "\n",
        "CLASS_NAME = \"RAGChunk\"\n",
        "\n",
        "def reset_class(client, class_name: str = CLASS_NAME):\n",
        "    \"\"\"Delete class if it exists. Safe to re-run during development.\"\"\"\n",
        "    try:\n",
        "        schema = client.schema.get()\n",
        "        names = {c['class'] for c in schema.get('classes', [])}\n",
        "        if class_name in names:\n",
        "            client.schema.delete_class(class_name)\n",
        "            print(f\"Deleted existing class: {class_name}\")\n",
        "    except Exception as e:\n",
        "        print(\"(reset) Warning:\", e)\n",
        "\n",
        "def create_class(client, class_name: str = CLASS_NAME):\n",
        "    \"\"\"Create a bare-bones class for RAG chunks with user-provided vectors.\"\"\"\n",
        "    class_obj = {\n",
        "        \"class\": class_name,\n",
        "        \"vectorizer\": \"none\",\n",
        "        \"properties\": [\n",
        "            {\"name\": \"text\", \"dataType\": [\"text\"]},\n",
        "            {\"name\": \"source\", \"dataType\": [\"text\"]},\n",
        "            {\"name\": \"start_idx\", \"dataType\": [\"int\"]},\n",
        "            {\"name\": \"end_idx\", \"dataType\": [\"int\"]},\n",
        "        ],\n",
        "    }\n",
        "    client.schema.create_class(class_obj)\n",
        "    print(f\"Created class: {class_name}\")\n",
        "\n",
        "reset_class(client, CLASS_NAME)\n",
        "create_class(client, CLASS_NAME)\n",
        "\n",
        "# Upsert chunks with vectors\n",
        "with client.batch as batch:\n",
        "    batch.batch_size = 64\n",
        "    for i, c in enumerate(chunks):\n",
        "        properties = {\n",
        "            \"text\": c[\"text\"],\n",
        "            \"source\": \"alice_ch1_fallback\",\n",
        "            \"start_idx\": int(c[\"start_idx\"]),\n",
        "            \"end_idx\": int(c[\"end_idx\"]),\n",
        "        }\n",
        "        uid = str(uuid.uuid4())\n",
        "        batch.add_data_object(\n",
        "            data_object=properties,\n",
        "            class_name=CLASS_NAME,\n",
        "            uuid=uid,\n",
        "            vector=embs[i].tolist(),  # pass our own vector\n",
        "        )\n",
        "print(\"Ingestion complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "304fce61",
      "metadata": {},
      "source": [
        "## 6) Retrieval\n",
        "We perform **vector search** in Weaviate by embedding the query with the same model, then comparing to the stored vectors.\n",
        "\n",
        "We'll implement:\n",
        "1. Direct Weaviate vector search\n",
        "2. *(Optional teaching step done here for transparency)* **Cosine re-ranking in Python** on the top-N.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b529d5c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "def weaviate_vector_search(query_text: str, top_k: int = 5):\n",
        "    \"\"\"Embed the query and search Weaviate by vector.\n",
        "\n",
        "    Returns a list of dicts with score and properties.\n",
        "    \"\"\"\n",
        "    q_emb = model.encode([query_text])\n",
        "    q_emb = l2_normalize(np.asarray(q_emb, dtype=np.float32))\n",
        "    res = client.query.get(\n",
        "        CLASS_NAME,\n",
        "        [\"text\", \"source\", \"start_idx\", \"end_idx\"],\n",
        "    ).with_near_vector({\"vector\": q_emb[0].tolist()}).with_limit(top_k).do()\n",
        "    # Extract results\n",
        "    hits = res.get(\"data\", {}).get(\"Get\", {}).get(CLASS_NAME, [])\n",
        "    # Weaviate returns a distance score sometimes; we'll compute cosine to show students\n",
        "    return hits, q_emb\n",
        "\n",
        "def cosine_rerank(hits, q_emb, top_k: int = 5):\n",
        "    \"\"\"Given Weaviate hits, compute cosine similarities locally and re-rank for transparency.\"\"\"\n",
        "    if not hits:\n",
        "        return []\n",
        "    texts = [h[\"text\"] for h in hits]\n",
        "    doc_embs = model.encode(texts)\n",
        "    doc_embs = l2_normalize(np.asarray(doc_embs, dtype=np.float32))\n",
        "    sims = cosine_similarity(q_emb, doc_embs)[0]\n",
        "    rows = []\n",
        "    for i, h in enumerate(hits):\n",
        "        rows.append({\n",
        "            \"cosine\": float(sims[i]),\n",
        "            \"preview\": (h[\"text\"][:160].replace(\"\\n\", \" \") + (\"...\" if len(h[\"text\"])>160 else \"\")),\n",
        "            \"start_idx\": h.get(\"start_idx\"),\n",
        "            \"end_idx\": h.get(\"end_idx\"),\n",
        "        })\n",
        "    df = pd.DataFrame(rows).sort_values(\"cosine\", ascending=False).head(top_k).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "# Example queries (tune as desired)\n",
        "example_queries = [\n",
        "    \"Why is Alice bored at the beginning?\",\n",
        "    \"What unusual thing did the White Rabbit do?\",\n",
        "    \"Where did Alice see the rabbit go?\",\n",
        "]\n",
        "\n",
        "for q in example_queries:\n",
        "    print(\"\\n=== Query:\", q)\n",
        "    hits, q_emb = weaviate_vector_search(q, top_k=5)\n",
        "    df = cosine_rerank(hits, q_emb, top_k=5)\n",
        "    display(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1e25138",
      "metadata": {},
      "source": [
        "## 7) Build a RAG Answer (Template)\n",
        "Below we assemble the **top-k** retrieved chunks into a single **context**. In a real RAG system, you'd pass this context to an LLM along with the user question.\n",
        "\n",
        "We also show a **prompt template** that encourages grounded answers and citing chunk IDs/offsets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03f3f03b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "def build_context(query: str, k: int = 3) -> str:\n",
        "    hits, q_emb = weaviate_vector_search(query, top_k=max(k, 5))\n",
        "    df = cosine_rerank(hits, q_emb, top_k=k)\n",
        "    context_blocks = []\n",
        "    for i, row in df.iterrows():\n",
        "        context_blocks.append(\n",
        "            f\"[CHUNK {i}] start={row['start_idx']}\\n\" +\n",
        "            f\"{row['preview']}\\n\"\n",
        "        )\n",
        "    context = \"\\n\\n\".join(context_blocks)\n",
        "    return context\n",
        "\n",
        "user_question = \"Why did Alice run after the rabbit?\"\n",
        "context = build_context(user_question, k=3)\n",
        "print(\"Constructed context:\\n\\n\" + context)\n",
        "\n",
        "prompt_template = f\"\"\"\n",
        "You are a helpful assistant. Answer the user's question **using only** the provided context.\n",
        "If the answer isn't in the context, say you don't know.\n",
        "\n",
        "Question: {user_question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Instructions:\n",
        "- Ground your answer in the context.\n",
        "- If the context is insufficient, say \"I don't know based on the provided context.\"\n",
        "- Cite the chunk indices you used (e.g., [CHUNK 0, CHUNK 2]).\n",
        "\"\"\"\n",
        "print(\"\\n--- RAG Prompt Template ---\\n\")\n",
        "print(prompt_template)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cddbd6b",
      "metadata": {},
      "source": [
        "## 8) Wrap-Up & Next Steps\n",
        "- **Adjust chunk size & overlap** and evaluate retrieval quality with a few representative questions.\n",
        "- **Swap the embedding model** (e.g., `intfloat/e5-small-v2`) and consider its query formatting (`\"query: ...\"` vs `\"passage: ...\"`).\n",
        "- **Scale up**: multiple documents, PDFs → text extraction, deduplication, metadata hygiene.\n",
        "- **Evaluation**: add a small labeled set of queries and compute Recall@k/Precision@k.\n",
        "- **Rerankers**: try stronger re-rankers if needed (cross-encoder or LLM-based), once students grasp the basics.\n",
        "- **Guardrails**: instruct the LLM to answer only from retrieved context and to cite chunks.\n"
      ]
    }
  ],
  "metadata": {
    "authors": [
      "ChatGPT"
    ],
    "colab": {
      "name": "RAG_Basics_Weaviate.ipynb",
      "provenance": []
    },
    "created": "2025-11-05T01:57:18.013632Z",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "notebookname": "RAG_Basics_Weaviate.ipynb"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
